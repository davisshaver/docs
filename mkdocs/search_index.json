{
    "docs": [
        {
            "location": "/", 
            "text": "This is a work in progress. Some sections are missing/incomplete\n\n\nThe Coral Project is an open source project to help publishers of all sizes build better communities around their journalism. \n\n\nWe believe that if journalism is going to remain relevant and connected to the needs of people who live in a democracy, journalists need to do a better job of listening, validating, factchecking, and responding to experiences, opinions, and ideas. \n\n\nTo do this, we need better tools that make everyone feel safe, respected, and heard. We need to focus our resources on identifying positive contributions as well as making negative ones less prominent.\n\n\nTo achieve these goals, we are:\n\n\n\n\ncreating open source software, and\n\n\ncreating, refining and disseminating practices, tools, and studies \n\n\n\n\nCore code principles\n\n\nIn order to serve communities of varying shapes and sizes, all Coral Project software is conceived from the ground up to be:\n\n\n\n\nConfigurable\n: We strive to use configuration to deliver as much business logic, data modeling, and other aspects of our systems as \nis practical.\n Doing so gives us the ability to quickly configure precise UI experiences, data structures, and data science analysis with minimal need for coding, upgrades, server work, etc. Ultimately, we want the community managers who run our software to feel like they are designing their own house. This means trying things to see how they feel, looking at the results, and quickly making changes based on what they learn. We take our inspiration from the ever-changing, adaptable ecosystems of coral reefs.\n\n\nModular\n: Coral products can be used together to form a fully functioning community platform, or be used in pieces to complement existing software. In order to accomplish this, we are building core API features, message passing and import/export strategies in everything we do. We are also refining, documenting, and publishing deployment strategies for each of our products both in isolation as well as together as groups of our products configured to work in concert.\n\n\nPrivacy Minded\n: There is an implicit act of trust involved in registration for and engagement in an online community. Maintaining that trust is a top priority for us. Privacy for us begins with security concerns, and stretches deep into our product thinking. Whenever information is entered into our systems, we want to make it clear who will be able to see that information and how it will be used. We want to build safe, comfortable places that allow for conversations of varying levels of exposure, without false expectations or nasty surprises.\n\n\nSecure, Stable and Scalable\n: Our deployment recommendations, if followed, provide usable and secure environments. Each piece of our software has internal checks to catch any error states and trigger alarms, as well as external restart mechanisms. All of our platforms have proven records for stability and well-known upgrade paths. We will publish auto-scaling deployment workflows where appropriate for large sites with varying loads.\n\n\n\n\nWhat\ns going on and how can I get involved?\n\n\nWe are still actively developing this open presence and we always looking for \ncontributions\n. We want to build this project with all of you.\n\n\nUnder the hood\n\n\nCoral products are based on the following technologies:\n\n\n\n\nThe \nGo\n programming language\n\n\nThe \nMongoDB\n database\n\n\nReact\n and \nRedux\n\n\nWebpack\n and \nBabel\n\n\nDocker\n\n\n\n\nYou can use Docker Compose to \nquick start\n the Coral System or read more about how the whole ecosystem and \neach of its pieces work\n.\n\n\nFor more information about us and to see our blog, please visit \nour website\n and \nsign up to our newsletter\n. We are also on \nTwitter\n.\n\n\nThe Coral Project is a collaboration between \nThe Mozilla Foundation\n, \nThe New York Times\n, and \nThe Washington Post\n, and is funded by a grant from \nThe John S. and James L. Knight Foundation\n.", 
            "title": "About"
        }, 
        {
            "location": "/#core-code-principles", 
            "text": "In order to serve communities of varying shapes and sizes, all Coral Project software is conceived from the ground up to be:   Configurable : We strive to use configuration to deliver as much business logic, data modeling, and other aspects of our systems as  is practical.  Doing so gives us the ability to quickly configure precise UI experiences, data structures, and data science analysis with minimal need for coding, upgrades, server work, etc. Ultimately, we want the community managers who run our software to feel like they are designing their own house. This means trying things to see how they feel, looking at the results, and quickly making changes based on what they learn. We take our inspiration from the ever-changing, adaptable ecosystems of coral reefs.  Modular : Coral products can be used together to form a fully functioning community platform, or be used in pieces to complement existing software. In order to accomplish this, we are building core API features, message passing and import/export strategies in everything we do. We are also refining, documenting, and publishing deployment strategies for each of our products both in isolation as well as together as groups of our products configured to work in concert.  Privacy Minded : There is an implicit act of trust involved in registration for and engagement in an online community. Maintaining that trust is a top priority for us. Privacy for us begins with security concerns, and stretches deep into our product thinking. Whenever information is entered into our systems, we want to make it clear who will be able to see that information and how it will be used. We want to build safe, comfortable places that allow for conversations of varying levels of exposure, without false expectations or nasty surprises.  Secure, Stable and Scalable : Our deployment recommendations, if followed, provide usable and secure environments. Each piece of our software has internal checks to catch any error states and trigger alarms, as well as external restart mechanisms. All of our platforms have proven records for stability and well-known upgrade paths. We will publish auto-scaling deployment workflows where appropriate for large sites with varying loads.", 
            "title": "Core code principles"
        }, 
        {
            "location": "/#whats-going-on-and-how-can-i-get-involved", 
            "text": "We are still actively developing this open presence and we always looking for  contributions . We want to build this project with all of you.", 
            "title": "What's going on and how can I get involved?"
        }, 
        {
            "location": "/#under-the-hood", 
            "text": "Coral products are based on the following technologies:   The  Go  programming language  The  MongoDB  database  React  and  Redux  Webpack  and  Babel  Docker   You can use Docker Compose to  quick start  the Coral System or read more about how the whole ecosystem and  each of its pieces work .  For more information about us and to see our blog, please visit  our website  and  sign up to our newsletter . We are also on  Twitter .  The Coral Project is a collaboration between  The Mozilla Foundation ,  The New York Times , and  The Washington Post , and is funded by a grant from  The John S. and James L. Knight Foundation .", 
            "title": "Under the hood"
        }, 
        {
            "location": "/coral_ecosystem/", 
            "text": "Coral Ecosystem\n\n\nOver the course of the project, we are building an ecosystem of products, tools and practices. These elements will work together and/or integrate with existing community tools.\n\n\n\n\nTrust\n\n\nTrust is our \nfirst product\n. It enables newsroom users to identify different kinds of end users in order to take actions (eg. I want to block these trolls on this author; I want to highlight the best commenters on this subject.) It allows newsrooms to make manual or automated lists of users via a series of filters. \n\n\nTrust introduces a number of technological components:\n\n\n\n\nRepositories\n\n\nSponge\n\n\nSponge\n is a command line tool to import comments, authors, assets and other entities into the Coral system. It is designed to read data from a foreign \nsource\n, translate the schema into Coral conventions, and POST entities to \nour service layer\n for inserting.\n\n\nRight now the foreign source could be any of the following:\n\n\n\n\nMySQL\n\n\nPostgreSQL\n\n\nMongoDB\n\n\nWeb Service\n\n\n\n\nPillar\n\n\nPillar\n is a REST based web-service module written in golang. It provides the following services:\n\n\n\n\nImports external data into the Coral data model\n\n\nAllows CRUD operation on Coral data model\n\n\nProvides simple queries on Coral data model\n\n\n\n\nXenia\n\n\nXenia\n is a configurable service layer that publishes endpoints against \nmongo aggregation pipeline queries\n.\n\n\nXenia Driver\n\n\nThis\n is a javascript driver for Xenia. It performs queris to xenia from the browser and node.js.\n\n\nCay\n\n\nCay\n is the front-end tool for the Coral Project.\n\n\nElkhorn\n\n\nElkhorn\n is the form composer.", 
            "title": "Architectural overview"
        }, 
        {
            "location": "/coral_ecosystem/#coral-ecosystem", 
            "text": "Over the course of the project, we are building an ecosystem of products, tools and practices. These elements will work together and/or integrate with existing community tools.", 
            "title": "Coral Ecosystem"
        }, 
        {
            "location": "/coral_ecosystem/#trust", 
            "text": "Trust is our  first product . It enables newsroom users to identify different kinds of end users in order to take actions (eg. I want to block these trolls on this author; I want to highlight the best commenters on this subject.) It allows newsrooms to make manual or automated lists of users via a series of filters.   Trust introduces a number of technological components:", 
            "title": "Trust"
        }, 
        {
            "location": "/coral_ecosystem/#repositories", 
            "text": "", 
            "title": "Repositories"
        }, 
        {
            "location": "/coral_ecosystem/#sponge", 
            "text": "Sponge  is a command line tool to import comments, authors, assets and other entities into the Coral system. It is designed to read data from a foreign  source , translate the schema into Coral conventions, and POST entities to  our service layer  for inserting.  Right now the foreign source could be any of the following:   MySQL  PostgreSQL  MongoDB  Web Service", 
            "title": "Sponge"
        }, 
        {
            "location": "/coral_ecosystem/#pillar", 
            "text": "Pillar  is a REST based web-service module written in golang. It provides the following services:   Imports external data into the Coral data model  Allows CRUD operation on Coral data model  Provides simple queries on Coral data model", 
            "title": "Pillar"
        }, 
        {
            "location": "/coral_ecosystem/#xenia", 
            "text": "Xenia  is a configurable service layer that publishes endpoints against  mongo aggregation pipeline queries .", 
            "title": "Xenia"
        }, 
        {
            "location": "/coral_ecosystem/#xenia-driver", 
            "text": "This  is a javascript driver for Xenia. It performs queris to xenia from the browser and node.js.", 
            "title": "Xenia Driver"
        }, 
        {
            "location": "/coral_ecosystem/#cay", 
            "text": "Cay  is the front-end tool for the Coral Project.", 
            "title": "Cay"
        }, 
        {
            "location": "/coral_ecosystem/#elkhorn", 
            "text": "Elkhorn  is the form composer.", 
            "title": "Elkhorn"
        }, 
        {
            "location": "/developer/", 
            "text": "Introduction\n\n\nWelcome! This is the place to be if you want to learn more about Coral from the standpoint of a developer or a technical user. Here you can learn about the inner workings of Coral, how to install the products, how to use the APIs, and more.\n\n\nIf you are a developer who is interested in contributing to our code, great! You can read more about contributing in our \nContribute section\n.\n\n\nThe Coral Ecosystem\n\n\nCoral is made up of a number of component apps that work together to power three products (Trust, Ask, and Talk). You can read more about how this ecosystem fits together \nhere\n.\n\n\nInstallation\n\n\nThere are a few different installation options to choose from. Which one is right for you?\n\n\nInstall a fully functioning single-server Coral Ecosystem, using Docker.\n\n\nThis is a quick, easy, packaged solution that requires few steps and should get all components up and running quickly. The downside is that this may not scale well, as everything is installed on one server. After a certain number of users (perhaps 50 or so), the server could become overloaded.\n\n\nYou should also have the following resources on your machine before installing:\n\n\n\n\nMinimum CPU: 2.0 GHz\n\n\nMinimum RAM: 4GB\n\n\nMinimum disk space required: 4GB\n\n\n\n\nYou can find instructions on how to install Coral as a single Docker Compose installation \nhere\n.\n\n\nProbably best for you if:\n\n\n\n\nYou want to install Coral to perform a demo.\n\n\nYou want to get Coral running on your local machine to get a sense of its functionality and capabilities.\n\n\nYou want to get Coral up and running, and aren\nt worried about scaling right now.\n\n\n\n\nInstalling each component individually, using Docker.\n\n\nThis will install each component on a separate server, which allows for scaling up in future. It does require some more work to get each component talking and interacting with each other, but will scale better.\n\n\nIf you choose this option, you can visit the installation page for each Coral component, which has installation instructions for Docker and source.\n\n\n\n\nCay installation instructions\n\n\nElkhorn installation instructions\n\n\nPillar installation instructions\n\n\nSponge installation instructions\n\n\nXenia installation instructions\n\n\n\n\nProbably best for you if:\n\n\n\n\nYou have multiple servers, and are concerned about scalability.\n\n\nYou have a sysadmin to help manage installation and maintenance.\n\n\n\n\nInstalling each component individually from source code.\n\n\nThis option will have you install each component individually from source code, which will either be Go (most components) or Node.js.\n\n\nIf you choose this option, you can visit the installation page for each Coral component, which has installation instructions for Docker and source.\n\n\n\n\nCay installation instructions\n\n\nElkhorn installation instructions\n\n\nPillar installation instructions\n\n\nSponge installation instructions\n\n\nXenia installation instructions\n\n\n\n\nProbably best for you if:\n\n\n\n\nYou are a developer that wants to work on Coral and potentially \ncontribute to our code base\n (thank you!).", 
            "title": "Introduction"
        }, 
        {
            "location": "/developer/#introduction", 
            "text": "Welcome! This is the place to be if you want to learn more about Coral from the standpoint of a developer or a technical user. Here you can learn about the inner workings of Coral, how to install the products, how to use the APIs, and more.  If you are a developer who is interested in contributing to our code, great! You can read more about contributing in our  Contribute section .", 
            "title": "Introduction"
        }, 
        {
            "location": "/developer/#the-coral-ecosystem", 
            "text": "Coral is made up of a number of component apps that work together to power three products (Trust, Ask, and Talk). You can read more about how this ecosystem fits together  here .", 
            "title": "The Coral Ecosystem"
        }, 
        {
            "location": "/developer/#installation", 
            "text": "There are a few different installation options to choose from. Which one is right for you?", 
            "title": "Installation"
        }, 
        {
            "location": "/developer/#install-a-fully-functioning-single-server-coral-ecosystem-using-docker", 
            "text": "This is a quick, easy, packaged solution that requires few steps and should get all components up and running quickly. The downside is that this may not scale well, as everything is installed on one server. After a certain number of users (perhaps 50 or so), the server could become overloaded.  You should also have the following resources on your machine before installing:   Minimum CPU: 2.0 GHz  Minimum RAM: 4GB  Minimum disk space required: 4GB   You can find instructions on how to install Coral as a single Docker Compose installation  here .  Probably best for you if:   You want to install Coral to perform a demo.  You want to get Coral running on your local machine to get a sense of its functionality and capabilities.  You want to get Coral up and running, and aren t worried about scaling right now.", 
            "title": "Install a fully functioning single-server Coral Ecosystem, using Docker."
        }, 
        {
            "location": "/developer/#installing-each-component-individually-using-docker", 
            "text": "This will install each component on a separate server, which allows for scaling up in future. It does require some more work to get each component talking and interacting with each other, but will scale better.  If you choose this option, you can visit the installation page for each Coral component, which has installation instructions for Docker and source.   Cay installation instructions  Elkhorn installation instructions  Pillar installation instructions  Sponge installation instructions  Xenia installation instructions   Probably best for you if:   You have multiple servers, and are concerned about scalability.  You have a sysadmin to help manage installation and maintenance.", 
            "title": "Installing each component individually, using Docker."
        }, 
        {
            "location": "/developer/#installing-each-component-individually-from-source-code", 
            "text": "This option will have you install each component individually from source code, which will either be Go (most components) or Node.js.  If you choose this option, you can visit the installation page for each Coral component, which has installation instructions for Docker and source.   Cay installation instructions  Elkhorn installation instructions  Pillar installation instructions  Sponge installation instructions  Xenia installation instructions   Probably best for you if:   You are a developer that wants to work on Coral and potentially  contribute to our code base  (thank you!).", 
            "title": "Installing each component individually from source code."
        }, 
        {
            "location": "/quickstart/install/", 
            "text": "All-in-One Docker Compose Installation\n\n\nThe all-in-one Docker Compose installation is a quick, easy, packaged solution that requires few steps and should get all components up and running quickly. The downside is that this may not scale well, as everything is installed on one server. After a certain number of users (perhaps 50 or so), the server could become overloaded.\n\n\nYou can read about the different types of installation options on the \ndeveloper introduction page\n.\n\n\nBefore you begin\n\n\nYou must have the following items installed and running:\n\n\n\n\nMongoDB Server\n: You can find instructions on installing MongoDB \non the MongoDB website\n.\n\n\nRabbitMQ\n: You can find instructions on installing RabbitMQ \non the RabbitMQ website\n.\n\n\n\n\nYou should also have the following resources on your machine before installing:\n\n\n\n\nMinimum CPU: 2.0 GHz\n\n\nMinimum RAM: 4GB\n\n\nMinimum disk space required: 4GB\n\n\n\n\nInstall Docker Toolbox\n\n\nIf you do not already have Docker installed, do that first. You can install Docker Toolbox using the Docker instructions \nlocated here\n.\n\n\nIf you do have Docker installed, you\nll want to make sure that you have Docker Compose version 1.7 or later. You can check your version using the command \ndocker version\n.\n\n\nGet the source code\n\n\nClone the Proxy repository. This repository contains a number of setup files that you can edit, and will help you easily spin up a Docker container.\n\n\ngit clone https://github.com/coralproject/Proxy.git\n\n\n\n\nThen cd into the Proxy directory.\n\n\ncd Proxy\n\n\n\n\nSet environment variables\n\n\nThe \nenv.conf\n file contains environment variables you need to set. Setting your environment variables tells Docker which IP address your Coral frontend will have, as well as other information such as your MongoDB username and password.\n\n\nexport FRONTEND_HOST=\n\nexport GAID_VALUE=xxxx\nexport AUTH_TOKEN_VALUE=xxxx\nexport RABBIT_USER=rabbitmq\nexport RABBIT_PASS=welcome\n# mongo:\nexport MONGO_AUTHDB=admin\nexport MONGO_USER=coral-user\nexport MONGO_PASS=welcome\nexport MONGO_DB=coral\n\n#elkhorn\nexport accessKeyId=xxx\nexport secretAccessKey=xxx\nexport pillarHost=xxx\nexport basicAuthorization=xxx\nexport bucket=xxx\nexport region=xxx\n# sponge:\nexport STRATEGY_CONF=/usr/local/strategy.json\n\n\n\n\nRequired edits:\n\n\n\n\nFRONTEND_HOST\n: set to your desired IP address for the front end. For this example, we will use \n192.168.99.100\n.\n\n\n\n\nOptional edits:\n\n\n\n\nGAID_VALUE=xxxx\n: If you\nre using Google Analytics, set your token at \nexport GAID_VALUE=xxxx\n. Otherwise, delete or comment out this line.\n\n\nexport AUTH_TOKEN_VALUE=xxxx\n: If you\nre using a custom auth token, set that at \nexport AUTH_TOKEN_VALUE=xxxx\n. Otherwise, delete or comment out this line.\n\n\n\n\nFinally, while inside the \nProxy\n directory, run the following command to export your edited variables and set the environment variables.\n\n\nsource env.conf\n\n\n\n\nSpin up the Docker container\n\n\nThe very first time that you spin up the Docker container, this will be a multi-step process:\n\n\n1. Spin up the Docker container:\n\n\ndocker-compose -f docker-compose.yml up -d\n\n\n\n\nThe \ndocker-compose.yml\n file contained in the docker-setup directory contains all the instructions that Docker Compose needs to set up the Coral Ecosystem.\n\n\n2. \nNote:\n If this is your first time spinning up the Docker container, Docker is going to download and install a number of Docker images first. These can be fairly large (~500MB per image), so it may take a few minutes.\n\n\n3. Once all Docker images have been downloaded, you\nll see something like the following in your terminal:\n\n\nCreating proxy_mongodata_1\nCreating proxy_sponge_1\nCreating proxy_rabbitmq_1\nCreating proxy_atollapp_1\nCreating proxy_xeniaapp_1\nCreating proxy_pillarapp_1\nCreating proxy_cayapp_1\nCreating proxy_proxy_1\n\n\n\n\n4. Now, shut everything down with the Docker \ndown\n command. This up-down-up sequence initializes authentication on MongoDB.\n\n\ndocker-compose -f docker-compose.yml down\n\n\n\n\n5. Finally, start the Docker container back up. In future (now that the Docker images have all been downloaded and set up), you can simply use this command to start your Docker container.\n\n\ndocker-compose -f docker-compose.yml up -d\n\n\n\n\nTest it out\n\n\nTo make sure it\ns working, try to hit the front-end URL in your browser. This is the URL you specified as \nFRONTEND_HOST\n in the \nenv.conf\n setup above; in this case, \nhttps://192.168.99.100\n.\n\n\nTroubleshooting\n\n\nViewing running Docker containers\n\n\nTo see all of the Docker containers currently running, use the command \ndocker ps\n (you can read more about this command and its options at the \nDocker website\n).\n\n\ndocker ps\n\n\n\n\nYou should have all of the following containers running:\n\n\n\n\nnginx:stable-alpine\n\n\ncoralproject/cay\n\n\ncoralproject/elkhorn\n\n\ncoralproject/pillar\n\n\ncoralproject/atoll\n\n\ncoralproject/xenia\n\n\ncoralproject/sponge\n\n\nrabbitmq:management\n\n\ncoralproject/mongodata\n\n\n\n\nViewing installed Docker images\n\n\nTo see all of the Docker images you have installed, use the command \ndocker images\n (you can read more about this command and its options at the \nDocker website\n).\n\n\ndocker images\n\n\n\n\nViewing Docker logs\n\n\nTo view Docker logs for a container, use the command \ndocker logs \ncontainer id\n (you can read more about this command and its options at the \nDocker website\n).\n\n\nFirst you have to find the container id:\n\n\ndocker ps\n\n\n\n\nThen use the container id to view the logs:\n\n\ndocker logs e0bbd7be19c7\n\n\n\n\nOperating system requirements\n\n\nOn Mac, we support OS X El Capitan (10.11) or newer. If you are on an older OS, you may have to upgrade.\n\n\nUninstalling Docker images", 
            "title": "Installing Coral Ecosystem"
        }, 
        {
            "location": "/quickstart/install/#all-in-one-docker-compose-installation", 
            "text": "The all-in-one Docker Compose installation is a quick, easy, packaged solution that requires few steps and should get all components up and running quickly. The downside is that this may not scale well, as everything is installed on one server. After a certain number of users (perhaps 50 or so), the server could become overloaded.  You can read about the different types of installation options on the  developer introduction page .", 
            "title": "All-in-One Docker Compose Installation"
        }, 
        {
            "location": "/quickstart/install/#before-you-begin", 
            "text": "You must have the following items installed and running:   MongoDB Server : You can find instructions on installing MongoDB  on the MongoDB website .  RabbitMQ : You can find instructions on installing RabbitMQ  on the RabbitMQ website .   You should also have the following resources on your machine before installing:   Minimum CPU: 2.0 GHz  Minimum RAM: 4GB  Minimum disk space required: 4GB", 
            "title": "Before you begin"
        }, 
        {
            "location": "/quickstart/install/#install-docker-toolbox", 
            "text": "If you do not already have Docker installed, do that first. You can install Docker Toolbox using the Docker instructions  located here .  If you do have Docker installed, you ll want to make sure that you have Docker Compose version 1.7 or later. You can check your version using the command  docker version .", 
            "title": "Install Docker Toolbox"
        }, 
        {
            "location": "/quickstart/install/#get-the-source-code", 
            "text": "Clone the Proxy repository. This repository contains a number of setup files that you can edit, and will help you easily spin up a Docker container.  git clone https://github.com/coralproject/Proxy.git  Then cd into the Proxy directory.  cd Proxy", 
            "title": "Get the source code"
        }, 
        {
            "location": "/quickstart/install/#set-environment-variables", 
            "text": "The  env.conf  file contains environment variables you need to set. Setting your environment variables tells Docker which IP address your Coral frontend will have, as well as other information such as your MongoDB username and password.  export FRONTEND_HOST=\n\nexport GAID_VALUE=xxxx\nexport AUTH_TOKEN_VALUE=xxxx\nexport RABBIT_USER=rabbitmq\nexport RABBIT_PASS=welcome\n# mongo:\nexport MONGO_AUTHDB=admin\nexport MONGO_USER=coral-user\nexport MONGO_PASS=welcome\nexport MONGO_DB=coral\n\n#elkhorn\nexport accessKeyId=xxx\nexport secretAccessKey=xxx\nexport pillarHost=xxx\nexport basicAuthorization=xxx\nexport bucket=xxx\nexport region=xxx\n# sponge:\nexport STRATEGY_CONF=/usr/local/strategy.json  Required edits:   FRONTEND_HOST : set to your desired IP address for the front end. For this example, we will use  192.168.99.100 .   Optional edits:   GAID_VALUE=xxxx : If you re using Google Analytics, set your token at  export GAID_VALUE=xxxx . Otherwise, delete or comment out this line.  export AUTH_TOKEN_VALUE=xxxx : If you re using a custom auth token, set that at  export AUTH_TOKEN_VALUE=xxxx . Otherwise, delete or comment out this line.   Finally, while inside the  Proxy  directory, run the following command to export your edited variables and set the environment variables.  source env.conf", 
            "title": "Set environment variables"
        }, 
        {
            "location": "/quickstart/install/#spin-up-the-docker-container", 
            "text": "The very first time that you spin up the Docker container, this will be a multi-step process:  1. Spin up the Docker container:  docker-compose -f docker-compose.yml up -d  The  docker-compose.yml  file contained in the docker-setup directory contains all the instructions that Docker Compose needs to set up the Coral Ecosystem.  2.  Note:  If this is your first time spinning up the Docker container, Docker is going to download and install a number of Docker images first. These can be fairly large (~500MB per image), so it may take a few minutes.  3. Once all Docker images have been downloaded, you ll see something like the following in your terminal:  Creating proxy_mongodata_1\nCreating proxy_sponge_1\nCreating proxy_rabbitmq_1\nCreating proxy_atollapp_1\nCreating proxy_xeniaapp_1\nCreating proxy_pillarapp_1\nCreating proxy_cayapp_1\nCreating proxy_proxy_1  4. Now, shut everything down with the Docker  down  command. This up-down-up sequence initializes authentication on MongoDB.  docker-compose -f docker-compose.yml down  5. Finally, start the Docker container back up. In future (now that the Docker images have all been downloaded and set up), you can simply use this command to start your Docker container.  docker-compose -f docker-compose.yml up -d", 
            "title": "Spin up the Docker container"
        }, 
        {
            "location": "/quickstart/install/#test-it-out", 
            "text": "To make sure it s working, try to hit the front-end URL in your browser. This is the URL you specified as  FRONTEND_HOST  in the  env.conf  setup above; in this case,  https://192.168.99.100 .", 
            "title": "Test it out"
        }, 
        {
            "location": "/quickstart/install/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/quickstart/install/#viewing-running-docker-containers", 
            "text": "To see all of the Docker containers currently running, use the command  docker ps  (you can read more about this command and its options at the  Docker website ).  docker ps  You should have all of the following containers running:   nginx:stable-alpine  coralproject/cay  coralproject/elkhorn  coralproject/pillar  coralproject/atoll  coralproject/xenia  coralproject/sponge  rabbitmq:management  coralproject/mongodata", 
            "title": "Viewing running Docker containers"
        }, 
        {
            "location": "/quickstart/install/#viewing-installed-docker-images", 
            "text": "To see all of the Docker images you have installed, use the command  docker images  (you can read more about this command and its options at the  Docker website ).  docker images", 
            "title": "Viewing installed Docker images"
        }, 
        {
            "location": "/quickstart/install/#viewing-docker-logs", 
            "text": "To view Docker logs for a container, use the command  docker logs  container id  (you can read more about this command and its options at the  Docker website ).  First you have to find the container id:  docker ps  Then use the container id to view the logs:  docker logs e0bbd7be19c7", 
            "title": "Viewing Docker logs"
        }, 
        {
            "location": "/quickstart/install/#operating-system-requirements", 
            "text": "On Mac, we support OS X El Capitan (10.11) or newer. If you are on an older OS, you may have to upgrade.", 
            "title": "Operating system requirements"
        }, 
        {
            "location": "/quickstart/install/#uninstalling-docker-images", 
            "text": "", 
            "title": "Uninstalling Docker images"
        }, 
        {
            "location": "/quickstart/mongodb/", 
            "text": "Using sample data with MongoDB\n\n\nIf you are installing locally, you will want to have a local MongoDB running with sample data. When you install the all-in-one Docker Compose installation, this portion is taken care of for you. However, if you are running from source, for example, you will want to set up your local MongoDB and import sample data.\n\n\nDownload a sample data dump\n\n\nWe have provided an anonymized comments data dump for MongoDB, available to downlaod here: \nMongoDB data dump\n.\nThis data dump is 100MB. Download it to your computer.\n\n\nDownload and install MongoDB\n\n\nFirst, download and set up MongoDB. The MongoDB website offers \ninstructions on how to download and install\n.\n\n\nA nice GUI tool you can use with MongoDB is \nMongoChef\n, and it\ns quite easy to install and set up.\n\n\nCreate your local coral database\n\n\nTODO: add here\n\n\nImport data\n\n\nThe simplest way to do this is from the command line. First ensure you have MongoDB running.\n\n\nThen, take the dump.tar.gz file you downloaded, place it in a folder, and extract it. Copy the directory path to the coral directory within the extracted file.\n\n\nThen, run the below command (filling in your own directory path):\n\n\nmongorestore -d coral /yourpath/dump/coral/\n\n\n\n\nThis will import all the data from the data dump into your own local MongoDB.", 
            "title": "Setting up MongoDB"
        }, 
        {
            "location": "/quickstart/mongodb/#using-sample-data-with-mongodb", 
            "text": "If you are installing locally, you will want to have a local MongoDB running with sample data. When you install the all-in-one Docker Compose installation, this portion is taken care of for you. However, if you are running from source, for example, you will want to set up your local MongoDB and import sample data.", 
            "title": "Using sample data with MongoDB"
        }, 
        {
            "location": "/quickstart/mongodb/#download-a-sample-data-dump", 
            "text": "We have provided an anonymized comments data dump for MongoDB, available to downlaod here:  MongoDB data dump .\nThis data dump is 100MB. Download it to your computer.", 
            "title": "Download a sample data dump"
        }, 
        {
            "location": "/quickstart/mongodb/#download-and-install-mongodb", 
            "text": "First, download and set up MongoDB. The MongoDB website offers  instructions on how to download and install .  A nice GUI tool you can use with MongoDB is  MongoChef , and it s quite easy to install and set up.", 
            "title": "Download and install MongoDB"
        }, 
        {
            "location": "/quickstart/mongodb/#create-your-local-coral-database", 
            "text": "TODO: add here", 
            "title": "Create your local coral database"
        }, 
        {
            "location": "/quickstart/mongodb/#import-data", 
            "text": "The simplest way to do this is from the command line. First ensure you have MongoDB running.  Then, take the dump.tar.gz file you downloaded, place it in a folder, and extract it. Copy the directory path to the coral directory within the extracted file.  Then, run the below command (filling in your own directory path):  mongorestore -d coral /yourpath/dump/coral/  This will import all the data from the data dump into your own local MongoDB.", 
            "title": "Import data"
        }, 
        {
            "location": "/cay/", 
            "text": "Cay Overview\n\n\nCay\n is the front-end application for the Coral Community Tools.\n\n\nStructure and development\n\n\nThe app is a series of React components compiled into modules with \nwebpack\n.\n\n\nThe basic idea is that the build process results in a \nbundle.js\n file containing all javascript and css. CSS cross-browser issues, ES6 transpilation, minification, etc, is all handled by webpack. \nbundle.js\n lives in-memory until build time (\nnpm run build\n).\n\n\nThe meat of the application lives in the \n/src\n folder. Components are grouped into domains. For instance, everything that has to do with creating, viewing and editing Searches lives in the \nsearch\n folder. This includes redux reducers and action files for that domain.\n\n\nNote: the reducers are combined in \n/src/app/MainReducer.js\n.\n\n\nCay Installation\n\n\nInstall from source\n\n\nBefore you begin\n\n\nBefore you begin, be sure you have the following installed and running:\n\n\n\n\nXenia\n\n\nPillar\n\n\n\n\nIn addition, you must have Node.js installed:\n\n\n\n\nNode.js\n\n\nYou must be running version 5.0.0 or higher of node. You can check your current version of node with the command \nnode --version\n\n\nWe recommend using \nnvm\n to manage your node installations.\n\n\n\n\n\n\n\n\nGet the source\n\n\nClone the Cay repository onto your machine:\n\n\ngit clone https://github.com/coralproject/cay.git\n\n\n\n\ncd into the Cay directory:\n\n\ncd cay\n\n\n\n\nDo an npm install to install the node code:\n\n\nnpm install\n\n\n\n\nSet up configuration file\n\n\nThe folder called \npublic\n has a sample configuration file, called \nconfig.sample.json\n. Copy this file to a new file called \nconfig.json\n, that you will edit:\n\n\ncp public/config.sample.json public/config.json\n\n\n\n\nNow edit the config.json file.\n\n\n  \nxeniaHost\n: \n,\n  \npillarHost\n: \n,\n  \nelkhornHost\n: \nhttp://localhost:4444\n,\n  \nenvironment\n: \ndevelopment\n,\n  \nbrand\n: \n,\n  \ngoogleAnalyticsId\n: \nUA-12345678-9\n,\n  \nrequireLogin\n: false,\n  \nbasicAuthorization\n: \n,\n  \nfeatures\n: {\n    \nask\n: false\n  }\n}\n\n\n\n\nFor \nxeniaHost\n, \npillarHost\n, and \nelkhornHost\n, enter the URLs where each service is running.\n\n\n\n\nFor \nxeniaHost\n:\n\n\nIf you installed Xenia \nlocally from source\n, \nxeniaHost\n should be \nhttp://localhost:4000/1.0/exec/\n.\n\n\n\n\n\n\nFor \npillarHost\n:\n\n\nIf you installed Pillar \nlocally from source\n, \npillarHost\n should be \nhttp://localhost:8080\n.\n\n\nIf you installed Pillar \nlocally as a Docker container\n, \npillarHost\n should be \nhttp://10.0.4.105:8080\n. \nhttp://10.0.4.105\n is the URL generated by Docker.\n\n\n\n\n\n\n\n\nFor \nelkhornHost\n:\n\n\n\n\nIf you installed \nlocally from source\n, \nelkhornHost\n should be \nhttp://localhost:4444\n.\n\n\n\n\n\n\n\n\nUnder \nfeatures\n, for the time being, only \nask\n will have a value of \ntrue\n. Later, we will be adding fields for our additional two products, Trust and Talk. By setting the value to \ntrue\n, you are turning on the features that can be seen in Cay.\n\n\n\n\n\n\nRun the app\n\n\nYou can now start Cay by running npm start:\n\n\nnpm start\n\n\n\n\nYou can now visit Cay by visiting the URL \nhttp://localhost:3000\n.", 
            "title": "Introduction"
        }, 
        {
            "location": "/cay/#cay-overview", 
            "text": "Cay  is the front-end application for the Coral Community Tools.", 
            "title": "Cay Overview"
        }, 
        {
            "location": "/cay/#structure-and-development", 
            "text": "The app is a series of React components compiled into modules with  webpack .  The basic idea is that the build process results in a  bundle.js  file containing all javascript and css. CSS cross-browser issues, ES6 transpilation, minification, etc, is all handled by webpack.  bundle.js  lives in-memory until build time ( npm run build ).  The meat of the application lives in the  /src  folder. Components are grouped into domains. For instance, everything that has to do with creating, viewing and editing Searches lives in the  search  folder. This includes redux reducers and action files for that domain.  Note: the reducers are combined in  /src/app/MainReducer.js .", 
            "title": "Structure and development"
        }, 
        {
            "location": "/cay/#cay-installation", 
            "text": "", 
            "title": "Cay Installation"
        }, 
        {
            "location": "/cay/#install-from-source", 
            "text": "", 
            "title": "Install from source"
        }, 
        {
            "location": "/cay/#before-you-begin", 
            "text": "Before you begin, be sure you have the following installed and running:   Xenia  Pillar   In addition, you must have Node.js installed:   Node.js  You must be running version 5.0.0 or higher of node. You can check your current version of node with the command  node --version  We recommend using  nvm  to manage your node installations.", 
            "title": "Before you begin"
        }, 
        {
            "location": "/cay/#get-the-source", 
            "text": "Clone the Cay repository onto your machine:  git clone https://github.com/coralproject/cay.git  cd into the Cay directory:  cd cay  Do an npm install to install the node code:  npm install", 
            "title": "Get the source"
        }, 
        {
            "location": "/cay/#set-up-configuration-file", 
            "text": "The folder called  public  has a sample configuration file, called  config.sample.json . Copy this file to a new file called  config.json , that you will edit:  cp public/config.sample.json public/config.json  Now edit the config.json file.     xeniaHost :  ,\n   pillarHost :  ,\n   elkhornHost :  http://localhost:4444 ,\n   environment :  development ,\n   brand :  ,\n   googleAnalyticsId :  UA-12345678-9 ,\n   requireLogin : false,\n   basicAuthorization :  ,\n   features : {\n     ask : false\n  }\n}  For  xeniaHost ,  pillarHost , and  elkhornHost , enter the URLs where each service is running.   For  xeniaHost :  If you installed Xenia  locally from source ,  xeniaHost  should be  http://localhost:4000/1.0/exec/ .    For  pillarHost :  If you installed Pillar  locally from source ,  pillarHost  should be  http://localhost:8080 .  If you installed Pillar  locally as a Docker container ,  pillarHost  should be  http://10.0.4.105:8080 .  http://10.0.4.105  is the URL generated by Docker.     For  elkhornHost :   If you installed  locally from source ,  elkhornHost  should be  http://localhost:4444 .     Under  features , for the time being, only  ask  will have a value of  true . Later, we will be adding fields for our additional two products, Trust and Talk. By setting the value to  true , you are turning on the features that can be seen in Cay.", 
            "title": "Set up configuration file"
        }, 
        {
            "location": "/cay/#run-the-app", 
            "text": "You can now start Cay by running npm start:  npm start  You can now visit Cay by visiting the URL  http://localhost:3000 .", 
            "title": "Run the app"
        }, 
        {
            "location": "/cay/install/", 
            "text": "Cay Installation\n\n\nInstall from source\n\n\nBefore you begin\n\n\nBefore you begin, be sure you have the following installed and running:\n\n\n\n\nXenia\n\n\nPillar\n\n\n\n\nIn addition, you must have Node.js installed:\n\n\n\n\nNode.js\n\n\nYou must be running version 5.0.0 or higher of node. You can check your current version of node with the command \nnode --version\n\n\nWe recommend using \nnvm\n to manage your node installations.\n\n\n\n\n\n\n\n\nGet the source\n\n\nClone the Cay repository onto your machine:\n\n\ngit clone https://github.com/coralproject/cay.git\n\n\n\n\ncd into the Cay directory:\n\n\ncd cay\n\n\n\n\nDo an npm install to install the node code:\n\n\nnpm install\n\n\n\n\nSet up configuration file\n\n\nThe folder called \npublic\n has a sample configuration file, called \nconfig.sample.json\n. Copy this file to a new file called \nconfig.json\n, that you will edit:\n\n\ncp public/config.sample.json public/config.json\n\n\n\n\nNow edit the config.json file.\n\n\n  \nxeniaHost\n: \n,\n  \npillarHost\n: \n,\n  \nelkhornHost\n: \nhttp://localhost:4444\n,\n  \nenvironment\n: \ndevelopment\n,\n  \nbrand\n: \n,\n  \ngoogleAnalyticsId\n: \nUA-12345678-9\n,\n  \nrequireLogin\n: false,\n  \nbasicAuthorization\n: \n,\n  \nfeatures\n: {\n    \nask\n: false\n  }\n}\n\n\n\n\nFor \nxeniaHost\n, \npillarHost\n, and \nelkhornHost\n, enter the URLs where each service is running.\n\n\n\n\nFor \nxeniaHost\n:\n\n\nIf you installed Xenia \nlocally from source\n, \nxeniaHost\n should be \nhttp://localhost:4000/1.0/exec/\n.\n\n\n\n\n\n\nFor \npillarHost\n:\n\n\nIf you installed Pillar \nlocally from source\n, \npillarHost\n should be \nhttp://localhost:8080\n.\n\n\nIf you installed Pillar \nlocally as a Docker container\n, \npillarHost\n should be \nhttp://10.0.4.105:8080\n. \nhttp://10.0.4.105\n is the URL generated by Docker.\n\n\n\n\n\n\n\n\nFor \nelkhornHost\n:\n\n\n\n\nIf you installed \nlocally from source\n, \nelkhornHost\n should be \nhttp://localhost:4444\n.\n\n\n\n\n\n\n\n\nUnder \nfeatures\n, for the time being, only \nask\n will have a value of \ntrue\n. Later, we will be adding fields for our additional two products, Trust and Talk. By setting the value to \ntrue\n, you are turning on the features that can be seen in Cay.\n\n\n\n\n\n\nRun the app\n\n\nYou can now start Cay by running npm start:\n\n\nnpm start\n\n\n\n\nYou can now visit Cay by visiting the URL \nhttp://localhost:3000\n.", 
            "title": "Installation"
        }, 
        {
            "location": "/cay/install/#cay-installation", 
            "text": "", 
            "title": "Cay Installation"
        }, 
        {
            "location": "/cay/install/#install-from-source", 
            "text": "", 
            "title": "Install from source"
        }, 
        {
            "location": "/cay/install/#before-you-begin", 
            "text": "Before you begin, be sure you have the following installed and running:   Xenia  Pillar   In addition, you must have Node.js installed:   Node.js  You must be running version 5.0.0 or higher of node. You can check your current version of node with the command  node --version  We recommend using  nvm  to manage your node installations.", 
            "title": "Before you begin"
        }, 
        {
            "location": "/cay/install/#get-the-source", 
            "text": "Clone the Cay repository onto your machine:  git clone https://github.com/coralproject/cay.git  cd into the Cay directory:  cd cay  Do an npm install to install the node code:  npm install", 
            "title": "Get the source"
        }, 
        {
            "location": "/cay/install/#set-up-configuration-file", 
            "text": "The folder called  public  has a sample configuration file, called  config.sample.json . Copy this file to a new file called  config.json , that you will edit:  cp public/config.sample.json public/config.json  Now edit the config.json file.     xeniaHost :  ,\n   pillarHost :  ,\n   elkhornHost :  http://localhost:4444 ,\n   environment :  development ,\n   brand :  ,\n   googleAnalyticsId :  UA-12345678-9 ,\n   requireLogin : false,\n   basicAuthorization :  ,\n   features : {\n     ask : false\n  }\n}  For  xeniaHost ,  pillarHost , and  elkhornHost , enter the URLs where each service is running.   For  xeniaHost :  If you installed Xenia  locally from source ,  xeniaHost  should be  http://localhost:4000/1.0/exec/ .    For  pillarHost :  If you installed Pillar  locally from source ,  pillarHost  should be  http://localhost:8080 .  If you installed Pillar  locally as a Docker container ,  pillarHost  should be  http://10.0.4.105:8080 .  http://10.0.4.105  is the URL generated by Docker.     For  elkhornHost :   If you installed  locally from source ,  elkhornHost  should be  http://localhost:4444 .     Under  features , for the time being, only  ask  will have a value of  true . Later, we will be adding fields for our additional two products, Trust and Talk. By setting the value to  true , you are turning on the features that can be seen in Cay.", 
            "title": "Set up configuration file"
        }, 
        {
            "location": "/cay/install/#run-the-app", 
            "text": "You can now start Cay by running npm start:  npm start  You can now visit Cay by visiting the URL  http://localhost:3000 .", 
            "title": "Run the app"
        }, 
        {
            "location": "/elkhorn/", 
            "text": "Introduction\n\n\nElkhorn\n is the form composer and embeddable builder.\n\n\n\n\nAskComposer:\n\n\n\n\ntakes an \nAsk spec\n in some sort of serialization (JSON or any other), and renders it.\n\n\ndoes not know where the serialized spec came from.\n\n\nstores the state of the form (completed fields, current progress, etc)\n\n\npersists the state by sending the completed form to a server destination\n\n\nmay persist partial states locally\n\n\n\n\nEmbed Service:\n\n\n\n\nuses \nrollup\n (and it\ns amazing \ntree-shaking\n feature) to generate a build of minimum size.\n\n\n\n\nTo view forms\n\n\nAs a standalone page\n\n\nA full page including the form is rendered from this endpoint:\n\n\nhttps://[elkhornserver]/iframe/[form_id]\n\n\n\n\nvia iFrame\n\n\nThe standalone page link is suitable for an iframe:\n\n\niframe src=\nhttps://[elkhornserver]/iframe/[form_id]\n width=\n100%\n height=\n600px\n/iframe\n\n\n\n\n\n\n\nNote that the width and height parameters may need to be tweaked.\n\n\n\n\niframes can be embedded directly into pages\n\n\nRendered directly into a page\n\n\nWe can also render a form directly into a page.  This provides the advantages of native css inheritance as well as all the other issues that come with iframes.\n\n\ndiv id=\nask-form\n/div\nscript src=\n[filewritelocation]/[formid].js\n/script", 
            "title": "Introduction"
        }, 
        {
            "location": "/elkhorn/#introduction", 
            "text": "Elkhorn  is the form composer and embeddable builder.", 
            "title": "Introduction"
        }, 
        {
            "location": "/elkhorn/#askcomposer", 
            "text": "takes an  Ask spec  in some sort of serialization (JSON or any other), and renders it.  does not know where the serialized spec came from.  stores the state of the form (completed fields, current progress, etc)  persists the state by sending the completed form to a server destination  may persist partial states locally", 
            "title": "AskComposer:"
        }, 
        {
            "location": "/elkhorn/#embed-service", 
            "text": "uses  rollup  (and it s amazing  tree-shaking  feature) to generate a build of minimum size.", 
            "title": "Embed Service:"
        }, 
        {
            "location": "/elkhorn/#to-view-forms", 
            "text": "", 
            "title": "To view forms"
        }, 
        {
            "location": "/elkhorn/#as-a-standalone-page", 
            "text": "A full page including the form is rendered from this endpoint:  https://[elkhornserver]/iframe/[form_id]", 
            "title": "As a standalone page"
        }, 
        {
            "location": "/elkhorn/#via-iframe", 
            "text": "The standalone page link is suitable for an iframe:  iframe src= https://[elkhornserver]/iframe/[form_id]  width= 100%  height= 600px /iframe    Note that the width and height parameters may need to be tweaked.   iframes can be embedded directly into pages", 
            "title": "via iFrame"
        }, 
        {
            "location": "/elkhorn/#rendered-directly-into-a-page", 
            "text": "We can also render a form directly into a page.  This provides the advantages of native css inheritance as well as all the other issues that come with iframes.  div id= ask-form /div script src= [filewritelocation]/[formid].js /script", 
            "title": "Rendered directly into a page"
        }, 
        {
            "location": "/elkhorn/install/", 
            "text": "Elkhorn Installation\n\n\nInstall from source\n\n\nBefore you begin\n\n\nBefore you begin, be sure you have the following installed and running:\n\n\n\n\nPillar\n\n\n\n\nIn addition, you must have Node.js installed:\n\n\n\n\nNode.js\n\n\nYou must be running version 5.0.0 or higher of node. You can check your current version of node with the command \nnode --version\n\n\nWe recommend using \nnvm\n to manage your node installations.\n\n\n\n\n\n\n\n\nClone the Elkhorn repository\n\n\nClone the Elkhorn repository:\n\n\ngit clone https://github.com/coralproject/elkhorn.git\n\n\n\n\nThen cd into the Elkhorn directory.\n\n\ncd elkhorn\n\n\n\n\nBuild Elkhorn.\n\n\nnpm install\n\n\n\n\nSet up configuration file\n\n\nThe Elkhorn directory has a configuration file, called \nconfig.sample.json\n. Copy this file to a new file called \nconfig.json\n, that you will edit:\n\n\ncp config.sample.json config.json\n\n\n\n\nNow edit the config.json file.\n\n\n{\n  \npillarHost\n: \n,\n  \nbasicAuthorization\n: \nBasic 123123123123213\n,\n  \ns3\n: {\n    \nbucket\n: \n,\n    \nregion\n: \n,\n    \naccessKeyId\n: \n,\n    \nsecretAccessKey\n: \n\n  }\n}\n\n\n\n\nFor \npillarHost\n, enter the URL where the service is running. If you installed \nlocally from source\n, \npillarHost\n should be \nhttp://localhost:8080\n.\n\n\nRun the app\n\n\nYou can now start Elkhorn by running npm start:\n\n\nnpm start\n\n\n\n\nElkhorn will now be running on port 4444. You can now visit Elkhorn by visiting the URL \nhttp://localhost:4444\n.\n\n\nInstall as a Docker container\n\n\nClone the Elkhorn repository\n\n\nClone the Elkhorn repository:\n\n\ngit clone https://github.com/coralproject/elkhorn.git\n\n\n\n\nThen cd into the Elkhorn directory.\n\n\ncd elkhorn\n\n\n\n\nBuild and run Elkhorn\n\n\nBuild Elkhorn:\n\n\ndocker build -t elkhorn .\n\n\n\n\nRun Elkhorn:\n\n\ndocker run  --name elkhorn -d -p 4444:4444 elkhorn\n\n\n\n\nElkhorn will now be running on port 4444.", 
            "title": "Installation"
        }, 
        {
            "location": "/elkhorn/install/#elkhorn-installation", 
            "text": "", 
            "title": "Elkhorn Installation"
        }, 
        {
            "location": "/elkhorn/install/#install-from-source", 
            "text": "", 
            "title": "Install from source"
        }, 
        {
            "location": "/elkhorn/install/#before-you-begin", 
            "text": "Before you begin, be sure you have the following installed and running:   Pillar   In addition, you must have Node.js installed:   Node.js  You must be running version 5.0.0 or higher of node. You can check your current version of node with the command  node --version  We recommend using  nvm  to manage your node installations.", 
            "title": "Before you begin"
        }, 
        {
            "location": "/elkhorn/install/#clone-the-elkhorn-repository", 
            "text": "Clone the Elkhorn repository:  git clone https://github.com/coralproject/elkhorn.git  Then cd into the Elkhorn directory.  cd elkhorn  Build Elkhorn.  npm install", 
            "title": "Clone the Elkhorn repository"
        }, 
        {
            "location": "/elkhorn/install/#set-up-configuration-file", 
            "text": "The Elkhorn directory has a configuration file, called  config.sample.json . Copy this file to a new file called  config.json , that you will edit:  cp config.sample.json config.json  Now edit the config.json file.  {\n   pillarHost :  ,\n   basicAuthorization :  Basic 123123123123213 ,\n   s3 : {\n     bucket :  ,\n     region :  ,\n     accessKeyId :  ,\n     secretAccessKey :  \n  }\n}  For  pillarHost , enter the URL where the service is running. If you installed  locally from source ,  pillarHost  should be  http://localhost:8080 .", 
            "title": "Set up configuration file"
        }, 
        {
            "location": "/elkhorn/install/#run-the-app", 
            "text": "You can now start Elkhorn by running npm start:  npm start  Elkhorn will now be running on port 4444. You can now visit Elkhorn by visiting the URL  http://localhost:4444 .", 
            "title": "Run the app"
        }, 
        {
            "location": "/elkhorn/install/#install-as-a-docker-container", 
            "text": "", 
            "title": "Install as a Docker container"
        }, 
        {
            "location": "/elkhorn/install/#clone-the-elkhorn-repository_1", 
            "text": "Clone the Elkhorn repository:  git clone https://github.com/coralproject/elkhorn.git  Then cd into the Elkhorn directory.  cd elkhorn", 
            "title": "Clone the Elkhorn repository"
        }, 
        {
            "location": "/elkhorn/install/#build-and-run-elkhorn", 
            "text": "Build Elkhorn:  docker build -t elkhorn .  Run Elkhorn:  docker run  --name elkhorn -d -p 4444:4444 elkhorn  Elkhorn will now be running on port 4444.", 
            "title": "Build and run Elkhorn"
        }, 
        {
            "location": "/pillar/", 
            "text": "Introduction\n\n\nPillar\n is a REST based web service written in Golang. It provides the following services:\n\n\n\n\nImports external data into Coral data model\n\n\nAllows CRUD operation on Coral data model\n\n\nProvides simple queries on Coral data model\n\n\n\n\nKey Points\n\n\n\n\n\n\nPillar APIs strongly adhere to \nREST style\n.\n\n\n\n\n\n\nPillar APIs only work with \nJSON\n data.\n\n\n\n\n\n\nThere are two broad types of API endpoints:\n\n\n\n\nThe URL pattern for the regular \nCRUD\n endpoints is \n/api/*\n.\n\n\nThe URL pattern for the import endpoints is \n/api/import/*\n.\n\n\n\n\n\n\n\n\nThe \nimport\n API endpoints allow you to import data into Coral from an existing source data store.\n\n\n\n\nThe key to successful importing and tracking lies in the \nImportSource\n structure. This structure keeps the original identifiers.\n\n\nMost top-level models (for example, \nUser\n or \nComment\n) embed this source data in a field named \nSource\n.\n\n\n\n\n\n\n\n\nWe understand that an import process can be challenging. Hence, all of the \nimport\n API endpoints \nupsert\n data. This means that each time you perform an import, existing entries are overwritten by the new entries (\nupsert\n basically means \ninsert or update\n).\n\n\n\n\n\n\nPillar Installation\n\n\nIf you want to install Pillar as part of an all-in-one installation of the Coral Ecosystem, you can \nfind instructions to do that here\n.\n\n\nWhen installing Pillar by itself, you can choose between installing Pillar as a Docker container, or installing from source.\n\n\n\n\nInstall Pillar from source\n\n\nInstall Pillar as a Docker container\n\n\n\n\nBefore you begin\n\n\nBefore you install Pillar, you must have the following items installed and running:\n\n\n\n\nMongoDB\n: You can find instructions on installing MongoDB \non the MongoDB website\n.\n\n\nThere are \ninstructions on importing sample comment data into MongoDB here\n\n\n\n\n\n\nRabbitMQ\n: You can find instructions on installing RabbitMQ \non the RabbitMQ website\n.\n\n\nXenia\n: Xenia is a configurable service layer that publishes endpoints against mongo aggregation pipeline queries. It is part of the Coral ecosystem. You can find instructions on how to install Xenia \nhere\n.\n\n\n\n\nInstall Pillar from source\n\n\nBefore you begin\n\n\nIf you want to install from source, you will need to have Go installed.\n\n\nYou can install \ninstall Go from their website\n. The \ninstallation and setup instructions\n on the Go website are quite good. Ensure that you have exported your $GOPATH environment variable, as detailed in the \ninstallation instructions\n.\n\n\nIf you are not on a version of Go that is 1.7 or higher, you will also have to set the GO15VENDOREXPERIMENT flag.\n\n\nexport GO15VENDOREXPERIMENT=1\n\n\n\n\nIf you are not on a version of Go 1.7 or higher, we recommend adding this to your ~/.bash_profile or other startup script.\n\n\nGet the source code\n\n\nYou can install the source code via using the \ngo get\n command, or by manually cloning the code.\n\n\nUsing the go get command\n\n\ngo get github.com/coralproject/pillar\n\n\n\n\nIf you see a message about \nno buildable Go source files\n as shown below, you can ignore it. It simply means that there are no buildable source files in the uppermost pillar directory (though there are buildable source files in subdirectories).\n\n\npackage github.com/coralproject/pillar: no buildable Go source files in [directory]\n\n\n\n\nCloning manually\n\n\nYou can also clone the code manually.\n\n\nmkdir $GOPATH/src/github.com/coralproject/pillar\ncd $GOPATH/src/github.com/coralproject/pillar\n\ngit clone https://github.com/coralproject/pillar.git\n\n\n\n\nSet your environment variables\n\n\nSetting your environment variables tells Pillar the URLs and other information for communicating with MongoDB, RabbitMQ, and Xenia.\n\n\nMake your own copy of the \nconfig/dev.cfg\n file (you can edit this configuration file with your own values, and then ensure that you don\nt commit it back to the repository). Call your config file whatever you like; we\nll call it \ncustom\n in this example.\n\n\ncd $GOPATH/src/github.com/coralproject/pillar\ncp config/dev.cfg config/custom.cfg\n\n\n\n\nNow edit the values in your custom.cfg file:\n\n\n#Resources\nexport MONGODB_URL=\nmongodb://localhost:27017/coral\n\nexport AMQP_URL=\namqp://localhost:5672/\n\nexport AMQP_EXCHANGE=\nPillarMQ\n\n\n#Pillar\nexport PILLAR_ADDRESS=\n:8080\n\nexport PILLAR_HOME=\n/opt/pillar\n\nexport PILLAR_CRON=\nfalse\n\nexport PILLAR_CRON_SEARCH=\n@every 30m\n\nexport PILLAR_CRON_STATS=\n@every 1m\n\n\n#Xenia\nexport XENIA_URL=\nhttp://localhost:4000/1.0/exec/\n\nexport XENIA_QUERY_PARAM=\n?skip=0\nlimit=100\n\nexport XENIA_AUTH=\nauth token\n\n\n# Stats\nexport MONGODB_ADDRESS=\n127.0.0.1:27017\n\nexport MONGODB_USERNAME=\n\nexport MONGODB_PASSWORD=\n\nexport MONGODB_DATABASE=\n\nexport MONGODB_SSL=\nFalse\n\n\n\n\n\n\n\nFor \nXENIA_URL\n:\n\n\nIf you installed \nlocally from source\n, \nXENIA_URL\n should be \nhttp://localhost:4000/1.0/exec/\n.\n\n\n\n\n\n\nFor \nMONGODB_URL\n:\n\n\nIf your MongoDB is a local installation, \nMONGODB_URL\n should be \nmongodb://localhost:27017/coral\n.\n\n\n\n\n\n\nFor \nAMQP_URL\n:\n\n\nIf your RabbitMQ is a local installation, \nAMQP_URL\n should be \namqp://localhost:5672/\n.\n\n\n\n\n\n\n\n\nOnce you\nve edited and saved your custom.cfg file, source it:\n\n\nsource $GOPATH/src/github.com/coralproject/pillar/config/custom.cfg\n\n\n\n\nRun Pillar\n\n\n$GOPATH/bin/pillar\n\n\n\n\nYou should see:\n\n\n[negroni] listening on :8080\n\n\n\n\nInstall as Docker Container\n\n\nInstall Docker\n\n\nYou can find more information on installing Docker on your machine \non the Docker website\n.\n\n\n\n\nOn the server, you can install Docker with the following command:\n\n\n\n\nsudo yum install docker\n\n\n\n\nClone the Pillar repository\n\n\nClone the Pillar repository:\n\n\ngit clone https://github.com/coralproject/pillar.git\n\n\n\n\nThen cd into the Pillar directory.\n\n\nStart Docker\n\n\nStart Docker.\n\n\n\n\nOn the server, you can do this via the command:\n  \nsudo service docker start\n\n\nOn your local machine, you can start Docker via the Docker Quickstart Terminal. This will usually be in your Applications folder, or (if on Mac) you can type \ndocker quickstart\n into Spotlight to find it quickly. The Docker Quickstart Terminal will open a new terminal window, running Docker, that you will then use to run the rest of the Docker related commands below.\n\n\n\n\nBuild Pillar server\n\n\nBuild the Pillar server using \ndocker build\n.\n\n\ndocker build -t pillar-server:0.1 .\n\n\n\n\n\n\nIf you are building on the server, you may have to use \nsudo\n:\n\n\n\n\nsudo docker build -t pillar-server:0.1 .\n\n\n\n\nEdit environment variables\n\n\nThe env.list file contains environment variables you need to set. Edit this file to reflect the settings on your own system.\n\n\n# TODO: include finished env.list here\n\n\n\n\n\n\nFor \nXENIA_URL\n:\n\n\nIf you installed \nlocally from source\n, \nXENIA_URL\n should be \nhttp://localhost:4000/1.0/exec/\n.\n\n\n\n\n\n\nFor \nMONGODB_URL\n:\n\n\nIf your MongoDB is a local installation, \nMONGODB_URL\n should be \nmongodb://localhost:27017/coral\n.\n\n\n\n\n\n\n\n\nRun Docker\n\n\nFirst, find the Image ID for the Pillar server:\n\n\ndocker images\n\n\n\n\n\n\nIf you are running on a server, you may have to use \nsudo\n.\n\n\n\n\nThis shows you the Image ID:\n\n\nREPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\npillar-server       0.1                 24b7acf7a4b3        4 hours ago         771 MB\ngolang              1.6                 024309f28934        8 days ago          744.1 MB\n\n\n\n\nThen run the docker run command with the Image ID:\n\n\ndocker run --env-file env.list --publish 8080:8080 24b7acf7a4b3\n\n\n\n\nYou should see the following:\n\n\n[negroni] listening on :8080\n\n\n\n\nTest it out\n\n\nTo see if Pillar is working correctly, visit this url: \nhttp://10.0.4.105:8080/about\n. \nhttp://10.0.4.105\n is the URL generated by Docker for Pillar.\n\n\nIf things are running properly, you should see this text:\n\n\n{\nApp\n:\nCoral Pillar Web Service\n,\nVersion\n:\nVersion - 0.0.1\n}\n\n\n\n\nShutting Pillar down\n\n\nShutting Pillar down when running from source\n\n\nIf you installed and ran Pillar from source using the command \n$GOPATH/bin/pillar\n, you can shut it down by using the Ctrl + C command.\n\n\nShutting Pillar down when running as a Docker container\n\n\nTo shut Pillar down when running as Docker container, first find the container ID (if you are running on a server, you may have to use \nsudo\n):\n\n\ndocker ps\n\n\n\n\nFind the container ID for Pillar:\n\n\nCONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS              PORTS                                                                                        NAMES\nb30f4fa5f497        coralproject/pillar      \n/bin/sh -c /go/bin/p\n   3 days ago          Up 3 days           0.0.0.0:32776-\n8080/tcp                                                                      proxy_pillarapp_1\n\n\n\n\nRun \ndocker stop\n with the container ID (if you are running on a server, you may have to use \nsudo\n):\n\n\ndocker stop b30f4fa5f497\n\n\n\n\nAPI Overview\n\n\n\n\n\n\nThe Pillar API adheres strongly to \nREST style\n.\n\n\n\n\n\n\nThe Pillar API works only with \nJSON\n data.\n\n\n\n\n\n\nThe regular \nCRUD\n endpoint URL pattern is \n/api/*\n, where as the URL pattern for import endpoints is \n/api/import/*\n.\n\n\n\n\n\n\nImport-related endpoints allow you to import data into Coral from an existing source system (i.e., an existing database of comment information).\n\n\n\n\nCoral keeps track of the original identifiers (i.e., the user id), and stores that data (using a structure called \nImportSource\n) in a field named \nSource\n. That means you won\nt lose the original identifier data from your original source when you import into Coral.\n\n\n\n\n\n\n\n\nAll import endpoints \nupsert\n data. This means that when you import an entry, it will overwrite the information for that entry if the entry already exists. This prevents duplications and other problems.\n\n\n\n\n\n\nCRUD endpoints\n\n\n\n\n\n\n\n\nURL\n\n\nHTTP Verb\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n/api/user\n\n\nGET\n\n\nQueries users\n\n\n\n\n\n\n/api/user\n\n\nPOST\n\n\nCreates and updates users\n\n\n\n\n\n\n/api/user\n\n\nPOST\n\n\nDeletes users\n\n\n\n\n\n\n/api/asset\n\n\nGET\n\n\nQueries assets\n\n\n\n\n\n\n/api/asset\n\n\nPOST\n\n\nCreates and updates assets\n\n\n\n\n\n\n/api/asset\n\n\nPOST\n\n\nDeletes assets\n\n\n\n\n\n\n/api/actions\n\n\nGET\n\n\nQueries actions\n\n\n\n\n\n\n/api/actions\n\n\nPOST\n\n\nCreates and updates actions\n\n\n\n\n\n\n/api/actions\n\n\nPOST\n\n\nDeletes actions\n\n\n\n\n\n\n\n\nImport endpoints\n\n\n\n\n\n\n\n\nURL\n\n\nHTTP Verb\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n/api/import/user\n\n\nGET\n\n\nImport users\n\n\n\n\n\n\n/api/import/asset\n\n\nGET\n\n\nImport assets\n\n\n\n\n\n\n/api//import/actions\n\n\nGET\n\n\nImport actions\n\n\n\n\n\n\n\n\nGet Users\n\n\n\n\n\n\n\n\nURL\n\n\nHTTP Verb\n\n\nFunctionality\n\n\n\n\n\n\n\n\n\n\n/api/user\n\n\nGET\n\n\nQueries users\n\n\n\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nName\n\n\nRequired?\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nY\n\n\nstring\n\n\n\n\n\n\n\n\nAvatar\n\n\n\n\nstring\n\n\n\n\n\n\n\n\nStatus\n\n\n\n\nstatus\n\n\n\n\n\n\n\n\n\n\nExample Call\n\n\nYou can retrieve multiple objects at once by sending a GET request to the class URL. Without any URL parameters, this simply lists objects in the class:\n\n\n curl -i -H \nAccept: application/json\n -XPOST -d '  {\n    \nname\n : \nIamSam\n,\n    \navatar\n : \nhttps://wpidentity.s3.amazonaws.com/assets/images/avatar-default.png\n,\n    \nstatus\n : \nNew\n,\n    \nsource\n : {\n      \nid\n:\noriginal-id-for-iam-sam\n\n    },\n    \ntags\n : [\ntop_commentor\n, \npowerball\n]\n  }\n' http://localhost:8080/api/import/user\n\n\n\n\nResponse\n\n\nThe return value is a JSON object that contains a results field with a JSON array that lists the objects.\n\n\n{\n  \nresults\n: [\n    {\n      \nplayerName\n: \nJang Min Chul\n,\n      \nupdatedAt\n: \n2011-08-19T02:24:17.787Z\n,\n      \ncheatMode\n: false,\n      \ncreatedAt\n: \n2011-08-19T02:24:17.787Z\n,\n      \nobjectId\n: \nA22v5zRAgd\n,\n      \nscore\n: 80075\n    },\n    {\n      \nplayerName\n: \nSean Plott\n,\n      \nupdatedAt\n: \n2011-08-21T18:02:52.248Z\n,\n      \ncheatMode\n: false,\n      \ncreatedAt\n: \n2011-08-20T02:06:57.931Z\n,\n      \nobjectId\n: \nEd1nuqPvcm\n,\n      \nscore\n: 73453\n    }\n  ]\n}\n\n\n\n\nCreate and Update Users\n\n\n\n\n\n\n\n\nURL\n\n\nHTTP Verb\n\n\nFunctionality\n\n\n\n\n\n\n\n\n\n\n/api/user\n\n\nGET\n\n\nQueries users\n\n\n\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nName\n\n\nRequired?\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nY\n\n\nstring\n\n\n\n\n\n\n\n\nAvatar\n\n\n\n\nstring\n\n\n\n\n\n\n\n\nStatus\n\n\n\n\nstatus\n\n\n\n\n\n\n\n\nLastLogin\n\n\n\n\ntime.Time\n\n\n\n\n\n\n\n\nMemberSince\n\n\nY\n\n\ntime.Time\n\n\n\n\n\n\n\n\nActions\n\n\n\n\n[]Action\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\n[]Note\n\n\n\n\n\n\n\n\nTags\n\n\n\n\n[]string\n\n\n\n\n\n\n\n\nStats\n\n\n\n\nbson.M\n\n\n\n\n\n\n\n\nMetadata\n\n\n\n\nbson.M\n\n\n\n\n\n\n\n\nSource\n\n\n\n\n*ImportSource\n\n\n\n\n\n\n\n\n\n\nExample Call\n\n\nYou can retrieve multiple objects at once by sending a GET request to the class URL. Without any URL parameters, this simply lists objects in the class:\n\n\n curl -i -H \nAccept: application/json\n -XPOST -d '  {\n    \nname\n : \nIamSam\n,\n    \navatar\n : \nhttps://wpidentity.s3.amazonaws.com/assets/images/avatar-default.png\n,\n    \nstatus\n : \nNew\n,\n    \nsource\n : {\n      \nid\n:\noriginal-id-for-iam-sam\n\n    },\n    \ntags\n : [\ntop_commentor\n, \npowerball\n]\n  }\n' http://localhost:8080/api/import/user\n\n\n\n\nResponse\n\n\nThe return value is a status response.\n\n\nStatus: 200 OK", 
            "title": "Introduction"
        }, 
        {
            "location": "/pillar/#introduction", 
            "text": "Pillar  is a REST based web service written in Golang. It provides the following services:   Imports external data into Coral data model  Allows CRUD operation on Coral data model  Provides simple queries on Coral data model", 
            "title": "Introduction"
        }, 
        {
            "location": "/pillar/#key-points", 
            "text": "Pillar APIs strongly adhere to  REST style .    Pillar APIs only work with  JSON  data.    There are two broad types of API endpoints:   The URL pattern for the regular  CRUD  endpoints is  /api/* .  The URL pattern for the import endpoints is  /api/import/* .     The  import  API endpoints allow you to import data into Coral from an existing source data store.   The key to successful importing and tracking lies in the  ImportSource  structure. This structure keeps the original identifiers.  Most top-level models (for example,  User  or  Comment ) embed this source data in a field named  Source .     We understand that an import process can be challenging. Hence, all of the  import  API endpoints  upsert  data. This means that each time you perform an import, existing entries are overwritten by the new entries ( upsert  basically means  insert or update ).", 
            "title": "Key Points"
        }, 
        {
            "location": "/pillar/#pillar-installation", 
            "text": "If you want to install Pillar as part of an all-in-one installation of the Coral Ecosystem, you can  find instructions to do that here .  When installing Pillar by itself, you can choose between installing Pillar as a Docker container, or installing from source.   Install Pillar from source  Install Pillar as a Docker container", 
            "title": "Pillar Installation"
        }, 
        {
            "location": "/pillar/#before-you-begin", 
            "text": "Before you install Pillar, you must have the following items installed and running:   MongoDB : You can find instructions on installing MongoDB  on the MongoDB website .  There are  instructions on importing sample comment data into MongoDB here    RabbitMQ : You can find instructions on installing RabbitMQ  on the RabbitMQ website .  Xenia : Xenia is a configurable service layer that publishes endpoints against mongo aggregation pipeline queries. It is part of the Coral ecosystem. You can find instructions on how to install Xenia  here .", 
            "title": "Before you begin"
        }, 
        {
            "location": "/pillar/#install-pillar-from-source", 
            "text": "", 
            "title": "Install Pillar from source"
        }, 
        {
            "location": "/pillar/#before-you-begin_1", 
            "text": "If you want to install from source, you will need to have Go installed.  You can install  install Go from their website . The  installation and setup instructions  on the Go website are quite good. Ensure that you have exported your $GOPATH environment variable, as detailed in the  installation instructions .  If you are not on a version of Go that is 1.7 or higher, you will also have to set the GO15VENDOREXPERIMENT flag.  export GO15VENDOREXPERIMENT=1  If you are not on a version of Go 1.7 or higher, we recommend adding this to your ~/.bash_profile or other startup script.", 
            "title": "Before you begin"
        }, 
        {
            "location": "/pillar/#get-the-source-code", 
            "text": "You can install the source code via using the  go get  command, or by manually cloning the code.", 
            "title": "Get the source code"
        }, 
        {
            "location": "/pillar/#using-the-go-get-command", 
            "text": "go get github.com/coralproject/pillar  If you see a message about  no buildable Go source files  as shown below, you can ignore it. It simply means that there are no buildable source files in the uppermost pillar directory (though there are buildable source files in subdirectories).  package github.com/coralproject/pillar: no buildable Go source files in [directory]", 
            "title": "Using the go get command"
        }, 
        {
            "location": "/pillar/#cloning-manually", 
            "text": "You can also clone the code manually.  mkdir $GOPATH/src/github.com/coralproject/pillar\ncd $GOPATH/src/github.com/coralproject/pillar\n\ngit clone https://github.com/coralproject/pillar.git", 
            "title": "Cloning manually"
        }, 
        {
            "location": "/pillar/#set-your-environment-variables", 
            "text": "Setting your environment variables tells Pillar the URLs and other information for communicating with MongoDB, RabbitMQ, and Xenia.  Make your own copy of the  config/dev.cfg  file (you can edit this configuration file with your own values, and then ensure that you don t commit it back to the repository). Call your config file whatever you like; we ll call it  custom  in this example.  cd $GOPATH/src/github.com/coralproject/pillar\ncp config/dev.cfg config/custom.cfg  Now edit the values in your custom.cfg file:  #Resources\nexport MONGODB_URL= mongodb://localhost:27017/coral \nexport AMQP_URL= amqp://localhost:5672/ \nexport AMQP_EXCHANGE= PillarMQ \n\n#Pillar\nexport PILLAR_ADDRESS= :8080 \nexport PILLAR_HOME= /opt/pillar \nexport PILLAR_CRON= false \nexport PILLAR_CRON_SEARCH= @every 30m \nexport PILLAR_CRON_STATS= @every 1m \n\n#Xenia\nexport XENIA_URL= http://localhost:4000/1.0/exec/ \nexport XENIA_QUERY_PARAM= ?skip=0 limit=100 \nexport XENIA_AUTH= auth token \n\n# Stats\nexport MONGODB_ADDRESS= 127.0.0.1:27017 \nexport MONGODB_USERNAME= \nexport MONGODB_PASSWORD= \nexport MONGODB_DATABASE= \nexport MONGODB_SSL= False    For  XENIA_URL :  If you installed  locally from source ,  XENIA_URL  should be  http://localhost:4000/1.0/exec/ .    For  MONGODB_URL :  If your MongoDB is a local installation,  MONGODB_URL  should be  mongodb://localhost:27017/coral .    For  AMQP_URL :  If your RabbitMQ is a local installation,  AMQP_URL  should be  amqp://localhost:5672/ .     Once you ve edited and saved your custom.cfg file, source it:  source $GOPATH/src/github.com/coralproject/pillar/config/custom.cfg", 
            "title": "Set your environment variables"
        }, 
        {
            "location": "/pillar/#run-pillar", 
            "text": "$GOPATH/bin/pillar  You should see:  [negroni] listening on :8080", 
            "title": "Run Pillar"
        }, 
        {
            "location": "/pillar/#install-as-docker-container", 
            "text": "", 
            "title": "Install as Docker Container"
        }, 
        {
            "location": "/pillar/#install-docker", 
            "text": "You can find more information on installing Docker on your machine  on the Docker website .   On the server, you can install Docker with the following command:   sudo yum install docker", 
            "title": "Install Docker"
        }, 
        {
            "location": "/pillar/#clone-the-pillar-repository", 
            "text": "Clone the Pillar repository:  git clone https://github.com/coralproject/pillar.git  Then cd into the Pillar directory.", 
            "title": "Clone the Pillar repository"
        }, 
        {
            "location": "/pillar/#start-docker", 
            "text": "Start Docker.   On the server, you can do this via the command:\n   sudo service docker start  On your local machine, you can start Docker via the Docker Quickstart Terminal. This will usually be in your Applications folder, or (if on Mac) you can type  docker quickstart  into Spotlight to find it quickly. The Docker Quickstart Terminal will open a new terminal window, running Docker, that you will then use to run the rest of the Docker related commands below.", 
            "title": "Start Docker"
        }, 
        {
            "location": "/pillar/#build-pillar-server", 
            "text": "Build the Pillar server using  docker build .  docker build -t pillar-server:0.1 .   If you are building on the server, you may have to use  sudo :   sudo docker build -t pillar-server:0.1 .", 
            "title": "Build Pillar server"
        }, 
        {
            "location": "/pillar/#edit-environment-variables", 
            "text": "The env.list file contains environment variables you need to set. Edit this file to reflect the settings on your own system.  # TODO: include finished env.list here   For  XENIA_URL :  If you installed  locally from source ,  XENIA_URL  should be  http://localhost:4000/1.0/exec/ .    For  MONGODB_URL :  If your MongoDB is a local installation,  MONGODB_URL  should be  mongodb://localhost:27017/coral .", 
            "title": "Edit environment variables"
        }, 
        {
            "location": "/pillar/#run-docker", 
            "text": "First, find the Image ID for the Pillar server:  docker images   If you are running on a server, you may have to use  sudo .   This shows you the Image ID:  REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\npillar-server       0.1                 24b7acf7a4b3        4 hours ago         771 MB\ngolang              1.6                 024309f28934        8 days ago          744.1 MB  Then run the docker run command with the Image ID:  docker run --env-file env.list --publish 8080:8080 24b7acf7a4b3  You should see the following:  [negroni] listening on :8080", 
            "title": "Run Docker"
        }, 
        {
            "location": "/pillar/#test-it-out", 
            "text": "To see if Pillar is working correctly, visit this url:  http://10.0.4.105:8080/about .  http://10.0.4.105  is the URL generated by Docker for Pillar.  If things are running properly, you should see this text:  { App : Coral Pillar Web Service , Version : Version - 0.0.1 }", 
            "title": "Test it out"
        }, 
        {
            "location": "/pillar/#shutting-pillar-down", 
            "text": "", 
            "title": "Shutting Pillar down"
        }, 
        {
            "location": "/pillar/#shutting-pillar-down-when-running-from-source", 
            "text": "If you installed and ran Pillar from source using the command  $GOPATH/bin/pillar , you can shut it down by using the Ctrl + C command.", 
            "title": "Shutting Pillar down when running from source"
        }, 
        {
            "location": "/pillar/#shutting-pillar-down-when-running-as-a-docker-container", 
            "text": "To shut Pillar down when running as Docker container, first find the container ID (if you are running on a server, you may have to use  sudo ):  docker ps  Find the container ID for Pillar:  CONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS              PORTS                                                                                        NAMES\nb30f4fa5f497        coralproject/pillar       /bin/sh -c /go/bin/p    3 days ago          Up 3 days           0.0.0.0:32776- 8080/tcp                                                                      proxy_pillarapp_1  Run  docker stop  with the container ID (if you are running on a server, you may have to use  sudo ):  docker stop b30f4fa5f497", 
            "title": "Shutting Pillar down when running as a Docker container"
        }, 
        {
            "location": "/pillar/#api-overview", 
            "text": "The Pillar API adheres strongly to  REST style .    The Pillar API works only with  JSON  data.    The regular  CRUD  endpoint URL pattern is  /api/* , where as the URL pattern for import endpoints is  /api/import/* .    Import-related endpoints allow you to import data into Coral from an existing source system (i.e., an existing database of comment information).   Coral keeps track of the original identifiers (i.e., the user id), and stores that data (using a structure called  ImportSource ) in a field named  Source . That means you won t lose the original identifier data from your original source when you import into Coral.     All import endpoints  upsert  data. This means that when you import an entry, it will overwrite the information for that entry if the entry already exists. This prevents duplications and other problems.", 
            "title": "API Overview"
        }, 
        {
            "location": "/pillar/#crud-endpoints", 
            "text": "URL  HTTP Verb  Description      /api/user  GET  Queries users    /api/user  POST  Creates and updates users    /api/user  POST  Deletes users    /api/asset  GET  Queries assets    /api/asset  POST  Creates and updates assets    /api/asset  POST  Deletes assets    /api/actions  GET  Queries actions    /api/actions  POST  Creates and updates actions    /api/actions  POST  Deletes actions", 
            "title": "CRUD endpoints"
        }, 
        {
            "location": "/pillar/#import-endpoints", 
            "text": "URL  HTTP Verb  Description      /api/import/user  GET  Import users    /api/import/asset  GET  Import assets    /api//import/actions  GET  Import actions", 
            "title": "Import endpoints"
        }, 
        {
            "location": "/pillar/#get-users", 
            "text": "URL  HTTP Verb  Functionality      /api/user  GET  Queries users", 
            "title": "Get Users"
        }, 
        {
            "location": "/pillar/#parameters", 
            "text": "Name  Required?  Type  Description      Name  Y  string     Avatar   string     Status   status", 
            "title": "Parameters"
        }, 
        {
            "location": "/pillar/#example-call", 
            "text": "You can retrieve multiple objects at once by sending a GET request to the class URL. Without any URL parameters, this simply lists objects in the class:   curl -i -H  Accept: application/json  -XPOST -d '  {\n     name  :  IamSam ,\n     avatar  :  https://wpidentity.s3.amazonaws.com/assets/images/avatar-default.png ,\n     status  :  New ,\n     source  : {\n       id : original-id-for-iam-sam \n    },\n     tags  : [ top_commentor ,  powerball ]\n  }\n' http://localhost:8080/api/import/user", 
            "title": "Example Call"
        }, 
        {
            "location": "/pillar/#response", 
            "text": "The return value is a JSON object that contains a results field with a JSON array that lists the objects.  {\n   results : [\n    {\n       playerName :  Jang Min Chul ,\n       updatedAt :  2011-08-19T02:24:17.787Z ,\n       cheatMode : false,\n       createdAt :  2011-08-19T02:24:17.787Z ,\n       objectId :  A22v5zRAgd ,\n       score : 80075\n    },\n    {\n       playerName :  Sean Plott ,\n       updatedAt :  2011-08-21T18:02:52.248Z ,\n       cheatMode : false,\n       createdAt :  2011-08-20T02:06:57.931Z ,\n       objectId :  Ed1nuqPvcm ,\n       score : 73453\n    }\n  ]\n}", 
            "title": "Response"
        }, 
        {
            "location": "/pillar/#create-and-update-users", 
            "text": "URL  HTTP Verb  Functionality      /api/user  GET  Queries users", 
            "title": "Create and Update Users"
        }, 
        {
            "location": "/pillar/#parameters_1", 
            "text": "Name  Required?  Type  Description      Name  Y  string     Avatar   string     Status   status     LastLogin   time.Time     MemberSince  Y  time.Time     Actions   []Action     Notes   []Note     Tags   []string     Stats   bson.M     Metadata   bson.M     Source   *ImportSource", 
            "title": "Parameters"
        }, 
        {
            "location": "/pillar/#example-call_1", 
            "text": "You can retrieve multiple objects at once by sending a GET request to the class URL. Without any URL parameters, this simply lists objects in the class:   curl -i -H  Accept: application/json  -XPOST -d '  {\n     name  :  IamSam ,\n     avatar  :  https://wpidentity.s3.amazonaws.com/assets/images/avatar-default.png ,\n     status  :  New ,\n     source  : {\n       id : original-id-for-iam-sam \n    },\n     tags  : [ top_commentor ,  powerball ]\n  }\n' http://localhost:8080/api/import/user", 
            "title": "Example Call"
        }, 
        {
            "location": "/pillar/#response_1", 
            "text": "The return value is a status response.  Status: 200 OK", 
            "title": "Response"
        }, 
        {
            "location": "/pillar/install/", 
            "text": "Pillar Installation\n\n\nIf you want to install Pillar as part of an all-in-one installation of the Coral Ecosystem, you can \nfind instructions to do that here\n.\n\n\nWhen installing Pillar by itself, you can choose between installing Pillar as a Docker container, or installing from source.\n\n\n\n\nInstall Pillar from source\n\n\nInstall Pillar as a Docker container\n\n\n\n\nBefore you begin\n\n\nBefore you install Pillar, you must have the following items installed and running:\n\n\n\n\nMongoDB\n: You can find instructions on installing MongoDB \non the MongoDB website\n.\n\n\nThere are \ninstructions on importing sample comment data into MongoDB here\n\n\n\n\n\n\nRabbitMQ\n: You can find instructions on installing RabbitMQ \non the RabbitMQ website\n.\n\n\nXenia\n: Xenia is a configurable service layer that publishes endpoints against mongo aggregation pipeline queries. It is part of the Coral ecosystem. You can find instructions on how to install Xenia \nhere\n.\n\n\n\n\nInstall Pillar from source\n\n\nBefore you begin\n\n\nIf you want to install from source, you will need to have Go installed.\n\n\nYou can install \ninstall Go from their website\n. The \ninstallation and setup instructions\n on the Go website are quite good. Ensure that you have exported your $GOPATH environment variable, as detailed in the \ninstallation instructions\n.\n\n\nIf you are not on a version of Go that is 1.7 or higher, you will also have to set the GO15VENDOREXPERIMENT flag.\n\n\nexport GO15VENDOREXPERIMENT=1\n\n\n\n\nIf you are not on a version of Go 1.7 or higher, we recommend adding this to your ~/.bash_profile or other startup script.\n\n\nGet the source code\n\n\nYou can install the source code via using the \ngo get\n command, or by manually cloning the code.\n\n\nUsing the go get command\n\n\ngo get github.com/coralproject/pillar\n\n\n\n\nIf you see a message about \nno buildable Go source files\n as shown below, you can ignore it. It simply means that there are no buildable source files in the uppermost pillar directory (though there are buildable source files in subdirectories).\n\n\npackage github.com/coralproject/pillar: no buildable Go source files in [directory]\n\n\n\n\nCloning manually\n\n\nYou can also clone the code manually.\n\n\nmkdir $GOPATH/src/github.com/coralproject/pillar\ncd $GOPATH/src/github.com/coralproject/pillar\n\ngit clone https://github.com/coralproject/pillar.git\n\n\n\n\nSet your environment variables\n\n\nSetting your environment variables tells Pillar the URLs and other information for communicating with MongoDB, RabbitMQ, and Xenia.\n\n\nMake your own copy of the \nconfig/dev.cfg\n file (you can edit this configuration file with your own values, and then ensure that you don\nt commit it back to the repository). Call your config file whatever you like; we\nll call it \ncustom\n in this example.\n\n\ncd $GOPATH/src/github.com/coralproject/pillar\ncp config/dev.cfg config/custom.cfg\n\n\n\n\nNow edit the values in your custom.cfg file:\n\n\n#Resources\nexport MONGODB_URL=\nmongodb://localhost:27017/coral\n\nexport AMQP_URL=\namqp://localhost:5672/\n\nexport AMQP_EXCHANGE=\nPillarMQ\n\n\n#Pillar\nexport PILLAR_ADDRESS=\n:8080\n\nexport PILLAR_HOME=\n/opt/pillar\n\nexport PILLAR_CRON=\nfalse\n\nexport PILLAR_CRON_SEARCH=\n@every 30m\n\nexport PILLAR_CRON_STATS=\n@every 1m\n\n\n#Xenia\nexport XENIA_URL=\nhttp://localhost:4000/1.0/exec/\n\nexport XENIA_QUERY_PARAM=\n?skip=0\nlimit=100\n\nexport XENIA_AUTH=\nauth token\n\n\n# Stats\nexport MONGODB_ADDRESS=\n127.0.0.1:27017\n\nexport MONGODB_USERNAME=\n\nexport MONGODB_PASSWORD=\n\nexport MONGODB_DATABASE=\n\nexport MONGODB_SSL=\nFalse\n\n\n\n\n\n\n\nFor \nXENIA_URL\n:\n\n\nIf you installed \nlocally from source\n, \nXENIA_URL\n should be \nhttp://localhost:4000/1.0/exec/\n.\n\n\n\n\n\n\nFor \nMONGODB_URL\n:\n\n\nIf your MongoDB is a local installation, \nMONGODB_URL\n should be \nmongodb://localhost:27017/coral\n.\n\n\n\n\n\n\nFor \nAMQP_URL\n:\n\n\nIf your RabbitMQ is a local installation, \nAMQP_URL\n should be \namqp://localhost:5672/\n.\n\n\n\n\n\n\n\n\nOnce you\nve edited and saved your custom.cfg file, source it:\n\n\nsource $GOPATH/src/github.com/coralproject/pillar/config/custom.cfg\n\n\n\n\nRun Pillar\n\n\n$GOPATH/bin/pillar\n\n\n\n\nYou should see:\n\n\n[negroni] listening on :8080\n\n\n\n\nInstall as Docker Container\n\n\nInstall Docker\n\n\nYou can find more information on installing Docker on your machine \non the Docker website\n.\n\n\n\n\nOn the server, you can install Docker with the following command:\n\n\n\n\nsudo yum install docker\n\n\n\n\nClone the Pillar repository\n\n\nClone the Pillar repository:\n\n\ngit clone https://github.com/coralproject/pillar.git\n\n\n\n\nThen cd into the Pillar directory.\n\n\nStart Docker\n\n\nStart Docker.\n\n\n\n\nOn the server, you can do this via the command:\n  \nsudo service docker start\n\n\nOn your local machine, you can start Docker via the Docker Quickstart Terminal. This will usually be in your Applications folder, or (if on Mac) you can type \ndocker quickstart\n into Spotlight to find it quickly. The Docker Quickstart Terminal will open a new terminal window, running Docker, that you will then use to run the rest of the Docker related commands below.\n\n\n\n\nBuild Pillar server\n\n\nBuild the Pillar server using \ndocker build\n.\n\n\ndocker build -t pillar-server:0.1 .\n\n\n\n\n\n\nIf you are building on the server, you may have to use \nsudo\n:\n\n\n\n\nsudo docker build -t pillar-server:0.1 .\n\n\n\n\nEdit environment variables\n\n\nThe env.list file contains environment variables you need to set. Edit this file to reflect the settings on your own system.\n\n\n# TODO: include finished env.list here\n\n\n\n\n\n\nFor \nXENIA_URL\n:\n\n\nIf you installed \nlocally from source\n, \nXENIA_URL\n should be \nhttp://localhost:4000/1.0/exec/\n.\n\n\n\n\n\n\nFor \nMONGODB_URL\n:\n\n\nIf your MongoDB is a local installation, \nMONGODB_URL\n should be \nmongodb://localhost:27017/coral\n.\n\n\n\n\n\n\n\n\nRun Docker\n\n\nFirst, find the Image ID for the Pillar server:\n\n\ndocker images\n\n\n\n\n\n\nIf you are running on a server, you may have to use \nsudo\n.\n\n\n\n\nThis shows you the Image ID:\n\n\nREPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\npillar-server       0.1                 24b7acf7a4b3        4 hours ago         771 MB\ngolang              1.6                 024309f28934        8 days ago          744.1 MB\n\n\n\n\nThen run the docker run command with the Image ID:\n\n\ndocker run --env-file env.list --publish 8080:8080 24b7acf7a4b3\n\n\n\n\nYou should see the following:\n\n\n[negroni] listening on :8080\n\n\n\n\nTest it out\n\n\nTo see if Pillar is working correctly, visit this url: \nhttp://10.0.4.105:8080/about\n. \nhttp://10.0.4.105\n is the URL generated by Docker for Pillar.\n\n\nIf things are running properly, you should see this text:\n\n\n{\nApp\n:\nCoral Pillar Web Service\n,\nVersion\n:\nVersion - 0.0.1\n}\n\n\n\n\nShutting Pillar down\n\n\nShutting Pillar down when running from source\n\n\nIf you installed and ran Pillar from source using the command \n$GOPATH/bin/pillar\n, you can shut it down by using the Ctrl + C command.\n\n\nShutting Pillar down when running as a Docker container\n\n\nTo shut Pillar down when running as Docker container, first find the container ID (if you are running on a server, you may have to use \nsudo\n):\n\n\ndocker ps\n\n\n\n\nFind the container ID for Pillar:\n\n\nCONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS              PORTS                                                                                        NAMES\nb30f4fa5f497        coralproject/pillar      \n/bin/sh -c /go/bin/p\n   3 days ago          Up 3 days           0.0.0.0:32776-\n8080/tcp                                                                      proxy_pillarapp_1\n\n\n\n\nRun \ndocker stop\n with the container ID (if you are running on a server, you may have to use \nsudo\n):\n\n\ndocker stop b30f4fa5f497", 
            "title": "Installation"
        }, 
        {
            "location": "/pillar/install/#pillar-installation", 
            "text": "If you want to install Pillar as part of an all-in-one installation of the Coral Ecosystem, you can  find instructions to do that here .  When installing Pillar by itself, you can choose between installing Pillar as a Docker container, or installing from source.   Install Pillar from source  Install Pillar as a Docker container", 
            "title": "Pillar Installation"
        }, 
        {
            "location": "/pillar/install/#before-you-begin", 
            "text": "Before you install Pillar, you must have the following items installed and running:   MongoDB : You can find instructions on installing MongoDB  on the MongoDB website .  There are  instructions on importing sample comment data into MongoDB here    RabbitMQ : You can find instructions on installing RabbitMQ  on the RabbitMQ website .  Xenia : Xenia is a configurable service layer that publishes endpoints against mongo aggregation pipeline queries. It is part of the Coral ecosystem. You can find instructions on how to install Xenia  here .", 
            "title": "Before you begin"
        }, 
        {
            "location": "/pillar/install/#install-pillar-from-source", 
            "text": "", 
            "title": "Install Pillar from source"
        }, 
        {
            "location": "/pillar/install/#before-you-begin_1", 
            "text": "If you want to install from source, you will need to have Go installed.  You can install  install Go from their website . The  installation and setup instructions  on the Go website are quite good. Ensure that you have exported your $GOPATH environment variable, as detailed in the  installation instructions .  If you are not on a version of Go that is 1.7 or higher, you will also have to set the GO15VENDOREXPERIMENT flag.  export GO15VENDOREXPERIMENT=1  If you are not on a version of Go 1.7 or higher, we recommend adding this to your ~/.bash_profile or other startup script.", 
            "title": "Before you begin"
        }, 
        {
            "location": "/pillar/install/#get-the-source-code", 
            "text": "You can install the source code via using the  go get  command, or by manually cloning the code.", 
            "title": "Get the source code"
        }, 
        {
            "location": "/pillar/install/#using-the-go-get-command", 
            "text": "go get github.com/coralproject/pillar  If you see a message about  no buildable Go source files  as shown below, you can ignore it. It simply means that there are no buildable source files in the uppermost pillar directory (though there are buildable source files in subdirectories).  package github.com/coralproject/pillar: no buildable Go source files in [directory]", 
            "title": "Using the go get command"
        }, 
        {
            "location": "/pillar/install/#cloning-manually", 
            "text": "You can also clone the code manually.  mkdir $GOPATH/src/github.com/coralproject/pillar\ncd $GOPATH/src/github.com/coralproject/pillar\n\ngit clone https://github.com/coralproject/pillar.git", 
            "title": "Cloning manually"
        }, 
        {
            "location": "/pillar/install/#set-your-environment-variables", 
            "text": "Setting your environment variables tells Pillar the URLs and other information for communicating with MongoDB, RabbitMQ, and Xenia.  Make your own copy of the  config/dev.cfg  file (you can edit this configuration file with your own values, and then ensure that you don t commit it back to the repository). Call your config file whatever you like; we ll call it  custom  in this example.  cd $GOPATH/src/github.com/coralproject/pillar\ncp config/dev.cfg config/custom.cfg  Now edit the values in your custom.cfg file:  #Resources\nexport MONGODB_URL= mongodb://localhost:27017/coral \nexport AMQP_URL= amqp://localhost:5672/ \nexport AMQP_EXCHANGE= PillarMQ \n\n#Pillar\nexport PILLAR_ADDRESS= :8080 \nexport PILLAR_HOME= /opt/pillar \nexport PILLAR_CRON= false \nexport PILLAR_CRON_SEARCH= @every 30m \nexport PILLAR_CRON_STATS= @every 1m \n\n#Xenia\nexport XENIA_URL= http://localhost:4000/1.0/exec/ \nexport XENIA_QUERY_PARAM= ?skip=0 limit=100 \nexport XENIA_AUTH= auth token \n\n# Stats\nexport MONGODB_ADDRESS= 127.0.0.1:27017 \nexport MONGODB_USERNAME= \nexport MONGODB_PASSWORD= \nexport MONGODB_DATABASE= \nexport MONGODB_SSL= False    For  XENIA_URL :  If you installed  locally from source ,  XENIA_URL  should be  http://localhost:4000/1.0/exec/ .    For  MONGODB_URL :  If your MongoDB is a local installation,  MONGODB_URL  should be  mongodb://localhost:27017/coral .    For  AMQP_URL :  If your RabbitMQ is a local installation,  AMQP_URL  should be  amqp://localhost:5672/ .     Once you ve edited and saved your custom.cfg file, source it:  source $GOPATH/src/github.com/coralproject/pillar/config/custom.cfg", 
            "title": "Set your environment variables"
        }, 
        {
            "location": "/pillar/install/#run-pillar", 
            "text": "$GOPATH/bin/pillar  You should see:  [negroni] listening on :8080", 
            "title": "Run Pillar"
        }, 
        {
            "location": "/pillar/install/#install-as-docker-container", 
            "text": "", 
            "title": "Install as Docker Container"
        }, 
        {
            "location": "/pillar/install/#install-docker", 
            "text": "You can find more information on installing Docker on your machine  on the Docker website .   On the server, you can install Docker with the following command:   sudo yum install docker", 
            "title": "Install Docker"
        }, 
        {
            "location": "/pillar/install/#clone-the-pillar-repository", 
            "text": "Clone the Pillar repository:  git clone https://github.com/coralproject/pillar.git  Then cd into the Pillar directory.", 
            "title": "Clone the Pillar repository"
        }, 
        {
            "location": "/pillar/install/#start-docker", 
            "text": "Start Docker.   On the server, you can do this via the command:\n   sudo service docker start  On your local machine, you can start Docker via the Docker Quickstart Terminal. This will usually be in your Applications folder, or (if on Mac) you can type  docker quickstart  into Spotlight to find it quickly. The Docker Quickstart Terminal will open a new terminal window, running Docker, that you will then use to run the rest of the Docker related commands below.", 
            "title": "Start Docker"
        }, 
        {
            "location": "/pillar/install/#build-pillar-server", 
            "text": "Build the Pillar server using  docker build .  docker build -t pillar-server:0.1 .   If you are building on the server, you may have to use  sudo :   sudo docker build -t pillar-server:0.1 .", 
            "title": "Build Pillar server"
        }, 
        {
            "location": "/pillar/install/#edit-environment-variables", 
            "text": "The env.list file contains environment variables you need to set. Edit this file to reflect the settings on your own system.  # TODO: include finished env.list here   For  XENIA_URL :  If you installed  locally from source ,  XENIA_URL  should be  http://localhost:4000/1.0/exec/ .    For  MONGODB_URL :  If your MongoDB is a local installation,  MONGODB_URL  should be  mongodb://localhost:27017/coral .", 
            "title": "Edit environment variables"
        }, 
        {
            "location": "/pillar/install/#run-docker", 
            "text": "First, find the Image ID for the Pillar server:  docker images   If you are running on a server, you may have to use  sudo .   This shows you the Image ID:  REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\npillar-server       0.1                 24b7acf7a4b3        4 hours ago         771 MB\ngolang              1.6                 024309f28934        8 days ago          744.1 MB  Then run the docker run command with the Image ID:  docker run --env-file env.list --publish 8080:8080 24b7acf7a4b3  You should see the following:  [negroni] listening on :8080", 
            "title": "Run Docker"
        }, 
        {
            "location": "/pillar/install/#test-it-out", 
            "text": "To see if Pillar is working correctly, visit this url:  http://10.0.4.105:8080/about .  http://10.0.4.105  is the URL generated by Docker for Pillar.  If things are running properly, you should see this text:  { App : Coral Pillar Web Service , Version : Version - 0.0.1 }", 
            "title": "Test it out"
        }, 
        {
            "location": "/pillar/install/#shutting-pillar-down", 
            "text": "", 
            "title": "Shutting Pillar down"
        }, 
        {
            "location": "/pillar/install/#shutting-pillar-down-when-running-from-source", 
            "text": "If you installed and ran Pillar from source using the command  $GOPATH/bin/pillar , you can shut it down by using the Ctrl + C command.", 
            "title": "Shutting Pillar down when running from source"
        }, 
        {
            "location": "/pillar/install/#shutting-pillar-down-when-running-as-a-docker-container", 
            "text": "To shut Pillar down when running as Docker container, first find the container ID (if you are running on a server, you may have to use  sudo ):  docker ps  Find the container ID for Pillar:  CONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS              PORTS                                                                                        NAMES\nb30f4fa5f497        coralproject/pillar       /bin/sh -c /go/bin/p    3 days ago          Up 3 days           0.0.0.0:32776- 8080/tcp                                                                      proxy_pillarapp_1  Run  docker stop  with the container ID (if you are running on a server, you may have to use  sudo ):  docker stop b30f4fa5f497", 
            "title": "Shutting Pillar down when running as a Docker container"
        }, 
        {
            "location": "/pillar/api/", 
            "text": "API Overview\n\n\n\n\n\n\nThe Pillar API adheres strongly to \nREST style\n.\n\n\n\n\n\n\nThe Pillar API works only with \nJSON\n data.\n\n\n\n\n\n\nThe regular \nCRUD\n endpoint URL pattern is \n/api/*\n, where as the URL pattern for import endpoints is \n/api/import/*\n.\n\n\n\n\n\n\nImport-related endpoints allow you to import data into Coral from an existing source system (i.e., an existing database of comment information).\n\n\n\n\nCoral keeps track of the original identifiers (i.e., the user id), and stores that data (using a structure called \nImportSource\n) in a field named \nSource\n. That means you won\nt lose the original identifier data from your original source when you import into Coral.\n\n\n\n\n\n\n\n\nAll import endpoints \nupsert\n data. This means that when you import an entry, it will overwrite the information for that entry if the entry already exists. This prevents duplications and other problems.\n\n\n\n\n\n\nCRUD endpoints\n\n\n\n\n\n\n\n\nURL\n\n\nHTTP Verb\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n/api/user\n\n\nGET\n\n\nQueries users\n\n\n\n\n\n\n/api/user\n\n\nPOST\n\n\nCreates and updates users\n\n\n\n\n\n\n/api/user\n\n\nPOST\n\n\nDeletes users\n\n\n\n\n\n\n/api/asset\n\n\nGET\n\n\nQueries assets\n\n\n\n\n\n\n/api/asset\n\n\nPOST\n\n\nCreates and updates assets\n\n\n\n\n\n\n/api/asset\n\n\nPOST\n\n\nDeletes assets\n\n\n\n\n\n\n/api/actions\n\n\nGET\n\n\nQueries actions\n\n\n\n\n\n\n/api/actions\n\n\nPOST\n\n\nCreates and updates actions\n\n\n\n\n\n\n/api/actions\n\n\nPOST\n\n\nDeletes actions\n\n\n\n\n\n\n\n\nImport endpoints\n\n\n\n\n\n\n\n\nURL\n\n\nHTTP Verb\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n/api/import/user\n\n\nGET\n\n\nImport users\n\n\n\n\n\n\n/api/import/asset\n\n\nGET\n\n\nImport assets\n\n\n\n\n\n\n/api//import/actions\n\n\nGET\n\n\nImport actions\n\n\n\n\n\n\n\n\nGet Users\n\n\n\n\n\n\n\n\nURL\n\n\nHTTP Verb\n\n\nFunctionality\n\n\n\n\n\n\n\n\n\n\n/api/user\n\n\nGET\n\n\nQueries users\n\n\n\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nName\n\n\nRequired?\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nY\n\n\nstring\n\n\n\n\n\n\n\n\nAvatar\n\n\n\n\nstring\n\n\n\n\n\n\n\n\nStatus\n\n\n\n\nstatus\n\n\n\n\n\n\n\n\n\n\nExample Call\n\n\nYou can retrieve multiple objects at once by sending a GET request to the class URL. Without any URL parameters, this simply lists objects in the class:\n\n\n curl -i -H \nAccept: application/json\n -XPOST -d '  {\n    \nname\n : \nIamSam\n,\n    \navatar\n : \nhttps://wpidentity.s3.amazonaws.com/assets/images/avatar-default.png\n,\n    \nstatus\n : \nNew\n,\n    \nsource\n : {\n      \nid\n:\noriginal-id-for-iam-sam\n\n    },\n    \ntags\n : [\ntop_commentor\n, \npowerball\n]\n  }\n' http://localhost:8080/api/import/user\n\n\n\n\nResponse\n\n\nThe return value is a JSON object that contains a results field with a JSON array that lists the objects.\n\n\n{\n  \nresults\n: [\n    {\n      \nplayerName\n: \nJang Min Chul\n,\n      \nupdatedAt\n: \n2011-08-19T02:24:17.787Z\n,\n      \ncheatMode\n: false,\n      \ncreatedAt\n: \n2011-08-19T02:24:17.787Z\n,\n      \nobjectId\n: \nA22v5zRAgd\n,\n      \nscore\n: 80075\n    },\n    {\n      \nplayerName\n: \nSean Plott\n,\n      \nupdatedAt\n: \n2011-08-21T18:02:52.248Z\n,\n      \ncheatMode\n: false,\n      \ncreatedAt\n: \n2011-08-20T02:06:57.931Z\n,\n      \nobjectId\n: \nEd1nuqPvcm\n,\n      \nscore\n: 73453\n    }\n  ]\n}\n\n\n\n\nCreate and Update Users\n\n\n\n\n\n\n\n\nURL\n\n\nHTTP Verb\n\n\nFunctionality\n\n\n\n\n\n\n\n\n\n\n/api/user\n\n\nGET\n\n\nQueries users\n\n\n\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nName\n\n\nRequired?\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nY\n\n\nstring\n\n\n\n\n\n\n\n\nAvatar\n\n\n\n\nstring\n\n\n\n\n\n\n\n\nStatus\n\n\n\n\nstatus\n\n\n\n\n\n\n\n\nLastLogin\n\n\n\n\ntime.Time\n\n\n\n\n\n\n\n\nMemberSince\n\n\nY\n\n\ntime.Time\n\n\n\n\n\n\n\n\nActions\n\n\n\n\n[]Action\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\n[]Note\n\n\n\n\n\n\n\n\nTags\n\n\n\n\n[]string\n\n\n\n\n\n\n\n\nStats\n\n\n\n\nbson.M\n\n\n\n\n\n\n\n\nMetadata\n\n\n\n\nbson.M\n\n\n\n\n\n\n\n\nSource\n\n\n\n\n*ImportSource\n\n\n\n\n\n\n\n\n\n\nExample Call\n\n\nYou can retrieve multiple objects at once by sending a GET request to the class URL. Without any URL parameters, this simply lists objects in the class:\n\n\n curl -i -H \nAccept: application/json\n -XPOST -d '  {\n    \nname\n : \nIamSam\n,\n    \navatar\n : \nhttps://wpidentity.s3.amazonaws.com/assets/images/avatar-default.png\n,\n    \nstatus\n : \nNew\n,\n    \nsource\n : {\n      \nid\n:\noriginal-id-for-iam-sam\n\n    },\n    \ntags\n : [\ntop_commentor\n, \npowerball\n]\n  }\n' http://localhost:8080/api/import/user\n\n\n\n\nResponse\n\n\nThe return value is a status response.\n\n\nStatus: 200 OK", 
            "title": "API"
        }, 
        {
            "location": "/pillar/api/#api-overview", 
            "text": "The Pillar API adheres strongly to  REST style .    The Pillar API works only with  JSON  data.    The regular  CRUD  endpoint URL pattern is  /api/* , where as the URL pattern for import endpoints is  /api/import/* .    Import-related endpoints allow you to import data into Coral from an existing source system (i.e., an existing database of comment information).   Coral keeps track of the original identifiers (i.e., the user id), and stores that data (using a structure called  ImportSource ) in a field named  Source . That means you won t lose the original identifier data from your original source when you import into Coral.     All import endpoints  upsert  data. This means that when you import an entry, it will overwrite the information for that entry if the entry already exists. This prevents duplications and other problems.", 
            "title": "API Overview"
        }, 
        {
            "location": "/pillar/api/#crud-endpoints", 
            "text": "URL  HTTP Verb  Description      /api/user  GET  Queries users    /api/user  POST  Creates and updates users    /api/user  POST  Deletes users    /api/asset  GET  Queries assets    /api/asset  POST  Creates and updates assets    /api/asset  POST  Deletes assets    /api/actions  GET  Queries actions    /api/actions  POST  Creates and updates actions    /api/actions  POST  Deletes actions", 
            "title": "CRUD endpoints"
        }, 
        {
            "location": "/pillar/api/#import-endpoints", 
            "text": "URL  HTTP Verb  Description      /api/import/user  GET  Import users    /api/import/asset  GET  Import assets    /api//import/actions  GET  Import actions", 
            "title": "Import endpoints"
        }, 
        {
            "location": "/pillar/api/#get-users", 
            "text": "URL  HTTP Verb  Functionality      /api/user  GET  Queries users", 
            "title": "Get Users"
        }, 
        {
            "location": "/pillar/api/#parameters", 
            "text": "Name  Required?  Type  Description      Name  Y  string     Avatar   string     Status   status", 
            "title": "Parameters"
        }, 
        {
            "location": "/pillar/api/#example-call", 
            "text": "You can retrieve multiple objects at once by sending a GET request to the class URL. Without any URL parameters, this simply lists objects in the class:   curl -i -H  Accept: application/json  -XPOST -d '  {\n     name  :  IamSam ,\n     avatar  :  https://wpidentity.s3.amazonaws.com/assets/images/avatar-default.png ,\n     status  :  New ,\n     source  : {\n       id : original-id-for-iam-sam \n    },\n     tags  : [ top_commentor ,  powerball ]\n  }\n' http://localhost:8080/api/import/user", 
            "title": "Example Call"
        }, 
        {
            "location": "/pillar/api/#response", 
            "text": "The return value is a JSON object that contains a results field with a JSON array that lists the objects.  {\n   results : [\n    {\n       playerName :  Jang Min Chul ,\n       updatedAt :  2011-08-19T02:24:17.787Z ,\n       cheatMode : false,\n       createdAt :  2011-08-19T02:24:17.787Z ,\n       objectId :  A22v5zRAgd ,\n       score : 80075\n    },\n    {\n       playerName :  Sean Plott ,\n       updatedAt :  2011-08-21T18:02:52.248Z ,\n       cheatMode : false,\n       createdAt :  2011-08-20T02:06:57.931Z ,\n       objectId :  Ed1nuqPvcm ,\n       score : 73453\n    }\n  ]\n}", 
            "title": "Response"
        }, 
        {
            "location": "/pillar/api/#create-and-update-users", 
            "text": "URL  HTTP Verb  Functionality      /api/user  GET  Queries users", 
            "title": "Create and Update Users"
        }, 
        {
            "location": "/pillar/api/#parameters_1", 
            "text": "Name  Required?  Type  Description      Name  Y  string     Avatar   string     Status   status     LastLogin   time.Time     MemberSince  Y  time.Time     Actions   []Action     Notes   []Note     Tags   []string     Stats   bson.M     Metadata   bson.M     Source   *ImportSource", 
            "title": "Parameters"
        }, 
        {
            "location": "/pillar/api/#example-call_1", 
            "text": "You can retrieve multiple objects at once by sending a GET request to the class URL. Without any URL parameters, this simply lists objects in the class:   curl -i -H  Accept: application/json  -XPOST -d '  {\n     name  :  IamSam ,\n     avatar  :  https://wpidentity.s3.amazonaws.com/assets/images/avatar-default.png ,\n     status  :  New ,\n     source  : {\n       id : original-id-for-iam-sam \n    },\n     tags  : [ top_commentor ,  powerball ]\n  }\n' http://localhost:8080/api/import/user", 
            "title": "Example Call"
        }, 
        {
            "location": "/pillar/api/#response_1", 
            "text": "The return value is a status response.  Status: 200 OK", 
            "title": "Response"
        }, 
        {
            "location": "/sponge/", 
            "text": "Introduction\n\n\nSponge is used to get your existing community (comments, authors, assets, and other entities) into the Coral ecosystem.\n\n\nSponge is Coral\ns Data Import Layer. It is an Extract, Transform, and Load command line tool designed to:\n\n\n\n\nRead data from a foreign source,\n\n\nTranslate the schema into Coral conventions, and\n\n\nPOST entities to our service layer for insertion.\n\n\n\n\nStrategy files\n are JSON files that are used to tell Sponge where to get the data, and how to translate it. You can read more about \nstrategy files here\n, including information on their structure and examples.\n\n\nData import sources supported\n\n\nSponge currently only supports importing data from mySQL, PostgreSQL, MongoDB or web services with REST APIs.\n\n\nCommand line tool\n\n\nUsage:\n\n\nsponge \nflag [command]\n\n\nAvailable Commands:\n\n\n\n\nimport      Import data to the coral database\n\n\nindex       Work with indexes in the coral database\n\n\nshow        Read the report on errors\n\n\nversion     Print the version number of Sponge\n\n\nall         Import and Create Indexes\n\n\n\n\nFlags\n\n\n      --dbname=\nreport.db\n: set the name for the db to read\n      --filepath=\nreport.db\n: set the file path for the report on errors (default is report.db)\n  -h, --help[=false]: help for sponge\n      --limit=9999999999: number of rows that we are going to import (default is 9999999999)\n      --offset=0: offset for rows to import (default is 0)\n      --onlyfails[=false]: import only the the records that failed in the last import(default is import all)\n      --orderby=\n: order by field on the external source (default is not ordered)\n      --query=\n: query on the external table (where condition on mysql, query document on mongodb). It only works with a specific --type. Example updated_date \n'2003-12-31 01:02:03'\n      --report[=false]: create report on records that fail importing (default is do not report)\n      --type=\n: import or create indexes for only these types of data (default is everything)\n\n\n\n\nRoadmap\n\n\nYou can find out more about potential upcoming features \non our roadmap\n.", 
            "title": "Introduction"
        }, 
        {
            "location": "/sponge/#introduction", 
            "text": "Sponge is used to get your existing community (comments, authors, assets, and other entities) into the Coral ecosystem.  Sponge is Coral s Data Import Layer. It is an Extract, Transform, and Load command line tool designed to:   Read data from a foreign source,  Translate the schema into Coral conventions, and  POST entities to our service layer for insertion.   Strategy files  are JSON files that are used to tell Sponge where to get the data, and how to translate it. You can read more about  strategy files here , including information on their structure and examples.", 
            "title": "Introduction"
        }, 
        {
            "location": "/sponge/#data-import-sources-supported", 
            "text": "Sponge currently only supports importing data from mySQL, PostgreSQL, MongoDB or web services with REST APIs.", 
            "title": "Data import sources supported"
        }, 
        {
            "location": "/sponge/#command-line-tool", 
            "text": "", 
            "title": "Command line tool"
        }, 
        {
            "location": "/sponge/#usage", 
            "text": "sponge  flag [command]", 
            "title": "Usage:"
        }, 
        {
            "location": "/sponge/#available-commands", 
            "text": "import      Import data to the coral database  index       Work with indexes in the coral database  show        Read the report on errors  version     Print the version number of Sponge  all         Import and Create Indexes", 
            "title": "Available Commands:"
        }, 
        {
            "location": "/sponge/#flags", 
            "text": "--dbname= report.db : set the name for the db to read\n      --filepath= report.db : set the file path for the report on errors (default is report.db)\n  -h, --help[=false]: help for sponge\n      --limit=9999999999: number of rows that we are going to import (default is 9999999999)\n      --offset=0: offset for rows to import (default is 0)\n      --onlyfails[=false]: import only the the records that failed in the last import(default is import all)\n      --orderby= : order by field on the external source (default is not ordered)\n      --query= : query on the external table (where condition on mysql, query document on mongodb). It only works with a specific --type. Example updated_date  '2003-12-31 01:02:03'\n      --report[=false]: create report on records that fail importing (default is do not report)\n      --type= : import or create indexes for only these types of data (default is everything)", 
            "title": "Flags"
        }, 
        {
            "location": "/sponge/#roadmap", 
            "text": "You can find out more about potential upcoming features  on our roadmap .", 
            "title": "Roadmap"
        }, 
        {
            "location": "/sponge/install/", 
            "text": "Installation\n\n\nBefore you begin\n\n\nPillar\n\n\nYou will need to have an instance of \nPillar\n running to send the data to. Instructions on installing Pillar \ncan be found here\n.\n\n\nExternal database source\n\n\nYou will also have your external database source running. This external database is the source of your existing comment data that will be extracted by Sponge and sent to Pillar, which will then load it into the Coral ecosystem.\n\n\nThe external sources we currently support are: PostgreSQL, MySQL, MongoDB, and REST APIs.\n\n\nVendoring dependencies\n\n\nYou should be vendoring the packages you choose to use. We recommend using \ngovendor\n. This tool will vendor from the vendor folder associated with this project repo for the dependencies in use. It is recommended to use a project based repo with a single vendor folder for all your dependencies.\n\n\nInstall from source\n\n\nBefore you begin\n\n\nIf you want to install from source, you will need to have Go installed.\n\n\nFirst \ninstall Go\n. The \ninstallation and setup instructions\n on the Go website are pretty good. Ensure that you have exported your $GOPATH environment variable, as detailed in the \ninstallation instructions\n.\n\n\nIf you are not on a version of Go that is 1.7 or higher, you will also have to set the GO15VENDOREXPERIMENT flag.\n\n\nexport GO15VENDOREXPERIMENT=1\n\n\n\n\nIf you are not on a version of Go 1.7 or higher, we recommend adding this to your ~/.bash_profile or other startup script.\n\n\nGet the source code\n\n\nYou can install the source code via using the \ngo get\n command, or by manually cloning the code.\n\n\nUsing the go get command\n\n\ngo get github.com/coralproject/sponge\n\n\n\n\nIf you see a message about \nno buildable Go source files\n like the below, you can ignore it. It simply means that there are buildable source files in subdirectories, just not the uppermost sponge directory.\n\n\npackage github.com/coralproject/sponge: no buildable Go source files in [directory]\n\n\n\n\nCloning manually\n\n\nYou can also clone the code manually.\n\n\nmkdir $GOPATH/src/github.com/coralproject/sponge\ncd $GOPATH/src/github.com/coralproject/sponge\n\ngit clone git@github.com:CoralProject/sponge.git\n\n\n\n\nSet up your strategy.json file\n\n\nYou can read about \nstrategy files in depth here\n.\n\n\nThe strategy.json file tells Sponge how to do the transformation between the publisher\ns existing data and the Coral data schema. It also tells us how to connect to the external publisher\ns source data. We currently support the following sources: PostgreSQL, MySQL, MongoDB, and REST APIs.\n\n\nWe have example strategy.json files for each of those source types. You can see those example strategy.json files in the \nexamples\n folder: \n$GOPATH/src/github.com/coralproject/sponge/examples\n\n\nTo copy one of the example strategy.json files to another folder, where you can then customize it:\n\n\ncp $GOPATH/src/github.com/coralproject/sponge/examples/strategy.json.example $GOPATH/src/github.com/coralproject/sponge/strategy/strategy.json\n\n\n\n\nSet your environment variables\n\n\nSetting your environment variables tells sponge which strategy file you want to use, and which \nPillar\n instance you are pushing data into.\n\n\nMake your own copy of the \nconfig/dev.cfg\n file (you can edit this configuration file with your own values, and then ensure that you don\nt commit it back to the repository). Call your config file whatever you like; we\nll call it \ncustom\n in this example.\n\n\ncd $GOPATH/src/github.com/coralproject/sponge\ncp config/dev.cfg config/custom.cfg\n\n\n\n\nNow edit the values in your custom.cfg file:\n\n\nexport STRATEGY_CONF=/path/to/my/strategy.json\nexport PILLAR_URL=http://localhost:8080\n\n\n\n\n\n\nThe \nSTRATEGY_CONF\n variable specifies the path to your strategy.json file.\n\n\nThe \nPILLAR_URL\n variable specifies the URL where your Pillar instance is running. If you installed Pillar locally from source, this will probably be \nhttp://localhost:8080\n.\n\n\n\n\nOnce you\nve edited and saved your custom.cfg file, source it:\n\n\nsource $GOPATH/src/github.com/coralproject/sponge/config/custom.cfg\n\n\n\n\nRun Sponge\n\n\nYou can either run Sponge using Go, or via a CLI tool.\n\n\nRunning Sponge using go run\n\n\ncd $GOPATH/src/github.com/coralproject/sponge/cmd/sponge\ngo run main.go\n\n\n\n\nRunning Sponge using the CLI tool\n\n\nFirst build the CLI tool:\n\n\ncd $GOPATH/src/github.com/coralproject/cmd/cmd/sponge\ngo build\n\n\n\n\nThen run the CLI tool\n\n\n./sponge -h\n\n\n\n\nInstall as a Docker container\n\n\nBuilding image\n\n\nTo build the docker image run this command:\n\n\ndocker build -t \nsponge:latest\n -f Dockerfile ./\n\n\n\n\nEdit env.list\n\n\nThe env.list file contains environment variables you need to set. Setting your environment variables tells sponge which strategy file you want to use, and which \nPillar\n instance you are pushing data into.\n\n\nPILLAR_URL=http://192.168.99.100:8080\nSTRATEGY_CONF=/strategy/strategy_psql.json\n\n# DATABASE\n# (optional if you want to overwrite strategy file values)\nDB_database= \n\nDB_username= \n\nDB_password= \n\nDB_host= \n\nDB_port= \n\n\n# WEB SERVICE\n# (optional if you want to overwrite strategy file values)\nWS_appkey= \n\nWS_endpoint= \n\nWS_records= \n\nWS_pagination= \n\nWS_useragent= \n\nWS_attributes= \n\n\n\n\n\n\n\nThe \nSTRATEGY_CONF\n variable specifies the path to your strategy.json file.\n\n\nThe \nPILLAR_URL\n variable specifies the URL where your Pillar instance is running. If you installed Pillar as a Docker container, this will probably be \nhttp://192.168.99.100:8080\n.\n\n\n\n\nRunning the container\n\n\nIt will start importing everything setup in the \nstrategy file\n.\n\n\ndocker run --env-file env.list -d sponge", 
            "title": "Installation"
        }, 
        {
            "location": "/sponge/install/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/sponge/install/#before-you-begin", 
            "text": "", 
            "title": "Before you begin"
        }, 
        {
            "location": "/sponge/install/#pillar", 
            "text": "You will need to have an instance of  Pillar  running to send the data to. Instructions on installing Pillar  can be found here .", 
            "title": "Pillar"
        }, 
        {
            "location": "/sponge/install/#external-database-source", 
            "text": "You will also have your external database source running. This external database is the source of your existing comment data that will be extracted by Sponge and sent to Pillar, which will then load it into the Coral ecosystem.  The external sources we currently support are: PostgreSQL, MySQL, MongoDB, and REST APIs.", 
            "title": "External database source"
        }, 
        {
            "location": "/sponge/install/#vendoring-dependencies", 
            "text": "You should be vendoring the packages you choose to use. We recommend using  govendor . This tool will vendor from the vendor folder associated with this project repo for the dependencies in use. It is recommended to use a project based repo with a single vendor folder for all your dependencies.", 
            "title": "Vendoring dependencies"
        }, 
        {
            "location": "/sponge/install/#install-from-source", 
            "text": "", 
            "title": "Install from source"
        }, 
        {
            "location": "/sponge/install/#before-you-begin_1", 
            "text": "If you want to install from source, you will need to have Go installed.  First  install Go . The  installation and setup instructions  on the Go website are pretty good. Ensure that you have exported your $GOPATH environment variable, as detailed in the  installation instructions .  If you are not on a version of Go that is 1.7 or higher, you will also have to set the GO15VENDOREXPERIMENT flag.  export GO15VENDOREXPERIMENT=1  If you are not on a version of Go 1.7 or higher, we recommend adding this to your ~/.bash_profile or other startup script.", 
            "title": "Before you begin"
        }, 
        {
            "location": "/sponge/install/#get-the-source-code", 
            "text": "You can install the source code via using the  go get  command, or by manually cloning the code.", 
            "title": "Get the source code"
        }, 
        {
            "location": "/sponge/install/#using-the-go-get-command", 
            "text": "go get github.com/coralproject/sponge  If you see a message about  no buildable Go source files  like the below, you can ignore it. It simply means that there are buildable source files in subdirectories, just not the uppermost sponge directory.  package github.com/coralproject/sponge: no buildable Go source files in [directory]", 
            "title": "Using the go get command"
        }, 
        {
            "location": "/sponge/install/#cloning-manually", 
            "text": "You can also clone the code manually.  mkdir $GOPATH/src/github.com/coralproject/sponge\ncd $GOPATH/src/github.com/coralproject/sponge\n\ngit clone git@github.com:CoralProject/sponge.git", 
            "title": "Cloning manually"
        }, 
        {
            "location": "/sponge/install/#set-up-your-strategyjson-file", 
            "text": "You can read about  strategy files in depth here .  The strategy.json file tells Sponge how to do the transformation between the publisher s existing data and the Coral data schema. It also tells us how to connect to the external publisher s source data. We currently support the following sources: PostgreSQL, MySQL, MongoDB, and REST APIs.  We have example strategy.json files for each of those source types. You can see those example strategy.json files in the  examples  folder:  $GOPATH/src/github.com/coralproject/sponge/examples  To copy one of the example strategy.json files to another folder, where you can then customize it:  cp $GOPATH/src/github.com/coralproject/sponge/examples/strategy.json.example $GOPATH/src/github.com/coralproject/sponge/strategy/strategy.json", 
            "title": "Set up your strategy.json file"
        }, 
        {
            "location": "/sponge/install/#set-your-environment-variables", 
            "text": "Setting your environment variables tells sponge which strategy file you want to use, and which  Pillar  instance you are pushing data into.  Make your own copy of the  config/dev.cfg  file (you can edit this configuration file with your own values, and then ensure that you don t commit it back to the repository). Call your config file whatever you like; we ll call it  custom  in this example.  cd $GOPATH/src/github.com/coralproject/sponge\ncp config/dev.cfg config/custom.cfg  Now edit the values in your custom.cfg file:  export STRATEGY_CONF=/path/to/my/strategy.json\nexport PILLAR_URL=http://localhost:8080   The  STRATEGY_CONF  variable specifies the path to your strategy.json file.  The  PILLAR_URL  variable specifies the URL where your Pillar instance is running. If you installed Pillar locally from source, this will probably be  http://localhost:8080 .   Once you ve edited and saved your custom.cfg file, source it:  source $GOPATH/src/github.com/coralproject/sponge/config/custom.cfg", 
            "title": "Set your environment variables"
        }, 
        {
            "location": "/sponge/install/#run-sponge", 
            "text": "You can either run Sponge using Go, or via a CLI tool.", 
            "title": "Run Sponge"
        }, 
        {
            "location": "/sponge/install/#running-sponge-using-go-run", 
            "text": "cd $GOPATH/src/github.com/coralproject/sponge/cmd/sponge\ngo run main.go", 
            "title": "Running Sponge using go run"
        }, 
        {
            "location": "/sponge/install/#running-sponge-using-the-cli-tool", 
            "text": "First build the CLI tool:  cd $GOPATH/src/github.com/coralproject/cmd/cmd/sponge\ngo build  Then run the CLI tool  ./sponge -h", 
            "title": "Running Sponge using the CLI tool"
        }, 
        {
            "location": "/sponge/install/#install-as-a-docker-container", 
            "text": "", 
            "title": "Install as a Docker container"
        }, 
        {
            "location": "/sponge/install/#building-image", 
            "text": "To build the docker image run this command:  docker build -t  sponge:latest  -f Dockerfile ./", 
            "title": "Building image"
        }, 
        {
            "location": "/sponge/install/#edit-envlist", 
            "text": "The env.list file contains environment variables you need to set. Setting your environment variables tells sponge which strategy file you want to use, and which  Pillar  instance you are pushing data into.  PILLAR_URL=http://192.168.99.100:8080\nSTRATEGY_CONF=/strategy/strategy_psql.json\n\n# DATABASE\n# (optional if you want to overwrite strategy file values)\nDB_database=  \nDB_username=  \nDB_password=  \nDB_host=  \nDB_port=  \n\n# WEB SERVICE\n# (optional if you want to overwrite strategy file values)\nWS_appkey=  \nWS_endpoint=  \nWS_records=  \nWS_pagination=  \nWS_useragent=  \nWS_attributes=     The  STRATEGY_CONF  variable specifies the path to your strategy.json file.  The  PILLAR_URL  variable specifies the URL where your Pillar instance is running. If you installed Pillar as a Docker container, this will probably be  http://192.168.99.100:8080 .", 
            "title": "Edit env.list"
        }, 
        {
            "location": "/sponge/install/#running-the-container", 
            "text": "It will start importing everything setup in the  strategy file .  docker run --env-file env.list -d sponge", 
            "title": "Running the container"
        }, 
        {
            "location": "/sponge/included_packages/", 
            "text": "Packages included in Sponge\n\n\n\n\nStrategy\n reads the translations file.\n\n\nSource\n does the extraction.\n\n\nFiddler\n does the transformations.\n\n\nCoral\n send data to Pillar.\n\n\nSponge\n ties all the pieces together.\n\n\n\n\nStrategy\n\n\nimport \ngithub.com/coralproject/sponge/pkg/strategy\n\n\n\n\n\nThe Strategy package handles the loading and distribution of configuration related with external sources. It handles the translation from the external database to our Coral schema.\n\n\nVariables\n\n\nvar pillarURL string\n\n\n\n\nURL of where Pillar is currently running.\n\n\nvar uuid      string\n\n\n\n\nUniversally Unique Identifier used for the logs.\n\n\nFields in the strategy JSON file\n\n\nName\n\n\nThe name of the strategy that we are describing.\n\n\nMap\n\n\nContains all the information to map fields from the external source to our local Coral database.\n\n\nForeign\n\n\nIt describes the foreign database source. Right now our supported data sources are mySQL, PostgreSQL, MongoDB, and web services (REST APIs).\n\n\nDateTimeFormat\n\n\nIf your source has date/time fields, we need to know how to parse them. In order to tell Sponge how to parse your date/time field, you will write out the date and time \n2006 Mon Jan 2 15:04:05\n in the format that it appears in your source database.\n\n\nFor more information on date and time layouts you can read the Golang documentation on \npkg-constants\n.\n\n\nEntities\n\n\nEntities describes all of the different entities we have in the Coral database and how to perform the transformations.\n\n\nExample of an entity:\n\n\nasset\n: {\n  \nForeign\n: \ncrnr_asset\n,\n  \nLocal\n: \nasset\n,\n  \nPriority\n: 1,\n  \nOrderBy\n: \nassetid\n,\n  \nID\n: \nassetid\n,\n  \nIndex\n: [\n    {\n      \nname\n: \nasset-url\n,\n      \nkey\n: \nasseturl\n,\n      \nunique\n: \ntrue\n,\n      \ndropdups\n: \ntrue\n\n    }\n  ],\n  \nFields\n: [\n    {\n      \nforeign\n: \nassetid\n,\n      \nlocal\n: \nasset_id\n,\n      \nrelation\n: \nSource\n,\n      \ntype\n: \nint\n\n    },\n    {\n      \nforeign\n: \nasseturl\n,\n      \nlocal\n: \nurl\n,\n      \nrelation\n: \nPassthrough\n,\n      \ntype\n: \nint\n\n    },\n    {\n      \nforeign\n: \nupdatedate\n,\n      \nlocal\n: \ndate_updated\n,\n      \nrelation\n: \nParseTimeDate\n,\n      \ntype\n: \ndateTime\n\n    },\n    {\n      \nforeign\n: \ncreatedate\n,\n      \nlocal\n: \ndate_created\n,\n      \nrelation\n: \nParseTimeDate\n,\n      \ntype\n: \ndateTime\n\n    }\n  ],\n  \nEndpoint\n: \nhttp://localhost:8080/api/import/asset\n\n}\n\n\n\n\nForeign\n\n\nThe name of the foreign entity.\n\n\nLocal\n\n\nThe collection to where we are importing this entity into.\n\n\nPriority\n\n\nThis is a number that specifies which entity to import first. The first priority starts in Zero.\n\n\nOrderBy\n\n\nA default order by when quering the foreign source.\n\n\nID\n\n\nThe identifier field for the foreign entity. We use this field when we need to import only some records and not the whole entity.\n\n\nEndpoint\n\n\nThis is the endpoint in the coral system were we are going to push the data into.\n\n\nFields\n\n\nAll the fields that are being mapped.\n\n\nforeign\n\n\nThe name of the field in the foreign entity.\n\n\nlocal\n\n\nThe name of the field in our local database.\n\n\nrelation\n\n\nThe relationship between the foreign field and the local one. We have the following options:\n\n\n\n\nPassthrough: when the value is the same\n\n\nSource: when it needs to be added to our source struct for the local collection (the original identifiers have to go into source)\n\n\nParseTimeDate: when we need to parse the foreign value as date time.\n\n\nConstant: when the local field should always be the same value. In this case we will have \nforeign\n blank and we will have other field called \nvalue\n with the value of the local field.\n\n\nSubDocument: when the local field has an array of documents in one of the fields.\n\n\nStatus: when the field need to be translated based on the status map that is declared in that same strategy file for the entity.\n\n\n\n\ntype\n\n\nThe type of the value we are converting. This can be one of the following:\n\n\n\n\nString\n\n\nTimedate\n\n\n\n\nCredentials\n\n\nThis has the credentials for the source database. It could be a web service or a database (MySQL, PostgreSQL or MongoDB).\n\n\nadapter\n\n\nThis tells us which driver we need to use to pull data. Right now, the available options are \nmysql\n, \npostgresql\n, \nmongodb\n or \nservice\n.\n\n\ntype\n\n\nThis tells us which type of credential this is. Right now, it is always \nforeign\n.\n\n\nSource\n\n\nimport \ngithub.com/coralproject/sponge/pkg/source\n\n\n\n\n\nThe package \nSource\n has the drivers to connect to the external source and retrieve data. The credentials for the external source are setup in the strategy file.\n\n\nVariables\n\n\nvar strategy str.Strategy\n\n\n\n\nHolds the credentials for the external source as well as all the entities that needs to be extracted.\n\n\nvar uuid     string\n\n\n\n\nUniversally Unique Identifier used for the logs.\n\n\nvar credential str.Credential\n\n\n\n\nCredential for the external source (database or web service).\n\n\nInterface Sourcer\n\n\nThis is the interface that needs to be implemented by any driver to databases that want to be included.\n\n\nfunc GetData\n\n\nGetData(string, *Options) ([]map[string]interface{}, error)\n\n\n\n\nReturns all the data (query by options in Options) in the format []map[string]interface{}\n\n\nfunc IsWebService\n\n\nIsWebService() bool\n\n\n\n\nReturns true if the implementation of sourcer is a web service.\n\n\nfunc New\n\n\nfunc New(d string) (Sourcer, error)\n\n\n\n\nDepending on the parameter, it returns a structure with the connection to the external source that implements the interface Sourcer.\n\n\nfunc GetEntities\n\n\nfunc GetEntities() ([]string, error)\n\n\n\n\nGets all the entities\n names from the source\n\n\nfunc GetforeignEntity\n\n\nfunc GetForeignEntity(name string) string\n\n\n\n\nGets the foreign entity\ns name for the Coral collection.\n\n\nDriver mySQL\n\n\nStructure mySQL that implements interface Sourcer. It gets data from a mySQL database.\n\n\nDriver PostgreSQL\n\n\nStructure PostgreSQL that implements interface Sourcer. It gets data from a PostgreSQL database.\n\n\nDriver MongoDB\n\n\nStructure to hold a connection to a MongoDB that implements interface Sourcer. It gets data from the MongoDB database.\n\n\nAPI\n\n\nStructure the way to get data from a web service that implements interface Sourcer.\n\n\nHow to add a new source\n\n\nIf you need to add a new external source, you can implement the Sourcer interface over a structure with the connection to the database.\n\n\nFiddler\n\n\nimport \ngithub.com/coralproject/sponge/pkg/fiddler\n\n\n\n\n\nThe Fiddler package does the translation from the external database schema into Coral\ns database, through a translation file called Strategy.\n\n\nVariables\n\n\nvar (\n    strategy   str.Strategy\n)\n\n\n\n\nHolds the translation to apply to the data.\n\n\nvar (\n    dateLayout string\n)\n\n\n\n\nDate Layout as specified in the translation\ns file.\n\n\nvar (\n    uuid       string\n)\n\n\n\n\nUniversally Unique Identifier used for the logs.\n\n\nFunctions\n\n\nfunc GetID\n\n\nfunc GetID(modelName string) string\n\n\n\n\nReturns the field that is the identifier for that model\n\n\nfunc GetCollections\n\n\nfunc GetCollections() []string\n\n\n\n\nReturns the names of all the collections in the strategy file.\n\n\nfunc TransformRow\n\n\nfunc TransformRow(row map[string]interface{}, coralName string) (interface{}, []map[string]interface{}, error)\n\n\n\n\nApplies the coral schema to a row of data from the external source.\n\n\nExamples\n\n\nTODO\n\n\n\n\nCoral\n\n\nNote\n: The \nCoral\n package is not to be confused with the Coral ecosystem as a whole. In this instance, this is merely the name of a package included in Sponge.\n\n\nimport \ngithub.com/coralproject/sponge/pkg/coral\n\n\n\n\n\nInteract with Pillar endpoints to import data into the Coral system.\n\n\nConstants\n\n\nconst (\n    retryTimes int    = 3\n    methodGet  string = \nGET\n\n    methodPost string = \nPOST\n\n  )\n\n\n\n\nretryTimes\n determines how many times to retry if it fails.\n\n\nVariables\n\n\nvar (\n    endpoints map[string]string // model -\n endpoint\n)\n\n\n\n\nendpoints\n is a map containing all of the services to send data to. Right now, that is only Pillar.\n\n\nvar (\n    uuid string\n    str  strategy.Strategy\n)\n\n\n\n\nuuid\n is the universal identifier we are using for logging.\n\nstr\n is the strategy configuration.\n\n\nFunctions\n\n\nfunc AddRow\n\n\nfunc AddRow(data map[string]interface{}, tableName string) error\n\n\n\n\nAdds data to the collection \ntableName\n.\n\n\nfunc CreateIndex\n\n\nfunc CreateIndex(collection string) error\n\n\n\n\nCalls the service to create index for \ncollection\n.\n\n\nSponge\n\n\nNote\n: The \nSponge\n package is not to be confused with the Sponge app as a whole. In this instance, this is merely the name of a package included in the larger Sponge app.\n\n\nimport \ngithub.com/coralproject/sponge/pkg/sponge\n\n\n\n\n\nThe Sponge package imports the external source database into the local database. It transforms it and sends it to the Coral system, through Pillar.\n\n\nConstants\n\n\nconst (\n    VersionNumber = 0.1\n)\n\n\n\n\nVersionNumber\n provides the version number of Sponge.\n\n\nVariables\n\n\nvar (\n    dbsource source.Sourcer\n    uuid     string\n    options  source.Options\n)\n\n\n\n\nFunctions\n\n\nfunc AddOptions\n\n\nfunc AddOptions(limit int, offset int, orderby string, query string, types string, importonlyfailed bool, reportOnFailedRecords bool, reportdbfile string)\n\n\n\n\nAddOptions\n sets options for how Sponge will run. The options are:\n\n\n*   Limit: limit for the query\n*   Offset: offset for the query\n*   Orderby:  order by this field\n*   Query:  we use this field if we want to specific a filter on WHERE for mySQL/PostgreSQL and Find for MongoDB\n*   Types: it specifies which entities to import (default is everything)\n*   Importonlyfailed: import only the documents that are in the report\n*   ReportOnFailedRecords: create a report with all the documents that failed the import\n*   Reportdbfile: name of the file for the report on documents that fail the import\n\n\n\nfunc Import\n\n\nfunc Import()\n\n\n\n\nGets data, transforms it and sends it to Pillar. It bases everything on the STRATEGY_CONF environment variable and the PILLAR_URL environment variable.\n\n\nfunc CreateIndex\n\n\nfunc CreateIndex(collection string)\n\n\n\n\nCreates index on the collection \ncollection\n. This feature creates indexes on the Coral database, depending on data in the strategy file.\n\n\nFor example:\n\n\nIndex\n: [\n  {\n    \nname\n: \nasset-url\n,\n    \nkeys\n: [\nasseturl\n],\n    \nunique\n: \ntrue\n,\n    \ndropdups\n: \ntrue\n\n  }\n],\n\n\n\n\nYou can read more information at the \nmongodb\ns create index definition\n.", 
            "title": "Included packages"
        }, 
        {
            "location": "/sponge/included_packages/#packages-included-in-sponge", 
            "text": "Strategy  reads the translations file.  Source  does the extraction.  Fiddler  does the transformations.  Coral  send data to Pillar.  Sponge  ties all the pieces together.", 
            "title": "Packages included in Sponge"
        }, 
        {
            "location": "/sponge/included_packages/#strategy", 
            "text": "import  github.com/coralproject/sponge/pkg/strategy   The Strategy package handles the loading and distribution of configuration related with external sources. It handles the translation from the external database to our Coral schema.", 
            "title": "Strategy"
        }, 
        {
            "location": "/sponge/included_packages/#variables", 
            "text": "var pillarURL string  URL of where Pillar is currently running.  var uuid      string  Universally Unique Identifier used for the logs.", 
            "title": "Variables"
        }, 
        {
            "location": "/sponge/included_packages/#fields-in-the-strategy-json-file", 
            "text": "", 
            "title": "Fields in the strategy JSON file"
        }, 
        {
            "location": "/sponge/included_packages/#name", 
            "text": "The name of the strategy that we are describing.", 
            "title": "Name"
        }, 
        {
            "location": "/sponge/included_packages/#map", 
            "text": "Contains all the information to map fields from the external source to our local Coral database.", 
            "title": "Map"
        }, 
        {
            "location": "/sponge/included_packages/#foreign", 
            "text": "It describes the foreign database source. Right now our supported data sources are mySQL, PostgreSQL, MongoDB, and web services (REST APIs).", 
            "title": "Foreign"
        }, 
        {
            "location": "/sponge/included_packages/#datetimeformat", 
            "text": "If your source has date/time fields, we need to know how to parse them. In order to tell Sponge how to parse your date/time field, you will write out the date and time  2006 Mon Jan 2 15:04:05  in the format that it appears in your source database.  For more information on date and time layouts you can read the Golang documentation on  pkg-constants .", 
            "title": "DateTimeFormat"
        }, 
        {
            "location": "/sponge/included_packages/#entities", 
            "text": "Entities describes all of the different entities we have in the Coral database and how to perform the transformations.  Example of an entity:  asset : {\n   Foreign :  crnr_asset ,\n   Local :  asset ,\n   Priority : 1,\n   OrderBy :  assetid ,\n   ID :  assetid ,\n   Index : [\n    {\n       name :  asset-url ,\n       key :  asseturl ,\n       unique :  true ,\n       dropdups :  true \n    }\n  ],\n   Fields : [\n    {\n       foreign :  assetid ,\n       local :  asset_id ,\n       relation :  Source ,\n       type :  int \n    },\n    {\n       foreign :  asseturl ,\n       local :  url ,\n       relation :  Passthrough ,\n       type :  int \n    },\n    {\n       foreign :  updatedate ,\n       local :  date_updated ,\n       relation :  ParseTimeDate ,\n       type :  dateTime \n    },\n    {\n       foreign :  createdate ,\n       local :  date_created ,\n       relation :  ParseTimeDate ,\n       type :  dateTime \n    }\n  ],\n   Endpoint :  http://localhost:8080/api/import/asset \n}", 
            "title": "Entities"
        }, 
        {
            "location": "/sponge/included_packages/#foreign_1", 
            "text": "The name of the foreign entity.", 
            "title": "Foreign"
        }, 
        {
            "location": "/sponge/included_packages/#local", 
            "text": "The collection to where we are importing this entity into.", 
            "title": "Local"
        }, 
        {
            "location": "/sponge/included_packages/#priority", 
            "text": "This is a number that specifies which entity to import first. The first priority starts in Zero.", 
            "title": "Priority"
        }, 
        {
            "location": "/sponge/included_packages/#orderby", 
            "text": "A default order by when quering the foreign source.", 
            "title": "OrderBy"
        }, 
        {
            "location": "/sponge/included_packages/#id", 
            "text": "The identifier field for the foreign entity. We use this field when we need to import only some records and not the whole entity.", 
            "title": "ID"
        }, 
        {
            "location": "/sponge/included_packages/#endpoint", 
            "text": "This is the endpoint in the coral system were we are going to push the data into.", 
            "title": "Endpoint"
        }, 
        {
            "location": "/sponge/included_packages/#fields", 
            "text": "All the fields that are being mapped.", 
            "title": "Fields"
        }, 
        {
            "location": "/sponge/included_packages/#foreign_2", 
            "text": "The name of the field in the foreign entity.", 
            "title": "foreign"
        }, 
        {
            "location": "/sponge/included_packages/#local_1", 
            "text": "The name of the field in our local database.", 
            "title": "local"
        }, 
        {
            "location": "/sponge/included_packages/#relation", 
            "text": "The relationship between the foreign field and the local one. We have the following options:   Passthrough: when the value is the same  Source: when it needs to be added to our source struct for the local collection (the original identifiers have to go into source)  ParseTimeDate: when we need to parse the foreign value as date time.  Constant: when the local field should always be the same value. In this case we will have  foreign  blank and we will have other field called  value  with the value of the local field.  SubDocument: when the local field has an array of documents in one of the fields.  Status: when the field need to be translated based on the status map that is declared in that same strategy file for the entity.", 
            "title": "relation"
        }, 
        {
            "location": "/sponge/included_packages/#type", 
            "text": "The type of the value we are converting. This can be one of the following:   String  Timedate", 
            "title": "type"
        }, 
        {
            "location": "/sponge/included_packages/#credentials", 
            "text": "This has the credentials for the source database. It could be a web service or a database (MySQL, PostgreSQL or MongoDB).", 
            "title": "Credentials"
        }, 
        {
            "location": "/sponge/included_packages/#adapter", 
            "text": "This tells us which driver we need to use to pull data. Right now, the available options are  mysql ,  postgresql ,  mongodb  or  service .", 
            "title": "adapter"
        }, 
        {
            "location": "/sponge/included_packages/#type_1", 
            "text": "This tells us which type of credential this is. Right now, it is always  foreign .", 
            "title": "type"
        }, 
        {
            "location": "/sponge/included_packages/#source", 
            "text": "import  github.com/coralproject/sponge/pkg/source   The package  Source  has the drivers to connect to the external source and retrieve data. The credentials for the external source are setup in the strategy file.", 
            "title": "Source"
        }, 
        {
            "location": "/sponge/included_packages/#variables_1", 
            "text": "var strategy str.Strategy  Holds the credentials for the external source as well as all the entities that needs to be extracted.  var uuid     string  Universally Unique Identifier used for the logs.  var credential str.Credential  Credential for the external source (database or web service).", 
            "title": "Variables"
        }, 
        {
            "location": "/sponge/included_packages/#interface-sourcer", 
            "text": "This is the interface that needs to be implemented by any driver to databases that want to be included.", 
            "title": "Interface Sourcer"
        }, 
        {
            "location": "/sponge/included_packages/#func-getdata", 
            "text": "GetData(string, *Options) ([]map[string]interface{}, error)  Returns all the data (query by options in Options) in the format []map[string]interface{}", 
            "title": "func GetData"
        }, 
        {
            "location": "/sponge/included_packages/#func-iswebservice", 
            "text": "IsWebService() bool  Returns true if the implementation of sourcer is a web service.", 
            "title": "func IsWebService"
        }, 
        {
            "location": "/sponge/included_packages/#func-new", 
            "text": "func New(d string) (Sourcer, error)  Depending on the parameter, it returns a structure with the connection to the external source that implements the interface Sourcer.", 
            "title": "func New"
        }, 
        {
            "location": "/sponge/included_packages/#func-getentities", 
            "text": "func GetEntities() ([]string, error)  Gets all the entities  names from the source", 
            "title": "func GetEntities"
        }, 
        {
            "location": "/sponge/included_packages/#func-getforeignentity", 
            "text": "func GetForeignEntity(name string) string  Gets the foreign entity s name for the Coral collection.", 
            "title": "func GetforeignEntity"
        }, 
        {
            "location": "/sponge/included_packages/#driver-mysql", 
            "text": "Structure mySQL that implements interface Sourcer. It gets data from a mySQL database.", 
            "title": "Driver mySQL"
        }, 
        {
            "location": "/sponge/included_packages/#driver-postgresql", 
            "text": "Structure PostgreSQL that implements interface Sourcer. It gets data from a PostgreSQL database.", 
            "title": "Driver PostgreSQL"
        }, 
        {
            "location": "/sponge/included_packages/#driver-mongodb", 
            "text": "Structure to hold a connection to a MongoDB that implements interface Sourcer. It gets data from the MongoDB database.", 
            "title": "Driver MongoDB"
        }, 
        {
            "location": "/sponge/included_packages/#api", 
            "text": "Structure the way to get data from a web service that implements interface Sourcer.", 
            "title": "API"
        }, 
        {
            "location": "/sponge/included_packages/#how-to-add-a-new-source", 
            "text": "If you need to add a new external source, you can implement the Sourcer interface over a structure with the connection to the database.", 
            "title": "How to add a new source"
        }, 
        {
            "location": "/sponge/included_packages/#fiddler", 
            "text": "import  github.com/coralproject/sponge/pkg/fiddler   The Fiddler package does the translation from the external database schema into Coral s database, through a translation file called Strategy.", 
            "title": "Fiddler"
        }, 
        {
            "location": "/sponge/included_packages/#variables_2", 
            "text": "var (\n    strategy   str.Strategy\n)  Holds the translation to apply to the data.  var (\n    dateLayout string\n)  Date Layout as specified in the translation s file.  var (\n    uuid       string\n)  Universally Unique Identifier used for the logs.", 
            "title": "Variables"
        }, 
        {
            "location": "/sponge/included_packages/#functions", 
            "text": "", 
            "title": "Functions"
        }, 
        {
            "location": "/sponge/included_packages/#func-getid", 
            "text": "func GetID(modelName string) string  Returns the field that is the identifier for that model", 
            "title": "func GetID"
        }, 
        {
            "location": "/sponge/included_packages/#func-getcollections", 
            "text": "func GetCollections() []string  Returns the names of all the collections in the strategy file.", 
            "title": "func GetCollections"
        }, 
        {
            "location": "/sponge/included_packages/#func-transformrow", 
            "text": "func TransformRow(row map[string]interface{}, coralName string) (interface{}, []map[string]interface{}, error)  Applies the coral schema to a row of data from the external source.", 
            "title": "func TransformRow"
        }, 
        {
            "location": "/sponge/included_packages/#examples", 
            "text": "TODO", 
            "title": "Examples"
        }, 
        {
            "location": "/sponge/included_packages/#coral", 
            "text": "Note : The  Coral  package is not to be confused with the Coral ecosystem as a whole. In this instance, this is merely the name of a package included in Sponge.  import  github.com/coralproject/sponge/pkg/coral   Interact with Pillar endpoints to import data into the Coral system.", 
            "title": "Coral"
        }, 
        {
            "location": "/sponge/included_packages/#constants", 
            "text": "const (\n    retryTimes int    = 3\n    methodGet  string =  GET \n    methodPost string =  POST \n  )  retryTimes  determines how many times to retry if it fails.", 
            "title": "Constants"
        }, 
        {
            "location": "/sponge/included_packages/#variables_3", 
            "text": "var (\n    endpoints map[string]string // model -  endpoint\n)  endpoints  is a map containing all of the services to send data to. Right now, that is only Pillar.  var (\n    uuid string\n    str  strategy.Strategy\n)  uuid  is the universal identifier we are using for logging. str  is the strategy configuration.", 
            "title": "Variables"
        }, 
        {
            "location": "/sponge/included_packages/#functions_1", 
            "text": "", 
            "title": "Functions"
        }, 
        {
            "location": "/sponge/included_packages/#func-addrow", 
            "text": "func AddRow(data map[string]interface{}, tableName string) error  Adds data to the collection  tableName .", 
            "title": "func AddRow"
        }, 
        {
            "location": "/sponge/included_packages/#func-createindex", 
            "text": "func CreateIndex(collection string) error  Calls the service to create index for  collection .", 
            "title": "func CreateIndex"
        }, 
        {
            "location": "/sponge/included_packages/#sponge", 
            "text": "Note : The  Sponge  package is not to be confused with the Sponge app as a whole. In this instance, this is merely the name of a package included in the larger Sponge app.  import  github.com/coralproject/sponge/pkg/sponge   The Sponge package imports the external source database into the local database. It transforms it and sends it to the Coral system, through Pillar.", 
            "title": "Sponge"
        }, 
        {
            "location": "/sponge/included_packages/#constants_1", 
            "text": "const (\n    VersionNumber = 0.1\n)  VersionNumber  provides the version number of Sponge.", 
            "title": "Constants"
        }, 
        {
            "location": "/sponge/included_packages/#variables_4", 
            "text": "var (\n    dbsource source.Sourcer\n    uuid     string\n    options  source.Options\n)", 
            "title": "Variables"
        }, 
        {
            "location": "/sponge/included_packages/#functions_2", 
            "text": "", 
            "title": "Functions"
        }, 
        {
            "location": "/sponge/included_packages/#func-addoptions", 
            "text": "func AddOptions(limit int, offset int, orderby string, query string, types string, importonlyfailed bool, reportOnFailedRecords bool, reportdbfile string)  AddOptions  sets options for how Sponge will run. The options are:  *   Limit: limit for the query\n*   Offset: offset for the query\n*   Orderby:  order by this field\n*   Query:  we use this field if we want to specific a filter on WHERE for mySQL/PostgreSQL and Find for MongoDB\n*   Types: it specifies which entities to import (default is everything)\n*   Importonlyfailed: import only the documents that are in the report\n*   ReportOnFailedRecords: create a report with all the documents that failed the import\n*   Reportdbfile: name of the file for the report on documents that fail the import", 
            "title": "func AddOptions"
        }, 
        {
            "location": "/sponge/included_packages/#func-import", 
            "text": "func Import()  Gets data, transforms it and sends it to Pillar. It bases everything on the STRATEGY_CONF environment variable and the PILLAR_URL environment variable.", 
            "title": "func Import"
        }, 
        {
            "location": "/sponge/included_packages/#func-createindex_1", 
            "text": "func CreateIndex(collection string)  Creates index on the collection  collection . This feature creates indexes on the Coral database, depending on data in the strategy file.  For example:  Index : [\n  {\n     name :  asset-url ,\n     keys : [ asseturl ],\n     unique :  true ,\n     dropdups :  true \n  }\n],  You can read more information at the  mongodb s create index definition .", 
            "title": "func CreateIndex"
        }, 
        {
            "location": "/sponge/roadmap/", 
            "text": "Sponge Roadmap\n\n\nThis document tracks features that are not yet prioritized into issues and releases.\n\n\nAPI Import\n\n\nPull data from Http(s) sources\n\n\n\n\nDisqus - https://disqus.com/api/docs/\n\n\nWordpress Core - https://codex.wordpress.org/XML-RPC_WordPress_API/Comments\n\n\nLyvewire - http://answers.livefyre.com/developers/api-reference/\n\n\nFacebook - https://developers.facebook.com/docs/graph-api/reference/v2.5/comment\n\n\n\n\nRate Limit Counter\n\n\nIn order to protect source databases, we want to be able to throttle the number of queries sponge is making.\n\n\n\n\nKeeps a sliding count of how many requests were made in the past time frame based on the strategy.\n\n\nExposes isOkToQuery() to determine if we are currently at the limit.\n\n\nEach request sends a message to this routine each time a request is made.\n\n\n\n\nSynchronization\n\n\nAn internal mechanism that regularly polls the source and imports any updates.\n\n\nBasic polling specification\n\n\nThe main loop that keeps the data up to date.  Gets \nslices\n of records based on \nupdated_at\n timestamps to account for changing records.\n\n\nFor each table or api in the strategy:\n\n\n\n\nEnsure maximum rate limit is not met\n\n\nDetermine which slice of data to get next\n\n\nFind the last updated timestamp in the \nlog collection\n (or the collection itself?)\n\n\n\n\n\n\nUse the strategy to request the slice (either db query or api call)\n\n\nUpdate the rate limit counter\n\n\n\n\n\n\nFor each record returned\n\n\nCheck to ensure the document isn\nt already added\n\n\nIf not, add the document and kick off \nimport actions\n\n\nIf it\ns there, update the document\n\n\nUpdate the \nlog collection\n\n\n\n\n\n\n\n\nChallenges\n\n\nData synchronization between complex living systems is a difficult challenge. Approach with caution.", 
            "title": "Roadmap"
        }, 
        {
            "location": "/sponge/roadmap/#sponge-roadmap", 
            "text": "This document tracks features that are not yet prioritized into issues and releases.", 
            "title": "Sponge Roadmap"
        }, 
        {
            "location": "/sponge/roadmap/#api-import", 
            "text": "Pull data from Http(s) sources   Disqus - https://disqus.com/api/docs/  Wordpress Core - https://codex.wordpress.org/XML-RPC_WordPress_API/Comments  Lyvewire - http://answers.livefyre.com/developers/api-reference/  Facebook - https://developers.facebook.com/docs/graph-api/reference/v2.5/comment", 
            "title": "API Import"
        }, 
        {
            "location": "/sponge/roadmap/#rate-limit-counter", 
            "text": "In order to protect source databases, we want to be able to throttle the number of queries sponge is making.   Keeps a sliding count of how many requests were made in the past time frame based on the strategy.  Exposes isOkToQuery() to determine if we are currently at the limit.  Each request sends a message to this routine each time a request is made.", 
            "title": "Rate Limit Counter"
        }, 
        {
            "location": "/sponge/roadmap/#synchronization", 
            "text": "An internal mechanism that regularly polls the source and imports any updates.", 
            "title": "Synchronization"
        }, 
        {
            "location": "/sponge/roadmap/#basic-polling-specification", 
            "text": "The main loop that keeps the data up to date.  Gets  slices  of records based on  updated_at  timestamps to account for changing records.  For each table or api in the strategy:   Ensure maximum rate limit is not met  Determine which slice of data to get next  Find the last updated timestamp in the  log collection  (or the collection itself?)    Use the strategy to request the slice (either db query or api call)  Update the rate limit counter    For each record returned  Check to ensure the document isn t already added  If not, add the document and kick off  import actions  If it s there, update the document  Update the  log collection", 
            "title": "Basic polling specification"
        }, 
        {
            "location": "/sponge/roadmap/#challenges", 
            "text": "Data synchronization between complex living systems is a difficult challenge. Approach with caution.", 
            "title": "Challenges"
        }, 
        {
            "location": "/sponge/logging/", 
            "text": "About LOGGING\n\n\nWe are using (Ardanlabs Log\ns package)[https://github.com/ardanlabs/kit/tree/master/log] for all the tools we are developing in GO.\n\n\nSpec:\n\n\nLogging levels:\n\n\n* Dev: to be outputted in development environment only\n* User (prod): to be outputted in dev and production environments\n\n\n\nAll logs should contain:\n\n\n* context uuid to identify a particular execution (aka, run of Sponge or a Request/Response execution from a web server.)\n* the name of the function that is executing\n* a readable message including relevant data\n\n\n\nLogs should write to stdout so they can be flexibly directed.", 
            "title": "Logging"
        }, 
        {
            "location": "/sponge/logging/#about-logging", 
            "text": "We are using (Ardanlabs Log s package)[https://github.com/ardanlabs/kit/tree/master/log] for all the tools we are developing in GO.", 
            "title": "About LOGGING"
        }, 
        {
            "location": "/sponge/logging/#spec", 
            "text": "", 
            "title": "Spec:"
        }, 
        {
            "location": "/sponge/logging/#logging-levels", 
            "text": "* Dev: to be outputted in development environment only\n* User (prod): to be outputted in dev and production environments", 
            "title": "Logging levels:"
        }, 
        {
            "location": "/sponge/logging/#all-logs-should-contain", 
            "text": "* context uuid to identify a particular execution (aka, run of Sponge or a Request/Response execution from a web server.)\n* the name of the function that is executing\n* a readable message including relevant data  Logs should write to stdout so they can be flexibly directed.", 
            "title": "All logs should contain:"
        }, 
        {
            "location": "/xenia/", 
            "text": "Xenia\n\n\nXenia\n is a configurable service layer that publishes endpoints against mongo aggregation pipeline queries.\n\n\nConcepts and Motivations\n\n\nComposition\n\n\nAggregation pipelines are \nchain-able\n, allowing for the output of one endpoint to be fed into the next. Xenia will provide a request syntax to allow for this, giving the requesting application another dimension of flexibility via query control.\n\n\nSimilarly, output documents from multiple pipelines can be \nbundled\n together. This is particularly useful in the no-sql/document db paradigm in which joins are not natively supported.\n\n\nRestructuring of Team Dynamics\n\n\nXenia moves 100% of the query logic out of the application code. Front end devs, data analysis, and anyone else familiar with the simple, declarative mongo aggregation syntax can alter the service behavior. This removes the requirement for back end engineering and devops expertise from the process of refining the data requests.\n\n\nXenia\ns CLI tools allow anyone with a basic understanding of document database concepts and aggregation pipeline syntax to create or update endpoints.  (Once the web UI is complete updates to the pipelines will be even more convenient.)", 
            "title": "Introduction"
        }, 
        {
            "location": "/xenia/#xenia", 
            "text": "Xenia  is a configurable service layer that publishes endpoints against mongo aggregation pipeline queries.", 
            "title": "Xenia"
        }, 
        {
            "location": "/xenia/#concepts-and-motivations", 
            "text": "", 
            "title": "Concepts and Motivations"
        }, 
        {
            "location": "/xenia/#composition", 
            "text": "Aggregation pipelines are  chain-able , allowing for the output of one endpoint to be fed into the next. Xenia will provide a request syntax to allow for this, giving the requesting application another dimension of flexibility via query control.  Similarly, output documents from multiple pipelines can be  bundled  together. This is particularly useful in the no-sql/document db paradigm in which joins are not natively supported.", 
            "title": "Composition"
        }, 
        {
            "location": "/xenia/#restructuring-of-team-dynamics", 
            "text": "Xenia moves 100% of the query logic out of the application code. Front end devs, data analysis, and anyone else familiar with the simple, declarative mongo aggregation syntax can alter the service behavior. This removes the requirement for back end engineering and devops expertise from the process of refining the data requests.  Xenia s CLI tools allow anyone with a basic understanding of document database concepts and aggregation pipeline syntax to create or update endpoints.  (Once the web UI is complete updates to the pipelines will be even more convenient.)", 
            "title": "Restructuring of Team Dynamics"
        }, 
        {
            "location": "/xenia/install/", 
            "text": "Xenia Installation\n\n\nXenia is a configurable service layer that publishes endpoints against \nmongo aggregation pipeline queries\n.\n\n\nBefore you begin\n\n\nBefore you install Xenia, you will want to have the following installed.\n\n\nMongoDB\n\n\nYou can find instructions on installing MongoDB \non the MongoDB website\n.\n\n\nThere are \ninstructions on importing sample comment data into MongoDB here\n\n\nGo\n\n\nIf you want to install from source, you will need to have Go installed.\n\n\nYou can install \ninstall Go from their website\n. The \ninstallation and setup instructions\n on the Go website are quite good. Ensure that you have exported your $GOPATH environment variable, as detailed in the \ninstallation instructions\n.\n\n\nIf you are not on a version of Go that is 1.7 or higher, you will also have to set the GO15VENDOREXPERIMENT flag.\n\n\nexport GO15VENDOREXPERIMENT=1\n\n\n\n\nIf you are not on a version of Go 1.7 or higher, we recommend adding this to your ~/.bash_profile or other startup script.\n\n\nObtaining the source code\n\n\nYou can install the source code via using the \ngo get\n command, or by manually cloning the code.\n\n\nUsing the go get command\n\n\ngo get github.com/coralproject/xenia\n\n\n\n\nIf you see a message about \nno buildable Go source files\n like the below, you can ignore it. It simply means that there are buildable source files in subdirectories, just not the uppermost xenia directory.\n\n\npackage github.com/coralproject/xenia: no buildable Go source files in [directory]\n\n\n\n\nCloning manually\n\n\nYou can also clone the code manually.\n\n\nmkdir $GOPATH/src/github.com/coralproject/xenia\ncd $GOPATH/src/github.com/coralproject/xenia\n\ngit clone git@github.com:CoralProject/xenia.git\n\n\n\n\nSet up your environment variables\n\n\nThis tells Xenia which database you want to use, sets your port, and sets your database password.\n\n\nMake your own copy of the \nconfig/dev.cfg\n file (this edited cfg file will contain your own values for things like your database password, and will not be committed back to the repository if you are doing development work on Xenia). Call your config file whatever you like; we\nll call it \ncustom\n in this example.\n\n\ncd $GOPATH/src/github.com/coralproject/xenia\ncp config/dev.cfg config/custom.cfg\n\n\n\n\nEdit the environment variables to reflect your MongoDB setup.\n\nRemember, you\nre entering your password here, so be sure not to commit this file to the repository!\n\n\nexport XENIA_MONGO_HOST=localhost:27017\nexport XENIA_MONGO_USER=coral-user\nexport XENIA_MONGO_AUTHDB=coral\nexport XENIA_MONGO_DB=coral\n\nexport XENIA_LOGGING_LEVEL=1\nexport XENIA_HOST=:4000\n\n# Use to have the CLI tooling hit the web service.\nexport XENIA_WEB_HOST=\nexport XENIA_WEB_AUTH=\n\n# Set host to Anvil if configured.\n# export XENIA_ANVIL_HOST=https://HOST\n\n# Use to apply extra key:value pairs to the header\n# export XENIA_HEADERS=key:value,key:value\n\n# DO NOT PUSH TO REPO\nexport XENIA_MONGO_PASS=\n\n\n\n\nRequired edits:\n\n\n\n\nXENIA_MONGO_HOST\n: set to your MongoDB where you will be communicating with.\n\n\nIf you are running MongoDB locally on your machine, you should set this to \nlocalhost:27017\n.\n\n\nIf you are pointing to a MongoDB running on a server somewhere, set this to the IP address and port of your MongoDB.\n\n\n\n\n\n\nXENIA_MONGO_DB\n: the database you are running queries against (\ncoral\n).\n\n\nXENIA_MONGO_USER\n: your MongoDB username.\n\n\nXENIA_MONGO_PASS\n: the password for your MongoDB user.\n\n\nXENIA_MONGO_AUTHDB\n: the database you are authenticating against (in most cases, should be the same \ncoral\n database as XENIA_MONGO_DB).\n\n\n\n\nOptional edits:\n\n\n\n\nXENIA_WEB_HOST\n: this is required for the CLI tool. It is the address of the Xenia web service (i.e., an instance of Xenia that you are running on a server).\n\n\nIf you are running everything locally, comment this variable out. This means that the CLI tool will connect directly to your local database, instead of connecting to a running web service.\n\n\n\n\n\n\nXENIA_HOST\n: default is \n:4000\n if this variable is not set.\n\n\nXENIA_LOGGING_LEVEL\n: default is \n2\n if this variable is not set.\n\n\nXENIA_WEB_AUTH\n: your Anvil token, if you have Anvil authentication set up. If you do not have authentication set up, leave this commented out.\n\n\nXENIA_ANVIL_HOST\n: the URL to the Anvil host, if you have Anvil authentication set up. If you do not have authentication set up, leave this commented out.\n\n\nXENIA_HEADERS\n: leave this commented out.\n\n\n\n\nOnce you\nve finished editing, source your config file using the source command:\n\n\nsource $GOPATH/src/github.com/coralproject/xenia/config/custom.cfg\n\n\n\n\nBuild the CLI tool\n\n\nXenia has a CLI tool that allows you to manage endpoints and perform other actions.\n\n\nTo build to the tool:\n\n\ncd $GOPATH/src/github.com/coralproject/xenia/cmd/xenia\ngo build\n\n\n\n\nNote: It is best to run with logging level 0 when using the xenia command:\n\n\nexport XENIA_LOGGING_LEVEL=0\n\n\n\n\nCreating a Xenia database for the first time\n\n\nIf you are running Xenia on a MongoDB database for the first time you will need the Xenia command line tool to set up the MongoDB for use with Xenia. The CLI tool will create collections and sets of indexes that you can use to execute queries: a sort of dictionary of pre-made queries for you to use.\n\n\n1) First cd into cmd/xenia directory (this contains the CLI tool):\n\n\ncd $GOPATH/src/github.com/coralproject/xenia/cmd/xenia\n\n\n\n\n2) Configure the database using \ndb create\n. The database.json file contains the information necessary to create the collections and indexes.\n\n\n./xenia db create -f scrdb/database.json\n\n\n\n\nExpected output:\n\n\nConfiguring MongoDB\nCreating collection query_sets\nCreating collection query_sets_history\nCreating collection query_scripts\nCreating collection query_scripts_history\nCreating collection query_masks\nCreating collection query_masks_history\nCreating collection query_regexs\nCreating collection query_regexs_history\n\n\n\n\n\n\nTroubleshooting note #1\n: if you get a response that contains \nERROR: Invalid DB provided\n, you may have an incorrectly set environment variable. If you are running everything locally and using a local MongoDB, use \nprintev\n to see if \nXENIA_WEB_HOST\n is set:\n\n\n\n\nprintenv XENIA_WEB_HOST\n\n\n\n\nIf you are using a local MongoDB, this should not return a value. If it does return a value, unset the variable using \nunset\n, and then try step 2 again:\n\n\nunset XENIA_WEB_HOST\n\n\n\n\n\n\nTroubleshooting note #2\n: instead of the expected output shown above, you may see something like:\n\n\n\n\nConfiguring MongoDB\nCreating collection query_sets\nERROR: collection already exists\n\n\n\n\nThat\ns okay! It means that your database is already set up with Xenia. Perhaps you imported data that already had configured collections set up. Continue on to step 3.\n\n\n3) Load all the existing queries:\n\n\n./xenia query upsert -p scrquery\n\n\n\n\nExpected output:\n\n\nConfiguring MongoDB\nUpserting Set : Path[scrquery]\nUpserting Set : Upserted\n\n\n\n\n4) Load all the existing masks:\n\n\n./xenia mask upsert -p scrmask\n\n\n\n\nExpected output:\n\n\nConfiguring MongoDB\nUpserting Set : Path[scrquery]\nUpserting Set : Upserted\n\n\n\n\n5) Load all the existing regexes:\n\n\n./xenia regex upsert -p scrregex\n\n\n\n\nExpected output:\n\n\nConfiguring MongoDB\nUpserting Regex : Path[scrregex]\nUpserting Regex : Upserted\n\n\n\n\n6) That\ns it! If you are using MongoChef, you should be able to see your newly created collections, which will look something like this:\n\n\n\n\nRun the web service\n\n\n1) First cd into the directory containing the web service, xeniad (the Xenia daemon):\n\n\ncd $GOPATH/src/github.com/coralproject/xenia/cmd/xeniad\n\n\n\n\n2) Build the Xenia web service:\n\n\ngo build\n\n\n\n\n3) Run the web service:\n\n\n./xeniad\n\n\n\n\nExpected output:\n\n\n2016/06/08 10:26:38 app.go:173: USER : startup : Init :\n\nConfig Settings:\nMONGO_USER=coral-user\nMONGO_AUTHDB=coral\nMONGO_DB=coral\nHOST=:4000\nLOGGING_LEVEL=1\nMONGO_HOST=localhost:27017\n\n2016/06/08 10:26:38 main.go:24: USER : startup : Init : Revision     : \nunknown\n\n2016/06/08 10:26:38 main.go:25: USER : startup : Init : Version      : \nunknown\n\n2016/06/08 10:26:38 main.go:26: USER : startup : Init : Build Date   : \nunknown\n\n2016/06/08 10:26:38 main.go:27: USER : startup : Init : Int Version  : \n201606081030\n\n2016/06/08 10:26:38 main.go:28: USER : startup : Init : Go Version   : \ngo1.6.2\n\n2016/06/08 10:26:38 main.go:29: USER : startup : Init : Go Compiler  : \ngc\n\n2016/06/08 10:26:38 main.go:30: USER : startup : Init : Go ARCH      : \namd64\n\n2016/06/08 10:26:38 main.go:31: USER : startup : Init : Go OS        : \ndarwin\n\n\n\n\n\n4) You can test your web service by going to the following URL in your browser: \nhttp://localhost:4000/1.0/query\n.\n\n\nIn your browser, you will see some json displayed. In your terminal, you should see something like:\n\n\n2016/06/08 13:30:58 app.go:104: USER : 6bd28905-8a92-4aa9-80fd-5e9cff199b3e : Request : Started : Method[GET] URL[/1.0/query] RADDR[[::1]:62121]\n2016/06/08 13:30:58 context.go:65: USER : 6bd28905-8a92-4aa9-80fd-5e9cff199b3e : api : Respond : Started : Code[200]\n2016/06/08 13:30:58 context.go:72: USER : startup : api : Respond : Setting user headers : Access-Control-Allow-Origin:*\n2016/06/08 13:30:58 context.go:110: USER : 6bd28905-8a92-4aa9-80fd-5e9cff199b3e : api : Respond : Completed\n2016/06/08 13:30:58 app.go:126: USER : 6bd28905-8a92-4aa9-80fd-5e9cff199b3e : Request : Completed : Status[200] Duration[46.497305ms]\n\n\n\n\nTroubleshooting\n\n\n\n\nAuthorization\n\n\nTODO", 
            "title": "Installation"
        }, 
        {
            "location": "/xenia/install/#xenia-installation", 
            "text": "Xenia is a configurable service layer that publishes endpoints against  mongo aggregation pipeline queries .", 
            "title": "Xenia Installation"
        }, 
        {
            "location": "/xenia/install/#before-you-begin", 
            "text": "Before you install Xenia, you will want to have the following installed.", 
            "title": "Before you begin"
        }, 
        {
            "location": "/xenia/install/#mongodb", 
            "text": "You can find instructions on installing MongoDB  on the MongoDB website .  There are  instructions on importing sample comment data into MongoDB here", 
            "title": "MongoDB"
        }, 
        {
            "location": "/xenia/install/#go", 
            "text": "If you want to install from source, you will need to have Go installed.  You can install  install Go from their website . The  installation and setup instructions  on the Go website are quite good. Ensure that you have exported your $GOPATH environment variable, as detailed in the  installation instructions .  If you are not on a version of Go that is 1.7 or higher, you will also have to set the GO15VENDOREXPERIMENT flag.  export GO15VENDOREXPERIMENT=1  If you are not on a version of Go 1.7 or higher, we recommend adding this to your ~/.bash_profile or other startup script.", 
            "title": "Go"
        }, 
        {
            "location": "/xenia/install/#obtaining-the-source-code", 
            "text": "You can install the source code via using the  go get  command, or by manually cloning the code.", 
            "title": "Obtaining the source code"
        }, 
        {
            "location": "/xenia/install/#using-the-go-get-command", 
            "text": "go get github.com/coralproject/xenia  If you see a message about  no buildable Go source files  like the below, you can ignore it. It simply means that there are buildable source files in subdirectories, just not the uppermost xenia directory.  package github.com/coralproject/xenia: no buildable Go source files in [directory]", 
            "title": "Using the go get command"
        }, 
        {
            "location": "/xenia/install/#cloning-manually", 
            "text": "You can also clone the code manually.  mkdir $GOPATH/src/github.com/coralproject/xenia\ncd $GOPATH/src/github.com/coralproject/xenia\n\ngit clone git@github.com:CoralProject/xenia.git", 
            "title": "Cloning manually"
        }, 
        {
            "location": "/xenia/install/#set-up-your-environment-variables", 
            "text": "This tells Xenia which database you want to use, sets your port, and sets your database password.  Make your own copy of the  config/dev.cfg  file (this edited cfg file will contain your own values for things like your database password, and will not be committed back to the repository if you are doing development work on Xenia). Call your config file whatever you like; we ll call it  custom  in this example.  cd $GOPATH/src/github.com/coralproject/xenia\ncp config/dev.cfg config/custom.cfg  Edit the environment variables to reflect your MongoDB setup. Remember, you re entering your password here, so be sure not to commit this file to the repository!  export XENIA_MONGO_HOST=localhost:27017\nexport XENIA_MONGO_USER=coral-user\nexport XENIA_MONGO_AUTHDB=coral\nexport XENIA_MONGO_DB=coral\n\nexport XENIA_LOGGING_LEVEL=1\nexport XENIA_HOST=:4000\n\n# Use to have the CLI tooling hit the web service.\nexport XENIA_WEB_HOST=\nexport XENIA_WEB_AUTH=\n\n# Set host to Anvil if configured.\n# export XENIA_ANVIL_HOST=https://HOST\n\n# Use to apply extra key:value pairs to the header\n# export XENIA_HEADERS=key:value,key:value\n\n# DO NOT PUSH TO REPO\nexport XENIA_MONGO_PASS=  Required edits:   XENIA_MONGO_HOST : set to your MongoDB where you will be communicating with.  If you are running MongoDB locally on your machine, you should set this to  localhost:27017 .  If you are pointing to a MongoDB running on a server somewhere, set this to the IP address and port of your MongoDB.    XENIA_MONGO_DB : the database you are running queries against ( coral ).  XENIA_MONGO_USER : your MongoDB username.  XENIA_MONGO_PASS : the password for your MongoDB user.  XENIA_MONGO_AUTHDB : the database you are authenticating against (in most cases, should be the same  coral  database as XENIA_MONGO_DB).   Optional edits:   XENIA_WEB_HOST : this is required for the CLI tool. It is the address of the Xenia web service (i.e., an instance of Xenia that you are running on a server).  If you are running everything locally, comment this variable out. This means that the CLI tool will connect directly to your local database, instead of connecting to a running web service.    XENIA_HOST : default is  :4000  if this variable is not set.  XENIA_LOGGING_LEVEL : default is  2  if this variable is not set.  XENIA_WEB_AUTH : your Anvil token, if you have Anvil authentication set up. If you do not have authentication set up, leave this commented out.  XENIA_ANVIL_HOST : the URL to the Anvil host, if you have Anvil authentication set up. If you do not have authentication set up, leave this commented out.  XENIA_HEADERS : leave this commented out.   Once you ve finished editing, source your config file using the source command:  source $GOPATH/src/github.com/coralproject/xenia/config/custom.cfg", 
            "title": "Set up your environment variables"
        }, 
        {
            "location": "/xenia/install/#build-the-cli-tool", 
            "text": "Xenia has a CLI tool that allows you to manage endpoints and perform other actions.  To build to the tool:  cd $GOPATH/src/github.com/coralproject/xenia/cmd/xenia\ngo build  Note: It is best to run with logging level 0 when using the xenia command:  export XENIA_LOGGING_LEVEL=0", 
            "title": "Build the CLI tool"
        }, 
        {
            "location": "/xenia/install/#creating-a-xenia-database-for-the-first-time", 
            "text": "If you are running Xenia on a MongoDB database for the first time you will need the Xenia command line tool to set up the MongoDB for use with Xenia. The CLI tool will create collections and sets of indexes that you can use to execute queries: a sort of dictionary of pre-made queries for you to use.  1) First cd into cmd/xenia directory (this contains the CLI tool):  cd $GOPATH/src/github.com/coralproject/xenia/cmd/xenia  2) Configure the database using  db create . The database.json file contains the information necessary to create the collections and indexes.  ./xenia db create -f scrdb/database.json  Expected output:  Configuring MongoDB\nCreating collection query_sets\nCreating collection query_sets_history\nCreating collection query_scripts\nCreating collection query_scripts_history\nCreating collection query_masks\nCreating collection query_masks_history\nCreating collection query_regexs\nCreating collection query_regexs_history   Troubleshooting note #1 : if you get a response that contains  ERROR: Invalid DB provided , you may have an incorrectly set environment variable. If you are running everything locally and using a local MongoDB, use  printev  to see if  XENIA_WEB_HOST  is set:   printenv XENIA_WEB_HOST  If you are using a local MongoDB, this should not return a value. If it does return a value, unset the variable using  unset , and then try step 2 again:  unset XENIA_WEB_HOST   Troubleshooting note #2 : instead of the expected output shown above, you may see something like:   Configuring MongoDB\nCreating collection query_sets\nERROR: collection already exists  That s okay! It means that your database is already set up with Xenia. Perhaps you imported data that already had configured collections set up. Continue on to step 3.  3) Load all the existing queries:  ./xenia query upsert -p scrquery  Expected output:  Configuring MongoDB\nUpserting Set : Path[scrquery]\nUpserting Set : Upserted  4) Load all the existing masks:  ./xenia mask upsert -p scrmask  Expected output:  Configuring MongoDB\nUpserting Set : Path[scrquery]\nUpserting Set : Upserted  5) Load all the existing regexes:  ./xenia regex upsert -p scrregex  Expected output:  Configuring MongoDB\nUpserting Regex : Path[scrregex]\nUpserting Regex : Upserted  6) That s it! If you are using MongoChef, you should be able to see your newly created collections, which will look something like this:", 
            "title": "Creating a Xenia database for the first time"
        }, 
        {
            "location": "/xenia/install/#run-the-web-service", 
            "text": "1) First cd into the directory containing the web service, xeniad (the Xenia daemon):  cd $GOPATH/src/github.com/coralproject/xenia/cmd/xeniad  2) Build the Xenia web service:  go build  3) Run the web service:  ./xeniad  Expected output:  2016/06/08 10:26:38 app.go:173: USER : startup : Init :\n\nConfig Settings:\nMONGO_USER=coral-user\nMONGO_AUTHDB=coral\nMONGO_DB=coral\nHOST=:4000\nLOGGING_LEVEL=1\nMONGO_HOST=localhost:27017\n\n2016/06/08 10:26:38 main.go:24: USER : startup : Init : Revision     :  unknown \n2016/06/08 10:26:38 main.go:25: USER : startup : Init : Version      :  unknown \n2016/06/08 10:26:38 main.go:26: USER : startup : Init : Build Date   :  unknown \n2016/06/08 10:26:38 main.go:27: USER : startup : Init : Int Version  :  201606081030 \n2016/06/08 10:26:38 main.go:28: USER : startup : Init : Go Version   :  go1.6.2 \n2016/06/08 10:26:38 main.go:29: USER : startup : Init : Go Compiler  :  gc \n2016/06/08 10:26:38 main.go:30: USER : startup : Init : Go ARCH      :  amd64 \n2016/06/08 10:26:38 main.go:31: USER : startup : Init : Go OS        :  darwin   4) You can test your web service by going to the following URL in your browser:  http://localhost:4000/1.0/query .  In your browser, you will see some json displayed. In your terminal, you should see something like:  2016/06/08 13:30:58 app.go:104: USER : 6bd28905-8a92-4aa9-80fd-5e9cff199b3e : Request : Started : Method[GET] URL[/1.0/query] RADDR[[::1]:62121]\n2016/06/08 13:30:58 context.go:65: USER : 6bd28905-8a92-4aa9-80fd-5e9cff199b3e : api : Respond : Started : Code[200]\n2016/06/08 13:30:58 context.go:72: USER : startup : api : Respond : Setting user headers : Access-Control-Allow-Origin:*\n2016/06/08 13:30:58 context.go:110: USER : 6bd28905-8a92-4aa9-80fd-5e9cff199b3e : api : Respond : Completed\n2016/06/08 13:30:58 app.go:126: USER : 6bd28905-8a92-4aa9-80fd-5e9cff199b3e : Request : Completed : Status[200] Duration[46.497305ms]", 
            "title": "Run the web service"
        }, 
        {
            "location": "/xenia/install/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/xenia/install/#authorization", 
            "text": "TODO", 
            "title": "Authorization"
        }, 
        {
            "location": "/xenia/api/", 
            "text": "API calls\n\n\nIf you set the authorization header properly in your browser (TODO: how?) you can run the following endpoints:\n\n\n1) Get a list of configured queries:\n\n\nGET\nhttp://localhost:4000/1.0/query\n\noutput:\n\n[\nbasic\n,\nbasic_var\n,\ntop_commenters_by_count\n]\n\n\n\n\n2) Get the query set document for the \nbasic\n query set:\n\n\nGET\nhttp://localhost:4000/1.0/query/basic\n\noutput:\n\n{\n   \nname\n:\nQTEST_basic\n,\n   \ndesc\n:\n,\n   \nenabled\n:true,\n   \nparams\n:[],\n   \nqueries\n:[\n      {\n         \nname\n:\nBasic\n,\n         \ntype\n:\npipeline\n,\n         \ncollection\n:\ntest_xenia_data\n,\n         \nreturn\n:true,\n         \ncommands\n:[\n            {\n$match\n: {\nstation_id\n : \n42021\n}},\n            {\n$project\n: {\n_id\n: 0, \nname\n: 1}}\n         ]\n      }\n   ]\n}\n\n\n\n\n\n3) Execute the query for the \nbasic\n query set:\n\n\nGET\nhttp://localhost:4000/1.0/exec/basic\n\nset:\n\n{\n   \nname\n:\nbasic\n,\n   \ndesc\n:\n,\n   \nenabled\n:true,\n   \nparams\n:[],\n   \nqueries\n:[\n      {\n         \nname\n:\nBasic\n,\n         \ntype\n:\npipeline\n,\n         \ncollection\n:\ntest_xenia_data\n,\n         \nreturn\n:true,\n         \ncommands\n:[\n            {\n$match\n: {\nstation_id\n : \n42021\n}},\n            {\n$project\n: {\n_id\n: 0, \nname\n: 1}}\n         ]\n      }\n   ]\n}\n\noutput:\n\n{\n  \nresults\n:[\n    {\n      \nName\n:\nbasic\n,\n      \nDocs\n:[\n        {\n          \nname\n:\nC14 - Pasco County Buoy, FL\n\n        }\n      ]\n    }\n  ],\n  \nerror\n:false\n}\n\n\n\n\n4) Execute the query for the \nbasic_var\n query set with variables:\n\n\nGET\nhttp://localhost:4000/1.0/exec/basic_var?station_id=42021\n\nset:\n\n{\n   \nname\n:\nbasic_var\n,\n   \ndesc\n:\n,\n   \nenabled\n:true,\n   \nparams\n:[],\n   \nqueries\n:[\n      {\n         \nname\n:\nBasicVar\n,\n         \ntype\n:\npipeline\n,\n         \ncollection\n:\ntest_xenia_data\n,\n         \nreturn\n:true,\n         \ncommands\n:[\n            {\n$match\n: {\nstation_id\n : \n#string:station_id\n}},\n            {\n$project\n: {\n_id\n: 0, \nname\n: 1}}\n         ]\n      }\n   ]\n}\n\noutput:\n\n{\n  \nresults\n:[\n    {\n      \nName\n:\nbasic_var\n,\n      \nDocs\n:[\n        {\n          \nname\n:\nC14 - Pasco County Buoy, FL\n\n        }\n      ]\n    }\n  ],\n  \nerror\n:false\n}\n\n\n\n\n5) You can execute a dynamic query set:\n\n\nPOST\nhttp://localhost:4000/1.0/exec\n\nPost Data:\n{\n   \nname\n:\nbasic\n,\n   \ndesc\n:\n,\n   \nenabled\n:true,\n   \nparams\n:[],\n   \nqueries\n:[\n      {\n         \nname\n:\nBasic\n,\n         \ntype\n:\npipeline\n,\n         \ncollection\n:\ntest_xenia_data\n,\n         \nreturn\n:true,\n         \ncommands\n:[\n            {\n$match\n: {\nstation_id\n : \n42021\n}},\n            {\n$project\n: {\n_id\n: 0, \nname\n: 1}}\n         ]\n      }\n   ]\n}\n\n\n\n\nQuery management\n\n\nUsing the Xenia command line tool you can manage query sets.\n\n\ncd $GOPATH/src/github.com/coralproject/xenia/cmd/xenia\n\n\n\n\n1) Get a list of saved queries:\n\n\n./xenia query list\n\noutput:\n\nbasic\nbasic_var\ntop_commenters_by_count\n\n\n\n\n3) Look at the details of a query:\n\n\n./xenia query get -n basic\n\noutput:\n\n{\n   \nname\n:\nbasic\n,\n   \ndesc\n:\n,\n   \nenabled\n:true,\n   \nparams\n:[],\n   \nqueries\n:[\n      {\n         \nname\n:\nBasic\n,\n         \ntype\n:\npipeline\n,\n         \ncollection\n:\ntest_xenia_data\n,\n         \nreturn\n:true,\n         \ncommands\n:[\n            {\n$match\n: {\nstation_id\n : \n42021\n}},\n            {\n$project\n: {\n_id\n: 0, \nname\n: 1}}\n         ]\n      }\n   ]\n}\n\n\n\n\n4) Execute a query:\n\n\n./xenia query exec -n basic\n\noutput:\n\n{\n  \nresults\n:[\n    {\n      \nName\n:\nbasic\n,\n      \nDocs\n:[\n        {\n          \nname\n:\nC14 - Pasco County Buoy, FL\n\n        }\n      ]\n    }\n  ],\n  \nerror\n:false\n}\n\n\n\n\n5) Add or update a query for use:\n\n\n./xenia query upsert -p ./scrquery/basic_var.json\n\noutput:\n\nUpserting Query : Path[./scrquery/basic_var.json]\n\n\n\n\nBy convention, we store core query scripts in the \n/xenia/cmd/xenia/scrquery\n folder.  As we develop Coral features, store the .json files there so other members can use them. Eventually, groups of query sets will be refactored to another location, as yet undetermined.\n\n\ncd $GOPATH/src/github.com/coralproject/xenia/cmd/xenia/scrquery\nls\n\n\n\n\nDirect Mongo access (optional)\n\n\nYou can look in the db at existing queries:\n\n\nmongo [flags to connect to your server]\nuse coral (or your databasename)\ndb.query_sets.find()\n\n\n\n\nWriting Sets\n\n\nWriting a set is essentially about creating a MongoDB aggregation pipeline. Xenia has built on top of this by providing extended functionality to make MongoDB more powerful.\n\n\nMulti query set with variable substitution and date processing.\n\n\nGET\nhttp://localhost:4000/1.0/exec/basic?station_id=42021\n\n{\n   \nname\n:\nbasic\n,\n   \ndesc\n:\nShows a basic multi result query.\n,\n   \nenabled\n:true,\n   \nqueries\n:[\n      {\n         \nname\n:\nBasic\n,\n         \ntype\n:\npipeline\n,\n         \ncollection\n:\ntest_bill\n,\n         \nreturn\n:true,\n         \nscripts\n:[\n            {\n$match\n: {\nstation_id\n : \n#station_id#\n}},\n            {\n$project\n: {\n_id\n: 0, \nname\n: 1}}\n         ]\n      },\n      {\n         \nname\n:\nTime\n,\n         \ntype\n:\npipeline\n,\n         \ncollection\n:\ntest_bill\n,\n         \nreturn\n:true,\n         \nscripts\n:[\n            {\n$match\n: {\ncondition.date\n : {\n$gt\n: \n#date:2013-01-01T00:00:00.000Z\n}}},\n            {\n$project\n: {\n_id\n: 0, \nname\n: 1}},\n            {\n$limit\n: 2}\n         ]\n      }\n   ]\n}\n\n\n\n\nHere is the list of commands that exist for variable substitution.\n\n\n{\nfield\n: \n#cmd:variable\n}\n\n// Basic commands.\nBefore: {\nfield\n: \n#number:variable_name\n}      After: {\nfield\n: 1234}\nBefore: {\nfield\n: \n#string:variable_name\n}      After: {\nfield\n: \nvalue\n}\nBefore: {\nfield\n: \n#date:variable_name\n}        After: {\nfield\n: time.Time}\nBefore: {\nfield\n: \n#objid:variable_name\n}       After: {\nfield\n: mgo.ObjectId}\nBefore: {\nfield\n: \n#regex:/pattern/{options}\n}  After: {\nfield\n: bson.RegEx}\n\n// data command can index into saved results.\nBefore: {\nfield\n : {\n$in\n: \n#data.*:list.station_id\n}}}   After: [{\nstation_id\n:\n42021\n}]\nBefore: {\nfield\n: \n#data.0:doc.station_id\n}               After: {\nfield\n: \n23453\n}\n\n// time command manipulates the current time.\nBefore: {\nfield\n: #time:0}                 After: {\nfield\n: time.Time(Current Time)}\nBefore: {\nfield\n: #time:-3600}             After: {\nfield\n: time.Time(3600 seconds in the past)}\nBefore: {\nfield\n: #time:3m}                After: {\nfield\n: time.Time(3 minutes in the future)}\n\nPossible duration types. Default is seconds if not provided.\n\nns\n: Nanosecond\n\nus\n: Microsecond\n\nms\n: Millisecond\n\ns\n : Second\n\nm\n : Minute\n\nh\n : Hour\n\n\n\n\nYou can save the result of one query for later use by the next.\n\n\nGET\nhttp://localhost:4000/1.0/exec/basic_save\n\n{\n   \nname\n:\nbasic_save\n,\n   \ndesc\n:\n,\n   \nenabled\n:true,\n   \nparams\n:[],\n   \nqueries\n:[\n      {\n         \nname\n:\nget_id_list\n,\n         \ndesc\n: \nGet the list of id's\n,\n         \ntype\n:\npipeline\n,\n         \ncollection\n:\ntest_xenia_data\n,\n         \nreturn\n:false,\n         \ncommands\n:[\n            {\n$project\n: {\n_id\n: 0, \nstation_id\n: 1}},\n            {\n$limit\n: 5}\n            {\n$save\n: {\n$map\n: \nlist\n}}\n         ]\n      },\n      {\n         \nname\n:\nretrieve_stations\n,\n         \ndesc\n: \nRetrieve the list of stations\n,\n         \ntype\n:\npipeline\n,\n         \ncollection\n:\ntest_xenia_data\n,\n         \nreturn\n:true,\n         \ncommands\n:[\n            {\n$match\n: {\nstation_id\n : {\n$in\n: \n#data.*:list.station_id\n}}},\n            {\n$project\n: {\n_id\n: 0, \nname\n: 1}},\n         ]\n      }\n   ]\n}\n\n\n\n\nThe \n$save\n command is an Xenia extension and currently only \n$map\n is supported.\n\n\n{\n$save\n: {\n$map\n: \nlist\n}}\n\n\n\n\nThe result will be saved in a map under the name \nlist\n.\n\n\nThe second query is using the \n#data\n command. The data command has two options. Use can use \n#data.*\n or \n#data.Idx\n.\n\n\nUse the \n*\n operator when you need an array. In this example we need to support an \n$in\n command:\n\n\n{\n   \nname\n:\nretrieve_stations\n,\n   \ndesc\n: \nRetrieve the list of stations\n,\n   \ntype\n:\npipeline\n,\n   \ncollection\n:\ntest_xenia_data\n,\n   \nreturn\n:true,\n   \ncommands\n:[\n      {\n$match\n: {\nstation_id\n : {\n$in\n: \n#data.*:list.station_id\n}}},\n      {\n$project\n: {\n_id\n: 0, \nname\n: 1}},\n   ]\n}\n\nWe you need an array to be substitued.\nBefore: {\nfield\n : {\n$in\n: \n#data.*:list.station_id\n}}}\nAfter : {\nfield\n : {\n$in\n: [\n42021\n]}}\n    dataOp : \n*\n\n    lookup : \nlist.station_id\n\n    results: {\nlist\n: [{\nstation_id\n:\n42021\n}]}\n\n\n\n\nUse the index operator when you need a single value. Specify which document in the array of documents you want to select:\n\n\n\n{\n   \nname\n:\nretrieve_stations\n,\n   \ndesc\n: \nRetrieve the list of stations\n,\n   \ntype\n:\npipeline\n,\n   \ncollection\n:\ntest_xenia_data\n,\n   \nreturn\n:true,\n   \ncommands\n:[\n      {\n$match\n: {\nstation_id\n : \n#data.0:list.station_id\n}},\n      {\n$project\n: {\n_id\n: 0, \nname\n: 1}},\n   ]\n}\n\nWe you need a single value to be substitued, select an index.\nBefore: {\nfield\n : \n#data.0:list.station_id\n}\nAfter : {\nfield\n : \n42021\n}\n    dataOp : 0\n    lookup : \nlist.station_id\n\n    results: {\nlist\n: [{\nstation_id\n:\n42021\n}, {\nstation_id\n:\n23567\n}]}\n\n\n\n\nYou can also replace field names in the query commands.\n\n\nVariables\n{\n  \ncond\n: \ncondition\n,\n  \ndt\n: \ndate\n\n}\n\nQuery Set\n{\n   \nname\n:\nbasic\n,\n   \ndesc\n:\nShows field substitution.\n,\n   \nenabled\n:true,\n   \nqueries\n:[\n      {\n         \nname\n:\nTime\n,\n         \ntype\n:\npipeline\n,\n         \ncollection\n:\ntest_bill\n,\n         \nreturn\n:true,\n         \nscripts\n:[\n            {\n$match\n: {\n{cond}.{dt}\n : {\n$gt\n: \n#date:2013-01-01T00:00:00.000Z\n}}},\n            {\n$project\n: {\n_id\n: 0, \nname\n: 1}},\n            {\n$limit\n: 2}\n         ]\n      }\n   ]\n}", 
            "title": "API and query management"
        }, 
        {
            "location": "/xenia/api/#api-calls", 
            "text": "If you set the authorization header properly in your browser (TODO: how?) you can run the following endpoints:  1) Get a list of configured queries:  GET\nhttp://localhost:4000/1.0/query\n\noutput:\n\n[ basic , basic_var , top_commenters_by_count ]  2) Get the query set document for the  basic  query set:  GET\nhttp://localhost:4000/1.0/query/basic\n\noutput:\n\n{\n    name : QTEST_basic ,\n    desc : ,\n    enabled :true,\n    params :[],\n    queries :[\n      {\n          name : Basic ,\n          type : pipeline ,\n          collection : test_xenia_data ,\n          return :true,\n          commands :[\n            { $match : { station_id  :  42021 }},\n            { $project : { _id : 0,  name : 1}}\n         ]\n      }\n   ]\n}  3) Execute the query for the  basic  query set:  GET\nhttp://localhost:4000/1.0/exec/basic\n\nset:\n\n{\n    name : basic ,\n    desc : ,\n    enabled :true,\n    params :[],\n    queries :[\n      {\n          name : Basic ,\n          type : pipeline ,\n          collection : test_xenia_data ,\n          return :true,\n          commands :[\n            { $match : { station_id  :  42021 }},\n            { $project : { _id : 0,  name : 1}}\n         ]\n      }\n   ]\n}\n\noutput:\n\n{\n   results :[\n    {\n       Name : basic ,\n       Docs :[\n        {\n           name : C14 - Pasco County Buoy, FL \n        }\n      ]\n    }\n  ],\n   error :false\n}  4) Execute the query for the  basic_var  query set with variables:  GET\nhttp://localhost:4000/1.0/exec/basic_var?station_id=42021\n\nset:\n\n{\n    name : basic_var ,\n    desc : ,\n    enabled :true,\n    params :[],\n    queries :[\n      {\n          name : BasicVar ,\n          type : pipeline ,\n          collection : test_xenia_data ,\n          return :true,\n          commands :[\n            { $match : { station_id  :  #string:station_id }},\n            { $project : { _id : 0,  name : 1}}\n         ]\n      }\n   ]\n}\n\noutput:\n\n{\n   results :[\n    {\n       Name : basic_var ,\n       Docs :[\n        {\n           name : C14 - Pasco County Buoy, FL \n        }\n      ]\n    }\n  ],\n   error :false\n}  5) You can execute a dynamic query set:  POST\nhttp://localhost:4000/1.0/exec\n\nPost Data:\n{\n    name : basic ,\n    desc : ,\n    enabled :true,\n    params :[],\n    queries :[\n      {\n          name : Basic ,\n          type : pipeline ,\n          collection : test_xenia_data ,\n          return :true,\n          commands :[\n            { $match : { station_id  :  42021 }},\n            { $project : { _id : 0,  name : 1}}\n         ]\n      }\n   ]\n}", 
            "title": "API calls"
        }, 
        {
            "location": "/xenia/api/#query-management", 
            "text": "Using the Xenia command line tool you can manage query sets.  cd $GOPATH/src/github.com/coralproject/xenia/cmd/xenia  1) Get a list of saved queries:  ./xenia query list\n\noutput:\n\nbasic\nbasic_var\ntop_commenters_by_count  3) Look at the details of a query:  ./xenia query get -n basic\n\noutput:\n\n{\n    name : basic ,\n    desc : ,\n    enabled :true,\n    params :[],\n    queries :[\n      {\n          name : Basic ,\n          type : pipeline ,\n          collection : test_xenia_data ,\n          return :true,\n          commands :[\n            { $match : { station_id  :  42021 }},\n            { $project : { _id : 0,  name : 1}}\n         ]\n      }\n   ]\n}  4) Execute a query:  ./xenia query exec -n basic\n\noutput:\n\n{\n   results :[\n    {\n       Name : basic ,\n       Docs :[\n        {\n           name : C14 - Pasco County Buoy, FL \n        }\n      ]\n    }\n  ],\n   error :false\n}  5) Add or update a query for use:  ./xenia query upsert -p ./scrquery/basic_var.json\n\noutput:\n\nUpserting Query : Path[./scrquery/basic_var.json]  By convention, we store core query scripts in the  /xenia/cmd/xenia/scrquery  folder.  As we develop Coral features, store the .json files there so other members can use them. Eventually, groups of query sets will be refactored to another location, as yet undetermined.  cd $GOPATH/src/github.com/coralproject/xenia/cmd/xenia/scrquery\nls", 
            "title": "Query management"
        }, 
        {
            "location": "/xenia/api/#direct-mongo-access-optional", 
            "text": "You can look in the db at existing queries:  mongo [flags to connect to your server]\nuse coral (or your databasename)\ndb.query_sets.find()", 
            "title": "Direct Mongo access (optional)"
        }, 
        {
            "location": "/xenia/api/#writing-sets", 
            "text": "Writing a set is essentially about creating a MongoDB aggregation pipeline. Xenia has built on top of this by providing extended functionality to make MongoDB more powerful.  Multi query set with variable substitution and date processing.  GET\nhttp://localhost:4000/1.0/exec/basic?station_id=42021\n\n{\n    name : basic ,\n    desc : Shows a basic multi result query. ,\n    enabled :true,\n    queries :[\n      {\n          name : Basic ,\n          type : pipeline ,\n          collection : test_bill ,\n          return :true,\n          scripts :[\n            { $match : { station_id  :  #station_id# }},\n            { $project : { _id : 0,  name : 1}}\n         ]\n      },\n      {\n          name : Time ,\n          type : pipeline ,\n          collection : test_bill ,\n          return :true,\n          scripts :[\n            { $match : { condition.date  : { $gt :  #date:2013-01-01T00:00:00.000Z }}},\n            { $project : { _id : 0,  name : 1}},\n            { $limit : 2}\n         ]\n      }\n   ]\n}  Here is the list of commands that exist for variable substitution.  { field :  #cmd:variable }\n\n// Basic commands.\nBefore: { field :  #number:variable_name }      After: { field : 1234}\nBefore: { field :  #string:variable_name }      After: { field :  value }\nBefore: { field :  #date:variable_name }        After: { field : time.Time}\nBefore: { field :  #objid:variable_name }       After: { field : mgo.ObjectId}\nBefore: { field :  #regex:/pattern/{options} }  After: { field : bson.RegEx}\n\n// data command can index into saved results.\nBefore: { field  : { $in :  #data.*:list.station_id }}}   After: [{ station_id : 42021 }]\nBefore: { field :  #data.0:doc.station_id }               After: { field :  23453 }\n\n// time command manipulates the current time.\nBefore: { field : #time:0}                 After: { field : time.Time(Current Time)}\nBefore: { field : #time:-3600}             After: { field : time.Time(3600 seconds in the past)}\nBefore: { field : #time:3m}                After: { field : time.Time(3 minutes in the future)}\n\nPossible duration types. Default is seconds if not provided. ns : Nanosecond us : Microsecond ms : Millisecond s  : Second m  : Minute h  : Hour  You can save the result of one query for later use by the next.  GET\nhttp://localhost:4000/1.0/exec/basic_save\n\n{\n    name : basic_save ,\n    desc : ,\n    enabled :true,\n    params :[],\n    queries :[\n      {\n          name : get_id_list ,\n          desc :  Get the list of id's ,\n          type : pipeline ,\n          collection : test_xenia_data ,\n          return :false,\n          commands :[\n            { $project : { _id : 0,  station_id : 1}},\n            { $limit : 5}\n            { $save : { $map :  list }}\n         ]\n      },\n      {\n          name : retrieve_stations ,\n          desc :  Retrieve the list of stations ,\n          type : pipeline ,\n          collection : test_xenia_data ,\n          return :true,\n          commands :[\n            { $match : { station_id  : { $in :  #data.*:list.station_id }}},\n            { $project : { _id : 0,  name : 1}},\n         ]\n      }\n   ]\n}  The  $save  command is an Xenia extension and currently only  $map  is supported.  { $save : { $map :  list }}  The result will be saved in a map under the name  list .  The second query is using the  #data  command. The data command has two options. Use can use  #data.*  or  #data.Idx .  Use the  *  operator when you need an array. In this example we need to support an  $in  command:  {\n    name : retrieve_stations ,\n    desc :  Retrieve the list of stations ,\n    type : pipeline ,\n    collection : test_xenia_data ,\n    return :true,\n    commands :[\n      { $match : { station_id  : { $in :  #data.*:list.station_id }}},\n      { $project : { _id : 0,  name : 1}},\n   ]\n}\n\nWe you need an array to be substitued.\nBefore: { field  : { $in :  #data.*:list.station_id }}}\nAfter : { field  : { $in : [ 42021 ]}}\n    dataOp :  * \n    lookup :  list.station_id \n    results: { list : [{ station_id : 42021 }]}  Use the index operator when you need a single value. Specify which document in the array of documents you want to select:  \n{\n    name : retrieve_stations ,\n    desc :  Retrieve the list of stations ,\n    type : pipeline ,\n    collection : test_xenia_data ,\n    return :true,\n    commands :[\n      { $match : { station_id  :  #data.0:list.station_id }},\n      { $project : { _id : 0,  name : 1}},\n   ]\n}\n\nWe you need a single value to be substitued, select an index.\nBefore: { field  :  #data.0:list.station_id }\nAfter : { field  :  42021 }\n    dataOp : 0\n    lookup :  list.station_id \n    results: { list : [{ station_id : 42021 }, { station_id : 23567 }]}  You can also replace field names in the query commands.  Variables\n{\n   cond :  condition ,\n   dt :  date \n}\n\nQuery Set\n{\n    name : basic ,\n    desc : Shows field substitution. ,\n    enabled :true,\n    queries :[\n      {\n          name : Time ,\n          type : pipeline ,\n          collection : test_bill ,\n          return :true,\n          scripts :[\n            { $match : { {cond}.{dt}  : { $gt :  #date:2013-01-01T00:00:00.000Z }}},\n            { $project : { _id : 0,  name : 1}},\n            { $limit : 2}\n         ]\n      }\n   ]\n}", 
            "title": "Writing Sets"
        }, 
        {
            "location": "/xenia/tests/", 
            "text": "Running Tests\n\n\nYou can run tests in the \napp\n and \npkg\n folder.\n\n\nIf you plan to run tests in parallel please use this command:\n\n\ngo test -cpu 1 ./...\n\n\n\n\nYou can alway run individual tests in each package using just:\n\n\ngo test\n\n\n\n\nDo not run tests in the vendor folder.", 
            "title": "Testing"
        }, 
        {
            "location": "/xenia/tests/#running-tests", 
            "text": "You can run tests in the  app  and  pkg  folder.  If you plan to run tests in parallel please use this command:  go test -cpu 1 ./...  You can alway run individual tests in each package using just:  go test  Do not run tests in the vendor folder.", 
            "title": "Running Tests"
        }, 
        {
            "location": "/xenia/xenia-driver-js/", 
            "text": "Xenia Driver\n\n\nXenia-driver-js\n is a Javascript library that performs queries to \nXenia\n from the browser and Node.js.", 
            "title": "Introduction"
        }, 
        {
            "location": "/xenia/xenia-driver-js/#xenia-driver", 
            "text": "Xenia-driver-js  is a Javascript library that performs queries to  Xenia  from the browser and Node.js.", 
            "title": "Xenia Driver"
        }, 
        {
            "location": "/xenia/xenia-driver-js-install/", 
            "text": "Xenia Driver Installation\n\n\n$ npm install --save xenia-driver\n\n\n\n\nImport and use\n\n\nimport XeniaDriver from 'xenia-driver'\n\n// Configure your instance\nconst xenia = XeniaDriver(baseUrl, {username: 'user', password: 'pass'})\n\n// Use the driver\nxenia()\n  .match({ 'category': 'sports' })\n  .include(['comments', 'name'])\n  .limit(14)\n  .skip(8)\n.join('my_collection')\n.exec().then(data =\n console.log(data.results))", 
            "title": "Installation"
        }, 
        {
            "location": "/xenia/xenia-driver-js-install/#xenia-driver-installation", 
            "text": "$ npm install --save xenia-driver", 
            "title": "Xenia Driver Installation"
        }, 
        {
            "location": "/xenia/xenia-driver-js-install/#import-and-use", 
            "text": "import XeniaDriver from 'xenia-driver'\n\n// Configure your instance\nconst xenia = XeniaDriver(baseUrl, {username: 'user', password: 'pass'})\n\n// Use the driver\nxenia()\n  .match({ 'category': 'sports' })\n  .include(['comments', 'name'])\n  .limit(14)\n  .skip(8)\n.join('my_collection')\n.exec().then(data =  console.log(data.results))", 
            "title": "Import and use"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/", 
            "text": "Xenia Driver API\n\n\nXeniaDriver(baseURL, auth [, queryParams] [, reqParams])\n\n\nCreates a new \nXeniaDriver\n instance\n\n\n\n\nbaseURL\n (String) - Xenia server base url\n\n\nauth\n (String | Object) - basic authentication credentials. If it\ns a string it should be the Basic authentication value for the Authorization header\n\n\n[auth.username]\n (String) - Auth username\n\n\n[auth.password]\n (String) - Auth password\n\n\n[queryParams]\n (Object) - It can hold a Xenia raw query to perform before the rest of the queries\n\n\n[reqParams]\n (Object) - Add your own parameters to the request\n\n\n\n\nconst xenia = XeniaDriver('https://my-xenia-url.com', 'Basic kewlrgm;we4p3jtqpwfawmeklfdmdsadlm')\n\n\n\n\naddQuery(queryData)\n\n\nInitialize a query. When the xenia constructor runs it will call this function for you. Use it for adding more than one query in the same request.\n\n\n\n\n[queryData]\n (Object) - Provide the configuration for the new query\n\n\n\n\nxenia()\n  .limit(20).skip(10)\n  .addQuery().match({'name': 'John Doe'})\n  .exec().then(data =\n console.log(data))\n\n\n\n\ncollection(name)\n\n\nSet the current query collection\n\n\n\n\nname\n (String) - collection name\n\n\n\n\nxenia()\n  .collection('users')\n  .exclude(['_id'])\n  .exec().then(data =\n console.log(data))\n\n\n\n\nexec(queryName, params)\n\n\nExecutes the request\n\n\n\n\n[name]\n (string) - Executes a saved query by name\n\n\n[params]\n (Object) - Custom request parameters\n\n\n\n\nxenia().exec('my_saved_query').then(data =\n console.log(data))\n\n// Or\n\nxenia().limit(20).skip(10)\n  .exec().then(data =\n console.log(data))\n  .catch(err =\n console.log(err))\n\n\n\n\ngetQueries()\n\n\nGet a list of available queries\n\n\nxenia().getQueries().then(data =\n console.log(data))\n\n\n\n\ngetQuery(name)\n\n\nGet a specific query document\n\n\n\n\nname\n (String) - query name\n\n\n\n\nxenia().getQuery('my_query').then(data =\n console.log(data))\n\n\n\n\nsaveQuery(name)\n\n\nSave a new query\n\n\n\n\n[name]\n (String) - query name\n\n\n\n\nxenia()\n  .collection('users')\n  .limit(5).include(['name', 'avatar'])\n  .saveQuery('first users').then(data =\n console.log(data))\n\n\n\n\ndeleteQuery(name)\n\n\nDelete a query\n\n\n\n\n[name]\n (String) - query name\n\n\n\n\nxenia()\n  .deleteQuery('first users')\n  .then(data =\n console.log(data))\n\n\n\n\nlimit(n)\n\n\nLimit the amount of retrieved documents\n\n\n\n\n[n]\n (Number) - number of docs to retrieve, default: 20\n\n\n\n\nxenia().limit(15)\n\n\n\n\nskip(n)\n\n\nSkip the first n documents\n\n\n\n\n[n]\n (Number) - number of skipped docs, default: 0\n\n\n\n\nxenia().skip(12)\n\n\n\n\nsample(n)\n\n\nReturn a document sample from the collection\n\n\n\n\n[n]\n (Number) - number of sample docs, default: 20\n\n\n\n\nxenia().sample(50)\n\n\n\n\nproject(fields)\n\n\nInclude and exclude fields from the result using the $project aggregation pipeline operator. You\nll find out that \nXenia#include\n and \nXenia#exclude\n can be easier to use for most scenarios.\n\n\n\n\nfields\n (Object) - Aggregation fields object\n\n\n\n\nxenia()\n  .project({'name': true, '_id': false, 'comments': { 'country': true}})\n\n\n\n\ninclude(fieldNames)\n\n\nWhitelist retrieved fields\n\n\n\n\nfieldNames\n (Array) - fields you want to retrieve\n\n\n\n\nxenia().include(['name', 'avatar'])\n\n\n\n\nexclude(fieldNames)\n\n\nBlacklist retrieved fields\n\n\n\n\nfieldNames\n (Array) - fields you don\nt want to retrieve\n\n\n\n\nxenia().exclude(['age', 'gender'])\n\n\n\n\nmatch(query)\n\n\nPerforms a match command on the aggregation pipeline\n\n\n\n\nquery\n (Object) - Match parameters\n\n\n\n\nxenia().match({ 'name': 'John', 'status': { '$in' : ['user', 'admin']} })\n\n\n\n\nredact(query)\n\n\nPerforms a redact command on the aggregation pipeline\n\n\n\n\nquery\n (Object) - Redact parameters\n\n\n\n\nxenia().redact({ $cond: {\n  if: { $gt: [ { $size: { $setIntersection: [ \n$tags\n, userAccess ] } }, 0 ] },\n  then: \n$$DESCEND\n,\n  else: \n$$PRUNE\n\n}})\n\n\n\n\nunwind(path, includeArrayIndex, preserveNullAndEmptyArrays)\n\n\nPerforms a unwind command on the aggregation pipeline - Deconstructs an array field from the input documents to output a document for each element\n\n\n\n\npath\n (Object | String) - Field path\n\n\n[includeArrayIndex]\n (String) - arrayIndex name\n\n\n[preserveNullAndEmptyArrays]\n (Boolean) - preserve null and empty arrays, default false\n\n\n\n\nxenia().unwind('$comments')\n\n\n\n\ngroup(groups)\n\n\nGroup documents\n\n\n\n\ngroups\n (Object) - group object\n\n\n\n\nxenia().group({ _id : { month: { $month: '$date' } })\n\n\n\n\nsort(order)\n\n\nSort documents by fields\n\n\n\n\norder\n (Object|Array) - how to sort the data\n\n\n\n\nxenia().sort(['name', 1])\n\n// Or\n\nxenia.sort({'name': 1, 'statistics.comments.count': -1})\n\n\n\n\njoin(collection, field, matchingField, name)\n\n\nCreates a new query joining the actual one using the save method from Xenia\n\n\n\n\ncollection\n (String) - The collection you want to join\n\n\n[field]\n (String) - The matching field in the collection you want to join, default: _id\n\n\n[matchingField]\n (String) - The field to match in your actual collection, default: same as field parameter\n\n\n[name]\n (String) - The field name on the results, default: list\n\n\n\n\nxenia().collection('comments')\n.include(['body', 'asset_id']).limit(5)\n\n.join('assets', '_id', 'asset_id', 'asset')\n\n.include(['section']).exec()\n\n\n\n\nDevelopment\n\n\n$ npm start\n\n\n\nTest\n\n\n$ npm test", 
            "title": "API"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#xenia-driver-api", 
            "text": "", 
            "title": "Xenia Driver API"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#xeniadriverbaseurl-auth-queryparams-reqparams", 
            "text": "Creates a new  XeniaDriver  instance   baseURL  (String) - Xenia server base url  auth  (String | Object) - basic authentication credentials. If it s a string it should be the Basic authentication value for the Authorization header  [auth.username]  (String) - Auth username  [auth.password]  (String) - Auth password  [queryParams]  (Object) - It can hold a Xenia raw query to perform before the rest of the queries  [reqParams]  (Object) - Add your own parameters to the request   const xenia = XeniaDriver('https://my-xenia-url.com', 'Basic kewlrgm;we4p3jtqpwfawmeklfdmdsadlm')", 
            "title": "XeniaDriver(baseURL, auth [, queryParams] [, reqParams])"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#addqueryquerydata", 
            "text": "Initialize a query. When the xenia constructor runs it will call this function for you. Use it for adding more than one query in the same request.   [queryData]  (Object) - Provide the configuration for the new query   xenia()\n  .limit(20).skip(10)\n  .addQuery().match({'name': 'John Doe'})\n  .exec().then(data =  console.log(data))", 
            "title": "addQuery(queryData)"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#collectionname", 
            "text": "Set the current query collection   name  (String) - collection name   xenia()\n  .collection('users')\n  .exclude(['_id'])\n  .exec().then(data =  console.log(data))", 
            "title": "collection(name)"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#execqueryname-params", 
            "text": "Executes the request   [name]  (string) - Executes a saved query by name  [params]  (Object) - Custom request parameters   xenia().exec('my_saved_query').then(data =  console.log(data))\n\n// Or\n\nxenia().limit(20).skip(10)\n  .exec().then(data =  console.log(data))\n  .catch(err =  console.log(err))", 
            "title": "exec(queryName, params)"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#getqueries", 
            "text": "Get a list of available queries  xenia().getQueries().then(data =  console.log(data))", 
            "title": "getQueries()"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#getqueryname", 
            "text": "Get a specific query document   name  (String) - query name   xenia().getQuery('my_query').then(data =  console.log(data))", 
            "title": "getQuery(name)"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#savequeryname", 
            "text": "Save a new query   [name]  (String) - query name   xenia()\n  .collection('users')\n  .limit(5).include(['name', 'avatar'])\n  .saveQuery('first users').then(data =  console.log(data))", 
            "title": "saveQuery(name)"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#deletequeryname", 
            "text": "Delete a query   [name]  (String) - query name   xenia()\n  .deleteQuery('first users')\n  .then(data =  console.log(data))", 
            "title": "deleteQuery(name)"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#limitn", 
            "text": "Limit the amount of retrieved documents   [n]  (Number) - number of docs to retrieve, default: 20   xenia().limit(15)", 
            "title": "limit(n)"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#skipn", 
            "text": "Skip the first n documents   [n]  (Number) - number of skipped docs, default: 0   xenia().skip(12)", 
            "title": "skip(n)"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#samplen", 
            "text": "Return a document sample from the collection   [n]  (Number) - number of sample docs, default: 20   xenia().sample(50)", 
            "title": "sample(n)"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#projectfields", 
            "text": "Include and exclude fields from the result using the $project aggregation pipeline operator. You ll find out that  Xenia#include  and  Xenia#exclude  can be easier to use for most scenarios.   fields  (Object) - Aggregation fields object   xenia()\n  .project({'name': true, '_id': false, 'comments': { 'country': true}})", 
            "title": "project(fields)"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#includefieldnames", 
            "text": "Whitelist retrieved fields   fieldNames  (Array) - fields you want to retrieve   xenia().include(['name', 'avatar'])", 
            "title": "include(fieldNames)"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#excludefieldnames", 
            "text": "Blacklist retrieved fields   fieldNames  (Array) - fields you don t want to retrieve   xenia().exclude(['age', 'gender'])", 
            "title": "exclude(fieldNames)"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#matchquery", 
            "text": "Performs a match command on the aggregation pipeline   query  (Object) - Match parameters   xenia().match({ 'name': 'John', 'status': { '$in' : ['user', 'admin']} })", 
            "title": "match(query)"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#redactquery", 
            "text": "Performs a redact command on the aggregation pipeline   query  (Object) - Redact parameters   xenia().redact({ $cond: {\n  if: { $gt: [ { $size: { $setIntersection: [  $tags , userAccess ] } }, 0 ] },\n  then:  $$DESCEND ,\n  else:  $$PRUNE \n}})", 
            "title": "redact(query)"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#unwindpath-includearrayindex-preservenullandemptyarrays", 
            "text": "Performs a unwind command on the aggregation pipeline - Deconstructs an array field from the input documents to output a document for each element   path  (Object | String) - Field path  [includeArrayIndex]  (String) - arrayIndex name  [preserveNullAndEmptyArrays]  (Boolean) - preserve null and empty arrays, default false   xenia().unwind('$comments')", 
            "title": "unwind(path, includeArrayIndex, preserveNullAndEmptyArrays)"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#groupgroups", 
            "text": "Group documents   groups  (Object) - group object   xenia().group({ _id : { month: { $month: '$date' } })", 
            "title": "group(groups)"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#sortorder", 
            "text": "Sort documents by fields   order  (Object|Array) - how to sort the data   xenia().sort(['name', 1])\n\n// Or\n\nxenia.sort({'name': 1, 'statistics.comments.count': -1})", 
            "title": "sort(order)"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#joincollection-field-matchingfield-name", 
            "text": "Creates a new query joining the actual one using the save method from Xenia   collection  (String) - The collection you want to join  [field]  (String) - The matching field in the collection you want to join, default: _id  [matchingField]  (String) - The field to match in your actual collection, default: same as field parameter  [name]  (String) - The field name on the results, default: list   xenia().collection('comments')\n.include(['body', 'asset_id']).limit(5)\n\n.join('assets', '_id', 'asset_id', 'asset')\n\n.include(['section']).exec()", 
            "title": "join(collection, field, matchingField, name)"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#development", 
            "text": "$ npm start", 
            "title": "Development"
        }, 
        {
            "location": "/xenia/xenia-driver-js-api/#test", 
            "text": "$ npm test", 
            "title": "Test"
        }, 
        {
            "location": "/contribute/", 
            "text": "Contributing to the Coral Project\n\n\nWelcome! We\nre delighted to have you contribute to the Coral Project. Before you get started, be sure to review our \nCode of Conduct\n, which governs all development and project contributions.\n\n\nThere are a number of ways that you can contribute to the Coral Project: you don\nt have to be a developer to help out.\n\n\nContributing without programming\n\n\nWe need help with a number of non-programming tasks:\n\n\n\n\nReport bugs\n\n\nRequest features\n\n\nWrite and edit documentation\n\n\n\n\nContributing by coding\n\n\nIf you are a developer, especially if you know Golang or Node.js, we would love your help in fixing bugs and developing new features and plug-ins.\n\n\n\n\nWrite code\n\n\n\n\nContributing to the community\n\n\nAlso, make sure to play \nat least\n three rounds of \nCards Against Community\n daily.", 
            "title": "Introduction"
        }, 
        {
            "location": "/contribute/#contributing-to-the-coral-project", 
            "text": "Welcome! We re delighted to have you contribute to the Coral Project. Before you get started, be sure to review our  Code of Conduct , which governs all development and project contributions.  There are a number of ways that you can contribute to the Coral Project: you don t have to be a developer to help out.", 
            "title": "Contributing to the Coral Project"
        }, 
        {
            "location": "/contribute/#contributing-without-programming", 
            "text": "We need help with a number of non-programming tasks:   Report bugs  Request features  Write and edit documentation", 
            "title": "Contributing without programming"
        }, 
        {
            "location": "/contribute/#contributing-by-coding", 
            "text": "If you are a developer, especially if you know Golang or Node.js, we would love your help in fixing bugs and developing new features and plug-ins.   Write code", 
            "title": "Contributing by coding"
        }, 
        {
            "location": "/contribute/#contributing-to-the-community", 
            "text": "Also, make sure to play  at least  three rounds of  Cards Against Community  daily.", 
            "title": "Contributing to the community"
        }, 
        {
            "location": "/contribute/", 
            "text": "Contributing to the Coral Project\n\n\nWelcome! We\nre delighted to have you contribute to the Coral Project. Before you get started, be sure to review our \nCode of Conduct\n, which governs all development and project contributions.\n\n\nThere are a number of ways that you can contribute to the Coral Project: you don\nt have to be a developer to help out.\n\n\nContributing without programming\n\n\nWe need help with a number of non-programming tasks:\n\n\n\n\nReport bugs\n\n\nRequest features\n\n\nWrite and edit documentation\n\n\n\n\nContributing by coding\n\n\nIf you are a developer, especially if you know Golang or Node.js, we would love your help in fixing bugs and developing new features and plug-ins.\n\n\n\n\nWrite code\n\n\n\n\nContributing to the community\n\n\nAlso, make sure to play \nat least\n three rounds of \nCards Against Community\n daily.", 
            "title": "Introduction"
        }, 
        {
            "location": "/contribute/#contributing-to-the-coral-project", 
            "text": "Welcome! We re delighted to have you contribute to the Coral Project. Before you get started, be sure to review our  Code of Conduct , which governs all development and project contributions.  There are a number of ways that you can contribute to the Coral Project: you don t have to be a developer to help out.", 
            "title": "Contributing to the Coral Project"
        }, 
        {
            "location": "/contribute/#contributing-without-programming", 
            "text": "We need help with a number of non-programming tasks:   Report bugs  Request features  Write and edit documentation", 
            "title": "Contributing without programming"
        }, 
        {
            "location": "/contribute/#contributing-by-coding", 
            "text": "If you are a developer, especially if you know Golang or Node.js, we would love your help in fixing bugs and developing new features and plug-ins.   Write code", 
            "title": "Contributing by coding"
        }, 
        {
            "location": "/contribute/#contributing-to-the-community", 
            "text": "Also, make sure to play  at least  three rounds of  Cards Against Community  daily.", 
            "title": "Contributing to the community"
        }, 
        {
            "location": "/contribute/documentation/writing_documentation/", 
            "text": "Writing Documentation\n\n\nThis section explains how the community can contribute to the Coral Project documentation.\n\n\nYou can contribute to the documentation by editing existing documents for clarity or correcting errors. Additionally, any new features or changes to the software should be thoroughly documented. We value clear, consistent, thorough, readable documentation!\n\n\nHere is how our documentation works:\n\n\n\n\nThe source documentation lives in GitHub at \nhttps://github.com/coralproject/docs/\n.\n\n\nThe documentation is hosted at \nRead the Docs\n.\n\n\nWhen the documentation is updated and pushed to the GitHub repository, this triggers a build of the Read the Docs site.\n\n\n\n\nGetting the documentation\n\n\nAll of the documentation for the Coral Project \nresides in GitHub\n in the \ndocs\n repository.\n\n\nTo get a local version of the documentation, clone the repository:\n\n\ngit clone https://github.com/coralproject/docs.git\n\n\n\n\nIf you haven\nt worked with Git before, you can find \ninstructions on downloading it here\n.\n\n\nGetting started with MkDocs\n\n\nThe Coral Project\ns documentation uses the \nMkDocs\n documentation system, which uses Markdown to format text. The \nMkDocs website\n contains a lot of good information on writing documentation in MkDocs, and there are plenty of decent Markdown cheatsheets floating around, \nlike this one\n.\n\n\nHow the documentation is organized\n\n\nThe documentation is organized into several categories:\n\n\n\n\nIntroduction\n: This offers a general overview of the Coral Project and its different components.\n\n\nDeveloper Guide\n: This provides information for technical users of the Coral Project. It offers installation instructions for the Coral Ecosystem as a whole, as well as installation instructions for each individual component.\n\n\nUser Guide\n: This provides information for end users (publishers, journalists, moderators, readers) on how to use the features of Coral. It includes tutorials and how-to guides.\n\n\nContribute\n: This provides information on how to contribute to the Coral Project through open source. There are sections for the developers (how to work with GitHub, etc.), as well as sections for those who want to contribute to other pieces of the Coral Project (such as the documentation!).\n\n\nFAQ\n\n\n\n\nWriting style\n\n\nThe key thing to remember when writing documentation is to be extremely explicit and detailed. Do not assume knowledge!\n\n\nDo not assume knowledge\n\n\n\n\nWhenever possible, include an actual instruction.\n\n\nGood:\n\n\nIf there are command line instructions involved, include the precise command line instruction. Instructions that users can copy/paste into the command line are great!\n\n\n\n\n\n\nShow \nexpected results\n wherever possible. If a command line instruction should return a certain result, show that expected result. If there is a URL you can visit to test whether or not something is working, provide that URL as a link.\n\n\nIf there are variables to configure, explicitly state the purpose of the variables. This should be explained in a comment in the configuration file, but you should also provide an explanation in your instructions.\n\n\nDo not merely explain HOW to do something, but also WHY you are doing it in that way. This makes it easier for users to troubleshoot issues.\n\n\n\n\nCommands and code blocks\n\n\n\n\nEnclose commands and code blocks in triple-tick blocks (```). Yes, Markdown supports indentation to delineate code blocks, but the triple-ticks make things more explicit.\n\n\nBe sure you are using universal command line commands, not shortcuts available in some shells!\n\n\nGood: \nmkdir exampledirectory\n\n\nBad: \nmd exampledirectory\n\n\n\n\n\n\n\n\nThird-party components\n\n\nIs there a third-party component the user needs to set up (for instance, MongoDB)?\n\n\n\n\nDon\nt include all setup instructions, but do link to the setup instructions on the website of the third-party app in question.\n\n\nBe sure to detail any specific instructions they will need to integrate this third-party component into the system.\n\n\nCurrently, much of this \nthird party setup\n information resides in the \nDeveloper Setup\n document. This reduces duplication if there is a component that needs to be set up for multiple items.\n\n\n\n\nMarkup specific style\n\n\n\n\nWhen adding headlines and section dividers, keep in mind that H1 (#) and H2 (##) text shows up in the Table of Contents. H3 (###) and below does not.\n\n\nTo avoid making the ToC too cluttered, try to only use H2 (##) for key large sections.\n\n\n\n\nInstallation instructions\n\n\n\n\nBe sure to include \nevery single step\n. Preferably copy paste.\n\n\nWhen working from the command line, include every step in code blocks.\n\n\n\n\nAPI documentation\n\n\nWhen documenting a REST-style API, such as the \nPillar API\n, there are certain pieces of information to include. The \nPillar API documentation\n is a useful template.\n\n\n\n\nCreate an initial table that lists endpoints, which contains:\n\n\nThe endpoint URL.\n\n\nThe HTTP verb for that endpoint (GET, POST).\n\n\nThe basic functionality description for that endpoint. Make this into a targeted link that jumps to the full description for that endpoint.\n\n\n\n\n\n\nCreate a full description for each endpoint. You can see the \nGet Users\n Pillar endpoint for an example). Each description should include:\n\n\nThe parameters for the endpoint (include the parameter name, whether it is required or optional, the type, and the description).\n\n\nAn example call.\n\n\nThe response for the example call (whether that is a JSON object, or simply a status response).\n\n\n\n\n\n\n\n\nTerminology\n\n\nHere are some commonly used terms:\n\n\nSoftware specific\n\n\n\n\nCoral\n: Refers to the Coral product, which includes the three components Trust, Ask, and Talk.\n\n\nCoral Project\n: Refers to the project of building and crafting the Coral product.\n\n\nCoral Ecosystem\n: Refers to all of the pieces that make up Coral from a more technical or development perspective: includes all the technical components such as Pillar, Sponge, and Cay.\n\n\n\n\nUsers\n\n\n\n\nDevelopers\n: Refers to the technical users of Coral who will be installing Coral and working with the backend. Also refers to open-source contributors.\n\n\nEnd Users\n: Refers to anyone who will be interacting with the Coral front end. Examples of end users are publishers, moderators, journalists, and readers.\n\n\n\n\nRelease notes\n\n\nImproving the documentation\n\n\nImprovements are welcomed! Did you perform an installation, and found some of the language or instructions confusing or missing? Help us fix it!\n\n\nTranslating documentation", 
            "title": "Writing documentation"
        }, 
        {
            "location": "/contribute/documentation/writing_documentation/#writing-documentation", 
            "text": "This section explains how the community can contribute to the Coral Project documentation.  You can contribute to the documentation by editing existing documents for clarity or correcting errors. Additionally, any new features or changes to the software should be thoroughly documented. We value clear, consistent, thorough, readable documentation!  Here is how our documentation works:   The source documentation lives in GitHub at  https://github.com/coralproject/docs/ .  The documentation is hosted at  Read the Docs .  When the documentation is updated and pushed to the GitHub repository, this triggers a build of the Read the Docs site.", 
            "title": "Writing Documentation"
        }, 
        {
            "location": "/contribute/documentation/writing_documentation/#getting-the-documentation", 
            "text": "All of the documentation for the Coral Project  resides in GitHub  in the  docs  repository.  To get a local version of the documentation, clone the repository:  git clone https://github.com/coralproject/docs.git  If you haven t worked with Git before, you can find  instructions on downloading it here .", 
            "title": "Getting the documentation"
        }, 
        {
            "location": "/contribute/documentation/writing_documentation/#getting-started-with-mkdocs", 
            "text": "The Coral Project s documentation uses the  MkDocs  documentation system, which uses Markdown to format text. The  MkDocs website  contains a lot of good information on writing documentation in MkDocs, and there are plenty of decent Markdown cheatsheets floating around,  like this one .", 
            "title": "Getting started with MkDocs"
        }, 
        {
            "location": "/contribute/documentation/writing_documentation/#how-the-documentation-is-organized", 
            "text": "The documentation is organized into several categories:   Introduction : This offers a general overview of the Coral Project and its different components.  Developer Guide : This provides information for technical users of the Coral Project. It offers installation instructions for the Coral Ecosystem as a whole, as well as installation instructions for each individual component.  User Guide : This provides information for end users (publishers, journalists, moderators, readers) on how to use the features of Coral. It includes tutorials and how-to guides.  Contribute : This provides information on how to contribute to the Coral Project through open source. There are sections for the developers (how to work with GitHub, etc.), as well as sections for those who want to contribute to other pieces of the Coral Project (such as the documentation!).  FAQ", 
            "title": "How the documentation is organized"
        }, 
        {
            "location": "/contribute/documentation/writing_documentation/#writing-style", 
            "text": "The key thing to remember when writing documentation is to be extremely explicit and detailed. Do not assume knowledge!", 
            "title": "Writing style"
        }, 
        {
            "location": "/contribute/documentation/writing_documentation/#do-not-assume-knowledge", 
            "text": "Whenever possible, include an actual instruction.  Good:  If there are command line instructions involved, include the precise command line instruction. Instructions that users can copy/paste into the command line are great!    Show  expected results  wherever possible. If a command line instruction should return a certain result, show that expected result. If there is a URL you can visit to test whether or not something is working, provide that URL as a link.  If there are variables to configure, explicitly state the purpose of the variables. This should be explained in a comment in the configuration file, but you should also provide an explanation in your instructions.  Do not merely explain HOW to do something, but also WHY you are doing it in that way. This makes it easier for users to troubleshoot issues.", 
            "title": "Do not assume knowledge"
        }, 
        {
            "location": "/contribute/documentation/writing_documentation/#commands-and-code-blocks", 
            "text": "Enclose commands and code blocks in triple-tick blocks (```). Yes, Markdown supports indentation to delineate code blocks, but the triple-ticks make things more explicit.  Be sure you are using universal command line commands, not shortcuts available in some shells!  Good:  mkdir exampledirectory  Bad:  md exampledirectory", 
            "title": "Commands and code blocks"
        }, 
        {
            "location": "/contribute/documentation/writing_documentation/#third-party-components", 
            "text": "Is there a third-party component the user needs to set up (for instance, MongoDB)?   Don t include all setup instructions, but do link to the setup instructions on the website of the third-party app in question.  Be sure to detail any specific instructions they will need to integrate this third-party component into the system.  Currently, much of this  third party setup  information resides in the  Developer Setup  document. This reduces duplication if there is a component that needs to be set up for multiple items.", 
            "title": "Third-party components"
        }, 
        {
            "location": "/contribute/documentation/writing_documentation/#markup-specific-style", 
            "text": "When adding headlines and section dividers, keep in mind that H1 (#) and H2 (##) text shows up in the Table of Contents. H3 (###) and below does not.  To avoid making the ToC too cluttered, try to only use H2 (##) for key large sections.", 
            "title": "Markup specific style"
        }, 
        {
            "location": "/contribute/documentation/writing_documentation/#installation-instructions", 
            "text": "Be sure to include  every single step . Preferably copy paste.  When working from the command line, include every step in code blocks.", 
            "title": "Installation instructions"
        }, 
        {
            "location": "/contribute/documentation/writing_documentation/#api-documentation", 
            "text": "When documenting a REST-style API, such as the  Pillar API , there are certain pieces of information to include. The  Pillar API documentation  is a useful template.   Create an initial table that lists endpoints, which contains:  The endpoint URL.  The HTTP verb for that endpoint (GET, POST).  The basic functionality description for that endpoint. Make this into a targeted link that jumps to the full description for that endpoint.    Create a full description for each endpoint. You can see the  Get Users  Pillar endpoint for an example). Each description should include:  The parameters for the endpoint (include the parameter name, whether it is required or optional, the type, and the description).  An example call.  The response for the example call (whether that is a JSON object, or simply a status response).", 
            "title": "API documentation"
        }, 
        {
            "location": "/contribute/documentation/writing_documentation/#terminology", 
            "text": "Here are some commonly used terms:", 
            "title": "Terminology"
        }, 
        {
            "location": "/contribute/documentation/writing_documentation/#software-specific", 
            "text": "Coral : Refers to the Coral product, which includes the three components Trust, Ask, and Talk.  Coral Project : Refers to the project of building and crafting the Coral product.  Coral Ecosystem : Refers to all of the pieces that make up Coral from a more technical or development perspective: includes all the technical components such as Pillar, Sponge, and Cay.", 
            "title": "Software specific"
        }, 
        {
            "location": "/contribute/documentation/writing_documentation/#users", 
            "text": "Developers : Refers to the technical users of Coral who will be installing Coral and working with the backend. Also refers to open-source contributors.  End Users : Refers to anyone who will be interacting with the Coral front end. Examples of end users are publishers, moderators, journalists, and readers.", 
            "title": "Users"
        }, 
        {
            "location": "/contribute/documentation/writing_documentation/#release-notes", 
            "text": "", 
            "title": "Release notes"
        }, 
        {
            "location": "/contribute/documentation/writing_documentation/#improving-the-documentation", 
            "text": "Improvements are welcomed! Did you perform an installation, and found some of the language or instructions confusing or missing? Help us fix it!", 
            "title": "Improving the documentation"
        }, 
        {
            "location": "/contribute/documentation/writing_documentation/#translating-documentation", 
            "text": "", 
            "title": "Translating documentation"
        }, 
        {
            "location": "/contribute/reporting_bugs/", 
            "text": "Reporting bugs and requesting features\n\n\nBefore reporting a bug or requesting a new feature, please consider these general points:\n\n\n\n\nCheck that someone hasn\u2019t already filed the bug or feature request by searching the \nIssues\n for the repo in question.\n\n\nDon\u2019t reopen issues that have been marked \u201cwontfix\u201d by a core developer. This mark means that the decision has been made that we can\u2019t or won\u2019t fix this particular issue.\n\n\n\n\nReporting bugs\n\n\nWe use GitHub Issues to track bugs. Each Coral component app has its own repository, but since it may not be clear precisely which component the bug is originating from, we use the Reef repository to track bugs.\n\n\nComplete, reproducible, specific bug reports are very helpful. When writing a bug report, be sure to include the following:\n\n\n\n\nA clear, concise description of the problem.\n\n\nA set of instructions for replicating it.\n\n\n\n\nAdd as much additional debug information as you can, such as: code snippets, test cases, exception backtraces, screenshots, etc. A nice small test case is the best way to report a bug, as it gives us an easy way to confirm the bug quickly.\n\n\nReporting user interface bugs and features\n\n\nIf your bug or feature request touches on anything visual in nature, there are a few additional guidelines to follow:\n\n\n\n\nInclude screenshots in your ticket which are the visual equivalent of a minimal testcase.\n\n\nIf the issue is difficult to show off using a still image, consider capturing a brief screencast. If possible, capture only the relevant area of the screen.\n\n\n\n\nRequesting features\n\n\nHere are some tips on how to make a request most effectively:\n\n\n\n\nDescribe clearly and concisely what the missing feature is and how you\u2019d like to see it implemented. Include example code (non-functional is OK) if possible.\n\n\nExplain why you\u2019d like the feature. In some cases this is obvious, but an explanation can help determine why and how the feature will be useful.", 
            "title": "Reporting bugs and requesting features"
        }, 
        {
            "location": "/contribute/reporting_bugs/#reporting-bugs-and-requesting-features", 
            "text": "Before reporting a bug or requesting a new feature, please consider these general points:   Check that someone hasn\u2019t already filed the bug or feature request by searching the  Issues  for the repo in question.  Don\u2019t reopen issues that have been marked \u201cwontfix\u201d by a core developer. This mark means that the decision has been made that we can\u2019t or won\u2019t fix this particular issue.", 
            "title": "Reporting bugs and requesting features"
        }, 
        {
            "location": "/contribute/reporting_bugs/#reporting-bugs", 
            "text": "We use GitHub Issues to track bugs. Each Coral component app has its own repository, but since it may not be clear precisely which component the bug is originating from, we use the Reef repository to track bugs.  Complete, reproducible, specific bug reports are very helpful. When writing a bug report, be sure to include the following:   A clear, concise description of the problem.  A set of instructions for replicating it.   Add as much additional debug information as you can, such as: code snippets, test cases, exception backtraces, screenshots, etc. A nice small test case is the best way to report a bug, as it gives us an easy way to confirm the bug quickly.", 
            "title": "Reporting bugs"
        }, 
        {
            "location": "/contribute/reporting_bugs/#reporting-user-interface-bugs-and-features", 
            "text": "If your bug or feature request touches on anything visual in nature, there are a few additional guidelines to follow:   Include screenshots in your ticket which are the visual equivalent of a minimal testcase.  If the issue is difficult to show off using a still image, consider capturing a brief screencast. If possible, capture only the relevant area of the screen.", 
            "title": "Reporting user interface bugs and features"
        }, 
        {
            "location": "/contribute/reporting_bugs/#requesting-features", 
            "text": "Here are some tips on how to make a request most effectively:   Describe clearly and concisely what the missing feature is and how you\u2019d like to see it implemented. Include example code (non-functional is OK) if possible.  Explain why you\u2019d like the feature. In some cases this is obvious, but an explanation can help determine why and how the feature will be useful.", 
            "title": "Requesting features"
        }, 
        {
            "location": "/contribute/development/writing_code/", 
            "text": "Writing Code\n\n\nSo you\u2019d like to write some code to improve the Coral Project? Great!\n\n\nBefore you begin\n\n\nBefore writing any code with the intention of merging into master, ensure the work you\nre doing has an GitHub issue and try in good faith to engage the community in conversation in the issue feed. Here is a checklist to follow before starting:\n\n\n\n\nCheck the FAQ to see if your issue has already been addressed.\n\n\nCheck the GitHub issues to see if an issue already exists for the work you want to do, and to make sure that there isn\nt already someone working on it.\n\n\nIf there isn\nt already someone working on it, then leave a comment to let everyone know that you are starting work on it.\n\n\nIf someone is already working on it, consider collaborating with them.\n\n\n\n\n\n\nIf no issue exists, then create an issue for it before getting started. You can find more information on writing a detailed issue \nhere\n.\n\n\n\n\nWriting code\n\n\nOnce you\nve followed the checklist above, browse the following sections to find out how to give your code the best chance of being included in the Coral Project.\n\n\n\n\nCoding style\n\n\nWorking with Git and GitHub", 
            "title": "Writing code"
        }, 
        {
            "location": "/contribute/development/writing_code/#writing-code", 
            "text": "So you\u2019d like to write some code to improve the Coral Project? Great!", 
            "title": "Writing Code"
        }, 
        {
            "location": "/contribute/development/writing_code/#before-you-begin", 
            "text": "Before writing any code with the intention of merging into master, ensure the work you re doing has an GitHub issue and try in good faith to engage the community in conversation in the issue feed. Here is a checklist to follow before starting:   Check the FAQ to see if your issue has already been addressed.  Check the GitHub issues to see if an issue already exists for the work you want to do, and to make sure that there isn t already someone working on it.  If there isn t already someone working on it, then leave a comment to let everyone know that you are starting work on it.  If someone is already working on it, consider collaborating with them.    If no issue exists, then create an issue for it before getting started. You can find more information on writing a detailed issue  here .", 
            "title": "Before you begin"
        }, 
        {
            "location": "/contribute/development/writing_code/#writing-code_1", 
            "text": "Once you ve followed the checklist above, browse the following sections to find out how to give your code the best chance of being included in the Coral Project.   Coding style  Working with Git and GitHub", 
            "title": "Writing code"
        }, 
        {
            "location": "/contribute/development/working_with_github/", 
            "text": "Working With GitHub\n\n\nThis section explains how open source contributors can contribute code to the Coral Project via pull requests.\n\n\nFor a general primer on contributing to open source projects, GitHub has created \na nice guide to contributing to open source\n.\n\n\nThe fundamentals (which are expanded on below) are:\n\n\n\n\nFork the project \n clone locally.\n\n\nCreate an upstream remote and sync your local copy before you branch.\n\n\nBranch for each separate piece of work.\n\n\nDo the work, and write good commit messages.\n\n\nPush to your origin repository.\n\n\nCreate a new PR in GitHub.\n\n\nRespond to any code review feedback.\n\n\n\n\nInstalling Git\n\n\nFirst, \ndownload and install Git\n. You can read more about Git on \ntheir website\n.\n\n\nAfter installing Git, the first thing you should do is setup your name and email:\n\n\ngit config --global user.name \nYour Real Name\n\ngit config --global user.email \nyou@email.com\n\n\n\n\n\nNote that user.name should be your real name, not your GitHub username. The email you use in the user.email field will be used to associate your commits with your GitHub account.\n\n\nSetting up local repository\n\n\nFirst, you need to fork the project: navigate to to the repo in GitHub you want to contribute to and press the \nFork\n button. This will create a copy of the repository in your own GitHub account and you\nll see a note that it\ns been forked underneath the project name.\n\n\nNow create a local copy of that fork (in this example, the \ndocs\n repository is the one being cloned):\n\n\ngit clone https://github.com/coralproject/docs.git\n\n\n\n\nThis will create a new directory \ndocs\n, containing a clone of your forked GitHub repository. Switch to the project\ns new directory:\n\n\ncd docs\n\n\n\n\nYou will now need to setup coralproject/docs as an \nupstream\n remote. This connects your local repository to the original \nupstream\n source repository (essentially, telling Git that the original reference repository is the source of your local forked copy).\n\n\ngit remote add upstream https://github.com/coralproject/docs.git\ngit fetch upstream\n\n\n\n\nIt\ns a good idea to regularly pull in changes from \nupstream\n so that when you submit your pull request, merge conflicts will be less likely. You can find more detailed instructions on \nsyncing a fork from GitHub\n.    \n\n\nWork on an issue\n\n\nWhen working on an issue, create a new branch for the work. Name the branch for the issue you are working on, capitalizing the word \nIssue\n (for example, \nIssue#82\n).\n\n\ngit checkout master\ngit pull upstream master \n git push origin master\ngit checkout -b Issue#82\n\n\n\n\nWhat this does: First, we ensure we\nre on the master branch. Then, the git pull command will sync our local copy with the upstream project and the git push syncs it to our forked GitHub project. Finally, we create our new branch.\n\n\nNow you can start coding! Ensure that you only fix the thing you\nre working on (don\nt get sidetracked into fixing other little things you see along the way).\n\n\nCommitting\n\n\nWhen committing, be sure to commit in logical blocks and add meaningful commit messages.\n\n\ngit commit -m 'Added instructions for commit messages'\n\n\n\n\nSome guidelines to follow:\n\n\n\n\nNever force-push your changes.\n\n\nWrite your commit messages in the past tense, not present tense.\n\n\nGood: \nAdded instructions for commit messages\n\n\nBad: \nAdd instructions for commit messages\n\n\n\n\n\n\nFor commits to a branch, prefix the commit message with the branch name. For example: \u201cIssue#82 Added instructions for commit messages.\u201d\n\n\n\n\nCreate the pull request\n\n\nTo create a PR you first need to push your branch to the origin remote.\n\n\nTo push a new branch:\n\n\ngit push origin Issue#82\n\n\n\n\nWhen you go to your GitHub page, you will notice that a new branch has been created, along with a button that says \nCompare and Pull Request.\n When you feel ready to create a pull request, go ahead and push the button.\n\n\nOn the \nOpen a pull request\n page, ensure that the \nbase fork\n points to the correct repository and branch. Then ensure that you provide a good, succinct title for your pull request and explain why you have created it in the description box.\n\n\nScroll down to see the diff of your changes. Double check that it contains what you expect.\n\n\nOnce you are happy, press the \nCreate pull request\n button.\n\n\nReview by the maintainers\n\n\nOnce you\nve created your pull request, recruit a code reviewer to take a look.\n\n\nThe reviewer will review and potentially offer suggestions. If that happens, make the suggested changes, commit, and push your changes. You may go through several cycles of reviews, changes, and updates.\n\n\nOnce both of you agree that the code is done, it\ns time to merge.\n\n\nMerging\n\n\nThe code reviewer will merge the pull request. If conflicts emerge, TODO.\n\n\nTODO: Add information about tagging.\n\n\nOnce the merge is complete, the branch can be deleted.\n\n!\n\n\nExceptions\n\n\n\n\nUpdates to documentation may be merged directly into master (instead of going through a branch).\n\n\nSmall bugs or tweaks caught by the maintainer post-merge may be merged directly into master.\n\n\nThese commits should include the issue number in the commit message for reference.", 
            "title": "Development workflow"
        }, 
        {
            "location": "/contribute/development/working_with_github/#working-with-github", 
            "text": "This section explains how open source contributors can contribute code to the Coral Project via pull requests.  For a general primer on contributing to open source projects, GitHub has created  a nice guide to contributing to open source .  The fundamentals (which are expanded on below) are:   Fork the project   clone locally.  Create an upstream remote and sync your local copy before you branch.  Branch for each separate piece of work.  Do the work, and write good commit messages.  Push to your origin repository.  Create a new PR in GitHub.  Respond to any code review feedback.", 
            "title": "Working With GitHub"
        }, 
        {
            "location": "/contribute/development/working_with_github/#installing-git", 
            "text": "First,  download and install Git . You can read more about Git on  their website .  After installing Git, the first thing you should do is setup your name and email:  git config --global user.name  Your Real Name \ngit config --global user.email  you@email.com   Note that user.name should be your real name, not your GitHub username. The email you use in the user.email field will be used to associate your commits with your GitHub account.", 
            "title": "Installing Git"
        }, 
        {
            "location": "/contribute/development/working_with_github/#setting-up-local-repository", 
            "text": "First, you need to fork the project: navigate to to the repo in GitHub you want to contribute to and press the  Fork  button. This will create a copy of the repository in your own GitHub account and you ll see a note that it s been forked underneath the project name.  Now create a local copy of that fork (in this example, the  docs  repository is the one being cloned):  git clone https://github.com/coralproject/docs.git  This will create a new directory  docs , containing a clone of your forked GitHub repository. Switch to the project s new directory:  cd docs  You will now need to setup coralproject/docs as an  upstream  remote. This connects your local repository to the original  upstream  source repository (essentially, telling Git that the original reference repository is the source of your local forked copy).  git remote add upstream https://github.com/coralproject/docs.git\ngit fetch upstream  It s a good idea to regularly pull in changes from  upstream  so that when you submit your pull request, merge conflicts will be less likely. You can find more detailed instructions on  syncing a fork from GitHub .", 
            "title": "Setting up local repository"
        }, 
        {
            "location": "/contribute/development/working_with_github/#work-on-an-issue", 
            "text": "When working on an issue, create a new branch for the work. Name the branch for the issue you are working on, capitalizing the word  Issue  (for example,  Issue#82 ).  git checkout master\ngit pull upstream master   git push origin master\ngit checkout -b Issue#82  What this does: First, we ensure we re on the master branch. Then, the git pull command will sync our local copy with the upstream project and the git push syncs it to our forked GitHub project. Finally, we create our new branch.  Now you can start coding! Ensure that you only fix the thing you re working on (don t get sidetracked into fixing other little things you see along the way).", 
            "title": "Work on an issue"
        }, 
        {
            "location": "/contribute/development/working_with_github/#committing", 
            "text": "When committing, be sure to commit in logical blocks and add meaningful commit messages.  git commit -m 'Added instructions for commit messages'  Some guidelines to follow:   Never force-push your changes.  Write your commit messages in the past tense, not present tense.  Good:  Added instructions for commit messages  Bad:  Add instructions for commit messages    For commits to a branch, prefix the commit message with the branch name. For example: \u201cIssue#82 Added instructions for commit messages.\u201d", 
            "title": "Committing"
        }, 
        {
            "location": "/contribute/development/working_with_github/#create-the-pull-request", 
            "text": "To create a PR you first need to push your branch to the origin remote.  To push a new branch:  git push origin Issue#82  When you go to your GitHub page, you will notice that a new branch has been created, along with a button that says  Compare and Pull Request.  When you feel ready to create a pull request, go ahead and push the button.  On the  Open a pull request  page, ensure that the  base fork  points to the correct repository and branch. Then ensure that you provide a good, succinct title for your pull request and explain why you have created it in the description box.  Scroll down to see the diff of your changes. Double check that it contains what you expect.  Once you are happy, press the  Create pull request  button.", 
            "title": "Create the pull request"
        }, 
        {
            "location": "/contribute/development/working_with_github/#review-by-the-maintainers", 
            "text": "Once you ve created your pull request, recruit a code reviewer to take a look.  The reviewer will review and potentially offer suggestions. If that happens, make the suggested changes, commit, and push your changes. You may go through several cycles of reviews, changes, and updates.  Once both of you agree that the code is done, it s time to merge.", 
            "title": "Review by the maintainers"
        }, 
        {
            "location": "/contribute/development/working_with_github/#merging", 
            "text": "The code reviewer will merge the pull request. If conflicts emerge, TODO.  TODO: Add information about tagging.  Once the merge is complete, the branch can be deleted. !", 
            "title": "Merging"
        }, 
        {
            "location": "/contribute/development/working_with_github/#exceptions", 
            "text": "Updates to documentation may be merged directly into master (instead of going through a branch).  Small bugs or tweaks caught by the maintainer post-merge may be merged directly into master.  These commits should include the issue number in the commit message for reference.", 
            "title": "Exceptions"
        }, 
        {
            "location": "/contribute/development/code_style/", 
            "text": "Coding Style\n\n\nGo Coding Style\n\n\nUnless explicitly stated, we follow all coding guidelines from the Go\ncommunity. While some of these standards may seem arbitrary, they somehow seem\nto result in a solid, consistent codebase.\n\n\nIt is possible that the code base does not currently comply with these\nguidelines. We are not looking for a massive PR that fixes this, since that\ngoes against the spirit of the guidelines. All new contributions should make a\nbest effort to clean up and make the code base better than they left it.\nObviously, apply your best judgement. Remember, the goal here is to make the\ncode base easier for humans to navigate and understand. Always keep that in\nmind when nudging others to comply.\n\n\nThe rules:\n\n\n\n\nAll code should be formatted with \ngofmt -s\n.\n\n\nAll code should pass the default levels of\n   \ngolint\n.\n\n\nAll code should follow the guidelines covered in \nEffective\n   Go\n and \nGo Code Review\n   Comments\n.\n\n\nComment the code. Tell us the why, the history and the context.\n\n\nDocument \nall\n declarations and methods, even private ones. Declare\n   expectations, caveats and anything else that may be important. If a type\n   gets exported, having the comments already there will ensure it\ns ready.\n\n\nVariable name length should be proportional to it\ns context and no longer.\n   \nnoCommaALongVariableNameLikeThisIsNotMoreClearWhenASimpleCommentWouldDo\n.\n   In practice, short methods will have short variable names and globals will\n   have longer names.\n\n\nNo underscores in package names. If you need a compound name, step back,\n   and re-examine why you need a compound name. If you still think you need a\n   compound name, lose the underscore.\n\n\nNo utils or helpers packages. If a function is not general enough to\n   warrant it\ns own package, it has not been written generally enough to be a\n   part of a util package. Just leave it unexported and well-documented.\n\n\nAll tests should run with \ngo test\n and outside tooling should not be\n   required. No, we don\nt need another unit testing framework. Assertion\n   packages are acceptable if they provide \nreal\n incremental value.\n\n\nEven though we call these \nrules\n above, they are actually just\n    guidelines. Since you\nve read all the rules, you now know that.\n\n\n\n\nIf you are having trouble getting into the mood of idiomatic Go, we recommend\nreading through \nEffective Go\n. The\n\nGo Blog\n is also a great resource. Drinking the\nkool-aid is a lot easier than going thirsty.\n\n\nAttribution: Go Coding Style adopted from the Docker project.\n\n\nUseful Tools when developing\n\n\n\n\ngolint\n\n\ngofmt\n\n\ngoimports\n\n\ngo vet\n\n\nerrcheck\n\n\n\n\nResources\n\n\n\n\nGo Code Review Comments\n\n\nBest Practices for Production Environments\n\n\n12 Factor App\n\n\nTools for working with Go Code\n\n\n\n\nJavascript conventions\n\n\nWe are currently working under the \nAirbnb js react conventions\n.", 
            "title": "Coding styleguide"
        }, 
        {
            "location": "/contribute/development/code_style/#coding-style", 
            "text": "", 
            "title": "Coding Style"
        }, 
        {
            "location": "/contribute/development/code_style/#go-coding-style", 
            "text": "Unless explicitly stated, we follow all coding guidelines from the Go\ncommunity. While some of these standards may seem arbitrary, they somehow seem\nto result in a solid, consistent codebase.  It is possible that the code base does not currently comply with these\nguidelines. We are not looking for a massive PR that fixes this, since that\ngoes against the spirit of the guidelines. All new contributions should make a\nbest effort to clean up and make the code base better than they left it.\nObviously, apply your best judgement. Remember, the goal here is to make the\ncode base easier for humans to navigate and understand. Always keep that in\nmind when nudging others to comply.  The rules:   All code should be formatted with  gofmt -s .  All code should pass the default levels of\n    golint .  All code should follow the guidelines covered in  Effective\n   Go  and  Go Code Review\n   Comments .  Comment the code. Tell us the why, the history and the context.  Document  all  declarations and methods, even private ones. Declare\n   expectations, caveats and anything else that may be important. If a type\n   gets exported, having the comments already there will ensure it s ready.  Variable name length should be proportional to it s context and no longer.\n    noCommaALongVariableNameLikeThisIsNotMoreClearWhenASimpleCommentWouldDo .\n   In practice, short methods will have short variable names and globals will\n   have longer names.  No underscores in package names. If you need a compound name, step back,\n   and re-examine why you need a compound name. If you still think you need a\n   compound name, lose the underscore.  No utils or helpers packages. If a function is not general enough to\n   warrant it s own package, it has not been written generally enough to be a\n   part of a util package. Just leave it unexported and well-documented.  All tests should run with  go test  and outside tooling should not be\n   required. No, we don t need another unit testing framework. Assertion\n   packages are acceptable if they provide  real  incremental value.  Even though we call these  rules  above, they are actually just\n    guidelines. Since you ve read all the rules, you now know that.   If you are having trouble getting into the mood of idiomatic Go, we recommend\nreading through  Effective Go . The Go Blog  is also a great resource. Drinking the\nkool-aid is a lot easier than going thirsty.  Attribution: Go Coding Style adopted from the Docker project.", 
            "title": "Go Coding Style"
        }, 
        {
            "location": "/contribute/development/code_style/#useful-tools-when-developing", 
            "text": "golint  gofmt  goimports  go vet  errcheck", 
            "title": "Useful Tools when developing"
        }, 
        {
            "location": "/contribute/development/code_style/#resources", 
            "text": "Go Code Review Comments  Best Practices for Production Environments  12 Factor App  Tools for working with Go Code", 
            "title": "Resources"
        }, 
        {
            "location": "/contribute/development/code_style/#javascript-conventions", 
            "text": "We are currently working under the  Airbnb js react conventions .", 
            "title": "Javascript conventions"
        }, 
        {
            "location": "/contribute/supporting_the_community/", 
            "text": "Organizing Events\n\n\nGoals\n\n\n\n\nFire up discussion on community\n\n\nShare our research/discussion on healthy community\n\n\nShare tools we are building\n\n\n\n\nActivities\n\n\nBreak the Ice Activity\n\n\nDiscuss:\n\n\n\n\nname / any organization\n\n\nwhat community you participate in?\n\n\n\n\nWrite down sticky notes:\n\n\n\n\nwhat makes that a healthy community\n\n\n\n\nSpectogram\n\n\nBrainstorm group\n\n\nLighting Talks", 
            "title": "Supporting the community"
        }, 
        {
            "location": "/contribute/supporting_the_community/#organizing-events", 
            "text": "", 
            "title": "Organizing Events"
        }, 
        {
            "location": "/contribute/supporting_the_community/#goals", 
            "text": "Fire up discussion on community  Share our research/discussion on healthy community  Share tools we are building", 
            "title": "Goals"
        }, 
        {
            "location": "/contribute/supporting_the_community/#activities", 
            "text": "", 
            "title": "Activities"
        }, 
        {
            "location": "/contribute/supporting_the_community/#break-the-ice-activity", 
            "text": "Discuss:   name / any organization  what community you participate in?   Write down sticky notes:   what makes that a healthy community", 
            "title": "Break the Ice Activity"
        }, 
        {
            "location": "/contribute/supporting_the_community/#spectogram", 
            "text": "", 
            "title": "Spectogram"
        }, 
        {
            "location": "/contribute/supporting_the_community/#brainstorm-group", 
            "text": "", 
            "title": "Brainstorm group"
        }, 
        {
            "location": "/contribute/supporting_the_community/#lighting-talks", 
            "text": "", 
            "title": "Lighting Talks"
        }, 
        {
            "location": "/contribute/contributor_license_agreement/", 
            "text": "To Be Determined", 
            "title": "Contributor License Agreement"
        }, 
        {
            "location": "/contribute/contributor_license_agreement/#to-be-determined", 
            "text": "", 
            "title": "To Be Determined"
        }, 
        {
            "location": "/faq/", 
            "text": "", 
            "title": "FAQ"
        }
    ]
}